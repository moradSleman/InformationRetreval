Data and Algorithmic Bias in the Web
(Keynote Extended Abstract)
Ricardo Baeza-Yates
Dept. of Information and Communication Technologies
Universitat Pompeu Fabra, Barcelona, Spain
&
Dept. of Computing Sciences
Universidad de Chile, Santiago, Chile

rbaeza@acm.org
ABSTRACT

As web content is a reflection of ourselves, it is a nontrivial problem to know which content is not appropriate, as
ultimately in most cases this depends in the context. The
first step is to be aware of the bias in web data. Bias awareness not only allows us to weight the results of, say, a machine learning algorithm, but also will guide us to improve
the results by removing the bias effect (either by changing
the input data, modifying the algorithm, and/or manipulating the results).
One important bias is activity bias, due to the effect of
Zipf’s law of minimal effort. That is, most people do not
generate content, they just watch the content generated by
others. This implies that a minority of active users generate
more than half of the content and hence the wisdom of the
crowds is really the wisdom of a few. This not only happens
at the level of paragraphs or web pages, but also at the
website level.
Even when there is no data bias, the algorithm being used
may generate its own data bias. One of the most important
cases of this type is presentation bias. This bias may change
future interaction data (e.g., clicks), creating a new data
bias. Interaction bias can also be due to social web data
such as ratings or comments. Even worse, in cases such as
tag recommendation, an algorithm may kill a folksonomy.
One way to fight presentation bias is to add elements such
as diversity, novelty and/or serendipity.
A more subtle algorithmic bias is how search engines
are biasing new web content with their ranking algorithms.
When people generate a new web page based on the top results of their searches, this new content is biased by what
the search engine believes is more relevant (and not what
really is most relevant), and also increases data redundancy.
In the context of personalization, another bias is given by
the “filter bubble” issue. That is, no algorithm can recommend something new that you may like, if does not use data
from other people. For this problem, collaborative filtering
is the simplest method that breaks this bubble, and also is
much better because the input data will be richer.
Another concern that plays an important role is privacy.
Indeed, too much personalization endangers privacy. In general, using the context of the task that the user is trying to
achieve and using data from people that in the past were in
the same context, improves the results. This does not only
solves the filter bubble problem, but also protects user privacy as individual user data is hidden inside data of many
people, where k-anonymity can be used.

The Web is the largest public big data repository that humankind has created. In this overwhelming data ocean we
need to be aware of the quality of data extracted from it.
One important quality issue is data bias, which appears in
different forms. These biases affect the (machine learning)
algorithms that we design to improve the user experience.
This problem is further exacerbated by biases that are added
by these algorithms, especially in the context of recommendation and personalization systems. We give several examples, stressing the importance of the user context to avoid
these biases.

CCS Concepts
•Computing methodologies → Machine learning;
•Information systems → Data extraction and integration;
Social networks;

Keywords
data bias, algorithmic bias, privacy, novelty, diversity, redundancy, noise, spam.

1.

SUMMARY

We know that the output quality of any algorithm is a
function of the quality of the data that it uses. However,
most of the time we assume that the input data is correct
and complete, when in practice most of the time is not. This
is even more true in the context of the Web, where the data
is generated by people in many different ways and within the
context of their own biases (cultural, political, economical,
etc.). A very recent and good example is the Twitter bot,
Tay, released by Microsoft on March 23, 2016. This bot
lasted one day as its tweets became sexist and racist because
its algorithm learned from biased malicious tweets coming
from trolls, one of the many facets of web spam.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).

WebSci ’16 May 22-25, 2016, Hannover, Germany
c 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4208-7/16/05.
DOI: http://dx.doi.org/10.1145/2908131.2908135

1

