IN BRIEF

Fairness and transparency
in the age of the algorithm
Sofia Olhede and Russell Rodrigues discuss recent efforts
to ensure greater scrutiny of machine-generated decisions

Sofia Olhede is a
professor of statistics
at University College
London, and scientific
director of the UCL Big
Data Institute

Russell Rodrigues is
operations manager
of the UCL Big Data
Institute

T

he algorithm might be considered
the workhorse of the modern
digital economy. No matter
what business you are interacting with
– whether in media, retail, healthcare
or finance – there is a good chance
that, behind the scenes, an algorithm
is churning through your data, running
through a series of steps or calculations
to automate or support a decision that
the company needs to make. It might be
a decision about what advert you see, or
what product to recommend, or whether
you qualify for a loan.
Reports from the UK (bit.ly/2fCKh7b)
and US governments (bit.ly/2lCSs8H)
point to efficiency savings as one of the
potential benefits of using algorithms.
But there are potential costs too.
Recent news stories have highlighted
the risks of skewed outputs, and how
they can entrench social inequalities
(bit.ly/2i5EB5Y).
With algorithms being applied in
a broadening range of domains, it
is crucial to ensure that machinegenerated decisions are as fair and
unbiased as possible, especially when
they affect human lives. The problem
is that much of what an algorithm does
is hidden from view – inscrutable to
those whose data it feeds on.

08 SIGNIFICANCE April 2017

A peek inside the “black box”
While great technological strides have
been made to enhance the capability of
algorithms, efforts to define the ethical
frameworks for implementation are
only just beginning, though they are
rapidly gathering pace.
“Transparency” is a concept
mentioned frequently in these debates,
and it is an important one: we cannot
evaluate the probity, or fairness, of a
computer-generated decision without a
clear understanding of the population
from which the data is drawn and the
logical steps an algorithm goes through
to determine its outputs.1,2
Even if we can identify the
variables fed into an algorithm,
data which reflect poor sampling
design, unconscious bias, or which
contain irrelevant correlations will
have repercussions for the computed
output: the algorithm can only work
with the data it has. Initiatives to
improve the quality and availability
of data, such as those championed by
the UK Open Data Institute, can help
to alleviate some of these issues. But
better data will not solve all problems
because – in many cases – an
algorithm’s inner workings resemble
a “black box”, and it is often unclear

Further reading
We recently
interviewed Cathy
O'Neil about her book
Weapons of Math
Destruction, which
argues that decisionmaking algorithms are
increasing inequality
and threatening
democracy. See
bit.ly/2mSh3Io

precisely how the input data are used
to reach a decision.
Modern machine-learning (ML)
algorithms are typically designed and
trained to excel in predictive accuracy
using massive volumes of data. The
availability of extremely large data sets,
together with modern computational
power, makes this approach quite
practical. However, with prediction as
the endpoint, such algorithms tend to
assimilate the input data and construct
complex models with convoluted
and interacting components. This is
especially evident with the intricate,
multi-layered ML systems used in deep
learning and convolutional neural
networks. It thus becomes difficult to
unpick specific strands of the decisionmaking process to understand precisely
how a conclusion was reached.
By contrast, traditional statistical
algorithms are concerned with
explanation as well as prediction,
and tend to use clearly specified,
often linear models, which are easier
to scrutinise – although they are,
on occasion, less powerful. In some
instances, the impressive performance
of ML algorithms can make the lack of
transparency a reasonable trade-off,
but this may not always be the case.
© 2017 The Royal Statistical Society

Main image: nullplus/Bigstock.com

Finding the right words
Early research efforts have focused
on developing a technical vocabulary
to describe such problems.1,2 Cynthia
Dwork of Harvard University has
proposed a framework which explicitly
quantifies the notion of fairness with
respect to a given population, thus
ensuring it is built into the decisionmaking process. Further work in this
direction will enable the balance
between algorithm performance
and the fairness of decisions to be
measured objectively, and not simply
described in abstract terms.
In addition, a number of scholarly
societies are convening discussions
and beginning to draw up ethical
frameworks for algorithm design and
implementation. The Association
for Computing Machinery has
issued a statement on “Algorithmic
Transparency and Accountability”, with
seven principles to maximise scrutiny
and minimise harm (bit.ly/2iopIi5).
Similarly, the IEEE has launched
a global initiative on the ethics of
artificial intelligence and autonomous
systems, highlighting the responsibility
and accountability needed to ensure
that algorithms do not infringe human
rights (bit.ly/2n9ljmd). In the UK, the
Royal Society and British Academy

have established a working group
comprising lawyers, philosophers,
social scientists, mathematicians,
statisticians and computer scientists.
This group is considering the ways in
which data-driven technologies can be
best governed so as to reap the benefits
of innovation while preserving integrity
and trust in the eyes of the public
(bit.ly/2n9rgzH).

A “right to
explanation” for
AI decisions?
Academics divided over wording of new
data protection regulation. By Brian Tarran

Open dialogue
Societal attitudes are ultimately shaped
through public discourse, and this will
be equally true of attitudes towards
algorithm-powered technologies.
However, public understanding is
currently hindered by a technical
barrier. Deep-learning algorithms,
for example, are a scientific frontier;
even experts can seldom describe their
mechanics in granular detail, and it
is even more challenging to do so in
a manner that allows fairness to be
assessed and understood.
But public understanding is crucial:
after all, a key principle of European
data protection law is that of “informed
consent” – that people give permission
for their data to be used only once
they understand exactly how and
why it will be used. In the context of
complex algorithms, it may be easy to
explain what data will be used, but
the technical detail of the “how” and
“why” is less easy to communicate.
This challenge will only become
greater as algorithms become ever
more sophisticated. Transparency
is an important step to establish
and maintain fairness, but a much
broader framework of governance is
required to ensure that algorithms are
implemented responsibly, with proper
accountability for – and understanding
of – the decisions that are made. n
References
1. Dwork, C., Hardt, M., Pitassi, T., Reingold,
O. and Zemel, R. S. (2012) Fairness through
awareness. In Innovations in Theoretical
Computer Science, pp. 214–226. New York:
Association for Computing Machinery.
2. Kleinberg, J., Mullainathan, S. and Raghavan,
M. (2016) Inherent trade-offs in the fair
determination of risk scores. arXiv:1609.05807.

Jorisvo/Bigstock.com

Sofia Olhede photo: Kirill Photography

Without the ability to thoroughly
scrutinise algorithms, there is little
recourse when contested judgements
are made. That is why transparency
features so heavily in current
discussion. Transparency may not seem
important if an algorithm is simply
recommending films and restaurants,
but it is of greater concern where
mortgage or employment applications
are concerned. Few people are likely
to be satisfied with a “computer says
no”-style rejection, should they fall foul
of an algorithm into which they have
no insight. Similarly, it is little comfort
to be told: “We used mean square error
as our error metric in prediction and
unfortunately you do not qualify.”
The challenge, therefore, is to make
algorithms transparent, fair and
intelligible to the people affected by
their outputs.

D

oes the European Union’s new General Data
Protection Regulation (GDPR) provide a “right to
explanation” of decisions made by automated systems
and algorithms? Academics from the Oxford Internet
Institute (OII) are split on the issue.
A paper by Bryce Goodman of the OII and Seth Flaxman
of the University of Oxford’s Department of Statistics
argues that the GDPR creates a legal right “whereby a
user can ask for an explanation of an algorithmic decision
that was made about them” (bit.ly/2lPFGUi).
Goodman and Flaxman point to the wording of
Articles 13 and 14, both of which are legally binding and
which state that people – or “data subjects” – must
be provided with certain information to ensure “fair
and transparent [data] processing”. Where automated
decision-making is concerned, this would include any
“meaningful information about the logic involved”.
But a more recent paper, written by Goodman’s OII
colleagues Sandra Wachter, Brent Mittelstadt and
Luciano Floridi, argues that the same articles only really
provide a limited “right to be informed” when automated
decision-making processes are to be used, rather than
any legal right to receive an explanation, after the fact, of
how specific decisions were made.
Wachter et al. note that in an accompanying piece of
guidance, known as Recital 71, explicit mention is made
of the right to obtain “an explanation of the decision
reached” – but this is not legally binding. They argue
that if the GDPR is meant to confer such a “right to
explanation”, it needs to be added as an article before the
new regulation comes into force in 2018 (bit.ly/2lPQLo4). n

April 2017 significancemagazine.com 09

