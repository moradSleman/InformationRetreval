Detecting Bias in Black-Box Models Using
Transparent Model Distillation
Sarah Tan

Cornell University
Ithaca, NY 14850
ht395@cornell.edu

arXiv:1710.06169v1 [stat.ML] 17 Oct 2017

Giles Hooker

Cornell University
Ithaca, NY 14850
gjh27@cornell.edu

ABSTRACT
Black-box risk scoring models permeate our lives, yet are typically
proprietary and opaque. We propose a transparent model distillation approach to understand and detect bias in such models. Model
distillation was originally designed to distill knowledge from a large,
complex model (the teacher model) to a faster, simpler model (the
student model) without significant loss in prediction accuracy. We
add a third restriction - transparency - and show that it is possible
to train transparent, yet still accurate student models to understand
the predictions made by black-box teacher models. Central to our
approach is the use of data sets that contain two labels to train
on: the risk score as well as the actual outcome the risk score was
intended to predict. We fully characterize the asymptotic distribution of the difference between the risk score and actual outcome
models with variance estimates based on bootstrap-of-little-bags.
This suggests a new method to detect bias in black-box risk scores
via assessing if contributions of protected features to the risk score
are statistically different from contributions to the actual outcome.

KEYWORDS
transparency, model distillation, black-box risk scores, detecting
bias, tree additive models, recidivism, loans

1

INTRODUCTION

Risk scoring models have a long history of usage in criminal justice,
finance, hiring, and other critical domains that impact people’s
lives [9, 18]. Recent work in recidivism risk scoring has centered on
the tension between using sophisticated black-box models for the
promise of high accuracy that promise to be highly accurate and
more transparent models that may come at the cost of prediction
accuracy [23]. Worryingly, risk scoring models are increasingly
used for high-stakes decisions, yet are typically proprietary and
opaque.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
xxx, xxx
© 2017 Copyright held by the owner/author(s). xxx-xxxx-xx-xxx/xx/xx. . . $xx.xx
DOI: xx.xxx/xxx

Rich Caruana

Microsoft Research
Redmond, WA 98052
rcaruana@microsoft.com

Yin Lou

Airbnb, Inc.
San Francisco, CA 94103
yin.lou@airbnb.com
Suppose the true risk scoring model is:
y S = r S (x)

(1)

designed to predict some future outcome, y O , for an individual, for
example defaulting on a loan or re-offending. While information
on the model class of r S , its functional form, or what features were
used to create it are sometimes divulged, rarely is r S revealed in
its entirety. One attempt to detect bias in r S could be to reverse
engineer it. However, reverse engineering can be stymied by the
lack of access to all features and the same data sample used to create
r S , or a means of determining how close the reverse engineered
result is to the true, unknown r S .
On the other hand, one could study the actual outcome y O ,
testing for disparate impact through methods such as training a
model to predict y O , removing, permuting, or obscuring a protected
feature [1, 11], then retraining the model. One challenge with this
approach is that advance knowledge of which feature to act on is
needed, and there may be biases in the data set arising from other
features not studied.
We propose a third approach - transparent model distillation
for bias detection. Here, we train a transparent student model to
mimic a black-box risk score teacher. We intentionally include all
features that may or may not be originally used in the creation
of the black-box risk score, even protected features, specifically
because we are interested in examining what the model learns from
these variables. Then, we train another transparent model to predict
the actual outcome that the risk score was intended to predict. We
then ask the question:
Are there systematic differences in the risk scoring
model from the actual outcome?
We operationalize any such systematic differences as bias. Of
particular concern is when such systematic differences exists for
protected features such as race. Hence, our goal is to automatically
detect feature values where the risk scoring model significantly
differs from the actual outcome.
Notably, the data set we use need not contain the same observations used to create r S , and may lack features used to create
r S (or have additional features not used to create r S ). The only
requirement our proposed approach places on the data set is that it
contains two labels: (1) the black-box risk score (e.g. credit score),
and (2) the actual outcome (e.g. whether a loan defaulted).

xxx, xxx, xxx

Tan, Caruana, Hooker, & Lou

To summarize the proposed approach: treating the black-box
risk score as the teacher model, we train a class of transparent yet
still accurate student models to predict the teacher. The transparent
models we use are interpretable generalized additive models (GAMs)
[7, 16, 17] based on a variant of short, boosted trees. Then we train
another model from the same transparent model class to predict
the actual outcome. Comparing the transparent student model
with the transparent model trained on the actual outcome reveals
feature values with potential biases in the teacher black-box risk
score, and we quantify whether this bias is statistically significant by
characterizing the asymptotic distribution of the difference between
these two transparent models.
The rest of the paper is organized as follows. Section 2 describes
the proposed transparent model distillation method based on interpretable GAMs. Sections 3 describes the data sets with two labels we
found, and experimental setup. Section 4 describes the results, focusing on the investigation and detection of biases, and the idoneity
of interpretable GAMs as transparent models compared to simpler
models such as linear or logistic regression. Sections 5 provide
additional discussions and related work. Section 6 concludes.

2

METHOD

N be a data set of size N , where y S is a risk
Let D = {(yiS , yiO , x i )}i=1
O
score and y is the actual outcome the risk score was intended to
predict. x i = (x i1 , . . . , x ip ) is a vector of p features for individual i,
and x j is the jth variable in feature space. Our goal is to detect regions of feature space, including posystematic differences between
y S and y O .

2.1

Transparent Model Distillation

Model distillation [6, 13] was originally designed to distill knowledge from a large, complex model (the teacher model) to a faster,
simpler model (the student model) without significant loss in prediction accuracy. We add a new restriction: transparency. Let
M denote a class of transparent models. We first describe the proposed distillation setup before delving into the choice of transparent
model in Section 2.3. At this point, we merely note that besides
being transparent, model class M must have both regression and
classification capabilities. We propose to train two models of class
M:
(Model S): Student model of true black-box risk score
teacher r S :
y S = f S (x)
(2)
which performs regression.
(Model O): Model of actual outcome:
д(y O ) = f O (x)

(3)

where д is the logistic link function, so the model performs
classification.
As in classic model distillation, if the student model (Model S) is
a high fidelity approximation of its teacher, we can then attempt
to understand r S by looking at f S . In other words, whenever the
unknown r S is needed, f S is used instead. This brings us to the
third requirement for model class M: that it is accurate.

Where does the model of the actual outcome (Model O) come into
the picture? Recall that r S , now approximated by f S , was designed
to predict the actual outcome y O — precisely what we trained f O
to predict. Examining the contribution of a protected feature to f S
compared to f O will invariably yield differences. How do we know
if such differences are real and systematic, or due to random noise?

2.2

Detecting Bias Using Hypothesis Testing

Hypothesis testing lets us answer this question. Denote feature x j ’s
contribution to the risk score student model and outcome model as
C S (x j ) and C O (x j ) respectively. The null hypothesis of no bias —
that a protected feature does not contribute to the risk score any
higher (or lower) than it contributes to the actual outcome — is:
Null hypothesis (no bias): C S (x j ) = C O (x j )
Two-sided alternative hypothesis: C S (x j ) , C O (x j )
The null hypothesis is rejected when the p-value, a function of
P(C S (x j ) − C O (x j )), is small. This brings us to the fourth and final
requirement for model class M: that the probability distribution
and uncertainty of the difference between a feature’s contribution
to two different models of class M is characterizable.

2.3

Choice of Transparent Model Class M

Table 1 summarizes common machine learning models and their
standing in terms of the four requirements for model class M.
For example, decision trees are transparent, reasonably accurate, and can perform both regression and classification, but are
notoriously unstable [12] which in turn leads to instability in inferences of how a feature contributes to the prediction. Linear
models (including logistic regression) are transparent and perhaps
less accurate, but each feature’s contribution to the prediction is
asymptotically normal with known variance1 , and hence the difference between two such models is characterizable. Full complexity
models such as random forests and neural networks are accurate
but lack transparency and the contribution of individual features
to the prediction, while visualizable using partial dependence plots,
does not have characterizable uncertainty nor distribution.
2.3.1 Generalized additive models. We propose to use tree-based
generalized additive models [7, 16, 17], a class of transparent models
based on a variant of bagged, short trees learned using gradient
boosting. Its transparency stems from its additive form2 :
д(y) = f (x) = β 0 +

p
Õ

h j (x j )

(4)

j=1

where each term h j (x j ) is a shallow tree restricted to only operate on
one feature, shaping it with two or three splits. h j is called the shape
function of feature x j and can be plotted against x j in graphs such
as the red or green lines in Figure 2. This allows the contribution of
any one feature to the prediction to be directly examined, making
the model transparent. Multiple terms are learned together using
gradient boosting to obtain an additive formulation, hence the name
generalized additive models (GAMs). However, unlike classical
1 From

maximum likelihood estimation theory.
is the logistic link function for classification. For regression, д is the identity
function.
2д

Black-Box Risk Models Transparent Student-Teacher Distillation

xxx, xxx, xxx

Table 1: Choosing a Transparent Model Class M
Model

Form

Generalized Linear
y = β 0 + β 1 x 1 + . . . + βp x p
Model*
д(y) = β 0 + β 1x 1 + . . . + βp xp
Generalized Additive
y = β 0 + h 1 (x 1 ) + . . . + hp (xp )
Model*
д(y) = β 0 + h 1 (x 1 ) + . . . + hp (xp )
Decision Tree
y = h(x 1 , . . . , xp )
Full Complexity
y = h(x 1 , . . . , xp )
Model**
* д is the logistic link function for classification
** E.g. random forest, boosted tree, neural network
GAMs where features are shaped using splines, tree-based GAMs
shape features using short trees3 .
Experiments [7, 16, 17] show that this model combines the predictive accuracy of tree ensembles with the transparency of linear
and logistic regression. In Section 4.3 and 4.4 we study these claims
of accuracy and transparency in the context of our proposed transparent model distillation approach.

2.4

Characterizing the Distribution of
Differences

Comparing the feature contributions across two models presents
some challenges. The first challenge is that one model (Model S)
performs regression and the other (Model O) performs classification.
Section 2.4.1 describes a way to make these feature contributions
comparable. The second challenge is the several possible sources of
variation can affect the difference between these feature contributions, leading to spurious judgments of bias. Section 2.4.2’s structured bagging setup removes avoidable sources of noise. Finally,
Section 2.4.3 details the estimation of variance and covariances for
feature contributions of this class of bagged, short boosted tree
models.
2.4.1 Making Outcomes and Scores Comparable. Additive models built on different labels are measured in different units. In our
approach, binary outcomes result in additive effects measured in
units of inverse logit probabilities while those based on scores are
given by the units of the score. In order to make these comparable,
we construct standardized effects by subtracting the average from
each effect and scaling by its standard deviation. Specifically, for
each effect we compare terms of the form
∫
h j (x j ) − h j (s)ds
∗
h j (x j ) = r
.
(5)
∫
2
∫
2
h j (s) ds − h j (s)ds
between models. These terms are now dimensionless [19] and
centered on zero and can be compared.
2.4.2 Structured Bagging Setup. Given the dataset, we randomly
select 15% of the samples to be the test set. The remaining 85% of the
3 This sacrifices some smoothness of the learned shape function for a gain in prediction

accuracy [16].

Transparency

Accuracy

Regression &
Classification

Characterizable
Difference

+++

+

Both

Yes

++

++

Both

Yes

++

++

Both

No

+

+++

Both

No

data is further split into training (70% of the data) and validation
(15%). The random split of this 85% of the data into train and
validation is repeated L times (while keeping the test set constant),
and the model predictions and feature contributions are averaged.
This whole process (selecting a test set and then performing L folds
of the remaining data) is repeated K times. Figure 1 illustrates this.
We use the same K outer folds and L inner folds to train both
models, and compare the models within each inner fold. This removes variation due to training the two models on different splits
of the data.
2.4.3 Variance Estimation. Sexton and Laake [21] proposed a
bootstrap-of-little-bags approach to estimate the variance of functions learned from bagged ensembles, and Athey et al. proved the
consistency of this variance estimate [4]. While bagging was originally introduced to this short boosted tree model to reduce the
variance of the learned shape functions, in this paper, bagging has
additional importance, as we can use the bootstrap-of-little-bags
variance estimate to estimate the variance of h j (x j ): feature x j ’s
contribution to the prediction.
Using Sexton and Laake’s bootstrap-of-little bags approach, given
k = 1, . . . , K outer folds, each of which has l = 1, . . . , L inner folds,
we first take the average of h j (x j ) across inner folds of the same
outer folds, yielding K averages. Then we take the variance of
these K averages as our variance estimate for one model. The variance estimate for the difference between two models is then a sum
of the individual variances and covariance, also estimated using
bootstrap-of-little-bags.
It is important to note that this gives us pointwise confidence intervals, i.e. confidence intervals at specific values of x j e.g. race=AfricanAmerican, age=50, etc., which is sufficient for our goal of detecting
specific feature values that exhibit systematic differences between
the risk score and actual outcome. We discuss how uniform confidence intervals can be constructed in Section 5.4.

2.5

Choice of Features

Also necessary to our approach is the use of only baseline features
(features available when the risk score is administered, i.e. when
the individual applies for a loan) x. Even though the prediction of
the actual future adverse outcome can likely be improved by the
inclusion of features-after-baseline (e.g. whether a loan payment

xxx, xxx, xxx

Tan, Caruana, Hooker, & Lou

Figure 1: Structured Bagging Setup

was missed), not including non-baseline features is necessary to
ensure that the two models utilize the same set of features.
Interpretable features. While not a strict requirement, noninterpretable features reduce the transparency of the results from
the learned models.

3 EXPERIMENTAL SETUP
3.1 Data
We use three publicly available data sets containing both a blackbox risk score and the actual outcome. Table 2 describes them.
Code to pre-process these data sets will be released on http://shftan.
github.io.
3.1.1 COMPAS Risk Score and True Recidivism Outcome. COMPAS, a proprietary score developed to predict recidivism risk has
been the subject of scrutiny for racial bias [3, 5, 8–10, 14, 22]. Because the algorithm does not use race as an input [20], its proponents suggest that it is race-blind. ProPublica collected, analyzed
[2], and released data4 on COMPAS scores and true recidivism
outcomes of defendants in Broward County, Florida. Potentially
protected features in this data set are age, race, and sex. Additional features are charge degree and number of priors. We also
constructed a new feature - prison length of prison stay - based on
available date variables.
3.1.2 Lending Club Loan Risk Score and True Default Outcome.
Lending Club, an online peer-to-peer lending company, makes
public information on all the loans it finances5 . We use a subset
of five years (2007-2011) of loans. This time period was chosen
because all loans have matured, and the outcome of whether the
loan defaulted has been observed. We use only individual, not
joint loans, and remove non-baseline features such as loan payment
information that could leak information into the label. Candidates
for protected features in this data include state and zipcode.
4 https://github.com/propublica/compas-analysis

5 https://www.lendingclub.com/info/download-data.action

3.1.3 Chicago Strategic Subject List Risk Score and True “Partyto-Violence” Outcome. We are still experimenting on this data set
and will update this paper accordingly.

3.2

Training Setup

We train a GAM regression model on the risk score label and GAM
classification model on the actual outcome label. We used 5,000
gradient boosting iterations to train each GAM model, selecting
the optimal number of iterations (≤ 5,000) based on optimal performance on a validation set. For comparison, we also perform
random forest and linear and logistic regression.
3.2.1 Detecting Bias Using Confidence Intervals. To determine if
the difference between the risk score label and outcome label models
is statistically significant, we could report p-values. However, the
duality between confidence intervals and hypothesis testing pvalues affords a quicker method - a visual inspection of whether
the horizontal line at y = 0 representing zero difference was within
the confidence interval. If this is not the case, we reject the null
hypothesis that there is no difference between the two models.
3.2.2 Accuracy Metrics. We compare regression models using
root mean square error (RMSE), and binary classification models
using area under the ROC curve (AUC) and prediction accuracy
(Acc), though as adverse outcome is typically imbalanced, accuracy
should be compared against the baseline rate of the ”Yes” class in
Table 2

4

RESULTS

In this section we demonstrate insights derived from comparing
transparent student models trained to predict black-box risk scores
to transparent models trained to predict actual outcomes. We also
validate that GAM models are good student models.

4.1

Detecting Bias in COMPAS

Figure 2 shows shape plots for four of the features used by COMPAS for recidivism prediction: Age, Race, Number of Priors, and
Gender. The top row shows what was learned by the transparent

Black-Box Risk Models Transparent Student-Teacher Distillation

xxx, xxx, xxx

Table 2: Description of datasets
Label 1 (Risk Score)
Data
COMPAS recidivism

# Observations
6172

# Features
6

Scale*
1-10

Mean ± SD**
4.41 ± 2.84

Label 2 (Outcome “Yes” or “No”)
Baseline Rate for “Yes” Class
0.46

Lending Club loans
42,506
28
A-G converted to 0-6
1.67 ± 1.44
0.15
* Throughout this paper, we use the convention that the higher the risk score, the more likely the outcome.
** Calculated on entire data set
models trained to predict either the COMPAS risk score (red), or
the true recidivism outcome (green). The transparent model trained
to mimic the COMPAS model (red) gives insight into how the COMPAS model works, where as the transparent model trained on the
true outcome shows what can be learned from the data itself. 95%
confidence intervals are shown for both models. The middle row of
Figure 2 shows the difference between the red and green terms in
the top row, along with 95% confidence intervals for this difference.
The bottom row shows frequency histograms for each feature.
4.1.1 COMPAS is biased for some age and race groups. Examining the plots on the left of Figure 2 for Age, we see that the red
mimic model and the green true labels model are very similar for
ages 20 to 70: the confidence intervals in the top plot overlap significantly, and the confidence intervals in the difference plot (middle
row) usually include zero. For Age greater than 70 where the number of samples is low (bottom plot), the variance of the true-labels
model is large and there is not enough evidence to conclude that
the models disagree. However, the difference between the COMPAS mimic model and the true label model is signficant for ages
18 and 19: the COMPAS model apparently predicts low risk for
very young offenders, but we see no evidence to support this in the
model trained on the true labels where risk appears to be highest
for young offenders. This suggests an interesting bias favoring
young offenders in the COMPAS model that does not appear to be
explained by the data.
The next set of graphs in Figure 2 show risk as a function of Race.
COMPAS apparently predicts Native Americans are a high risk
group, despite the fact that the model trained on the true outcomes
predicts this group is relatively low risk. The COMPAS mimic
model also predicts that African Americans are higher risk, and
that Caucasians are lower risk, than the transparent model trained
on the true labels suggests is warranted. Apparently the COMPAS
model is even more biased against African Americans and towards
Caucasians than the (potentially biased) training data warrants.

lower risk than the data suggests is correct for men. We suspect
this difference arises because most of the training data is for males
(bottom graph), and that this COMPAS model is not as good at
distinguishing between male and female as it could be. Interestingly, there is now a separate COMPAS model aimed specifically at
recidivism risk prediction for women.
In summary, comparing transparent student models trained to
mimic the black-box COMPAS model to transparent models trained
on the true recidivism labels allows us to identify biases in the
COMPAS model that do not appear to be justified by the data,
raising potential red flags about the validity of the COMPAS model
that are very specific and warrant further investigation.

4.2

Investigating Lending Club Loan Risk
Scores

Figure 3 shows shape plots for four of the features used in the loan
default risk prediction model: Annual Income, FICO Score, Interest
Rate, and Loan Purpose. As in Figure 2, red lines show what was
learned by the transparent student model trained to mimic the
lending model by training on scores predicted by that model, and
green lines show what a transparent model learned when training
on the true credit fault labels. Comparing the red and green lines
helps us understand what the black-box lending model learned, and
how it differs from what a model could have learned from the true
labels.

4.1.2 COMPAS agrees with data on number of priors. In the 3rd
column, the COMPAS mimic model and the true-labels model agree
on the impact of Number of Priors on risk — the error bars overlap
through most of the range and become very wide when the largest
difference is observed for more than 30 priors.

4.2.1 The black-box lending model probably ignores income and
loan purpose. Interestingly, the black-box lending model appears
to ignore Self-Reported Income (red), even though a model trained
on the true labels shows a strong effect for income on risk (green).
We suspect the lending model may ignore income because it is
self-reported and thus easy to fake.
In the last plot for the Loan Purpose term, we see that the lending
model probably also ignores the loan purpose (red), but a model
trained on the true labels (green) suggests that purpose is a useful
feature and that risk is highest for small business loans, and least
for loans taken out for weddings. For the lending model we see a
number of graphs like this where the mimic model is essentially flat
on a feature that the model trained on the true labels finds useful,
and suspect that this is one way of determining which features are
or are not used by a black-box model.

4.1.3 Gender has opposite effects on COMPAS compared to true
outcome. In the 4th column, we see a discrepancy between what
the COMPAS mimic model and the true-labels model learned for
Gender. The COMPAS model predicts that Females are higher risk
than the data suggests is correct for women, and that males are

4.2.2 The models agree and disagree on FICO. As for FICO score,
the models agree for low to intermediate FICO scores, but disagree
for scores around 800. A transparent model trained on the true
labels predicts that FICO score near 800 indicate very low risk,
but the mimic model suggests that there is little difference in risk

Tan, Caruana, Hooker, & Lou

Density of Observations

Score − Outcome

Standardized Effects

xxx, xxx, xxx

Figure 2: Shape plots for four of six features for recidivism prediction. The remaining two features are included in the Appendix.
Top row: Red lines: effect of feature on COMPAS risk score. Green lines: standardized effect of feature on actual recidivism
outcome. Categorical terms ordered in decreasing predicted risk of the score. All plots mean-centered on the vertical axes to
allows terms to be easily added or subtracted from the model.
Middle row: Blue lines: difference between score and outcome models (score - outcome).
Bottom row: Density histogram of number of observations at each feature value.
between scores of 750 and 800. The black-box lending model appears
to be a smooth, simple function of FICO scores above 675 (and
possibly below 675, but the error bars are too large to be conclusive).

4.2.3 The models are qualitatively similar but quantitatively different on interest rate. On interest rate, the mimic model (red) suggests that the black-box lending model places significantly more
emphasis on interest rate than a model trained on the true labels.
Both models show a strong, linear increase in risk with interest
rate, but the slope is twice as high on the mimic model.

4.3

Are GAMs Successful Student Models?
Accuracy Comparison

The success of the model distillation approach hinges on whether
the student model is able to mimic its teacher with high fidelity. To
assess this, we compared the accuracy of interpretable GAMs in
predicting its teacher black-box risk score (Model M S ) to that of another class of transparent student models - logistic regression - and
a non-transparent student model - random forests. We also check
the prediction accuracy of the model of actual outcome (Model
MO ).
Our results are shown in Table 3. On an absolute scale, AUC
numbers in the range of 0.70 could be improved, however we note

xxx, xxx, xxx

Score and Outcome

Black-Box Risk Models Transparent Student-Teacher Distillation

Figure 3: Shape plots for four features for loan default prediction
that even powerful, non-transparent models such as random forests
could not exceed this number for the COMPAS data set. The same
applies to RMSE - RMSE of 2 on a 1-10 scale could also be improved,
however none of the three student models could improve on this.
Likely reasons why the COMPAS data set is challenging include
the lack of essential features from the public data set released by
ProPublica, and its smaller size compared to modern data sets.
On the other hand, if we look at these accuracy results on a relative scale, interpretable GAMs exceed linear and logistic regression
and random forests on all evaluation metrics.
On the Lending Club loan data, the AUC numbers are higher (in
the 0.80s range). Interestingly, here logistic regression performs
comparably to GAMs and random forests in predicting true outcome. These prediction accuracies can likely be improved with the
addition of post-baseline features, however since these features are
not available when a risk score is assigned, our proposed approach
advocates sacrificing some prediction accuracy by not including
them in order to equitably compare M S and MO .
In general, the GAM accuracies are comparable to that of complex, powerful random forests. This shows that GAM student models are competitive candidates for student models to mimic blackbox risk scores, but come imbued with the crucial transparency
needed in this transparent model distillation approach.

4.4

Are GAMs Successful Student Models?
Transparency Comparison

We now compare the estimated effects to that of linear and logistic
regression. For categorical features, GAMs are equivalent to logistic
regression. However, for continuous features, the difference between GAMs and logistic regression is significant. Consider Figure
4, which is the equivalent of the middle row of Figure 2. Where
the GAM model was able to shape the age feature in a non-linear
manner across the age range, and detected systematic bias in the
young and old ages, logistic regression ascribes only one number as
the effect of the age variable on the predicted outcome, and detected
no systematic bias between the risk score and outcome (bar graph
at y=0 in Figure 4. This example demonstrates that the use of GAMs
for bias detection is especially valuable for continuous features that
GAM is able to shape in interesting ways.

Figure 4: Logistic regression

5 DISCUSSION
5.1 Bias Discovery via Hypothesis Testing
One of the key advantages of using transparent models to understand bias in data, and bias in black-box models trained on that data,
is that you do not need to know in advance what biases to look
for. Examining the black-box model often shows bias that would
not have been anticipated in advance. For example, in addition to
the expected bias on race, the COMPAS recidivism model appears
also to be biased in favor of very young offenders age 18-19, and
against women. Once unexpected biases like these are discovered
by examining the transparent model, further testing can be done to
study the bias and determine its source. Not having to know what
to look for in advance is very useful because there are many kinds
of bias in real data, some of which we would not have known to
look for or design statistical tests to test for.

5.2

Using Excluded Variables to Detect Bias

Sometimes we are interested in detecting bias on variables that
have intentionally been excluded from the black-box model. For
example, a model trained for recidivism prediction or credit scoring

xxx, xxx, xxx

Tan, Caruana, Hooker, & Lou

Table 3: Test-set prediction accuracy (RMSE for risk scores, AUC and prediction accuracy for true outcomes) of M S and MO
across various model classes
Dataset
Compas

Lending Club

Label

Linear / Logistic Regression

GAM

Random Forest

Risk score (1-10)

RMSE: 2.09 ± 0.014

RMSE: 2.01 ± 0.0225

RMSE: 2.02 ± 0.019

True outcome

AUC: 0.74 ± 0.007
Acc: 0.68 ± 0.008

AUC: 0.74 ± 0.018
Acc: 0.69 ± 0.008

AUC: 0.73 ± 0.013
Acc: 0.68 ± 0.008

Risk score (0-6)

RMSE: 0.46 ± 0.002

RMSE: 0.25 ± 0.002

RMSE: 0.21 ± 0.005

True outcome

AUC:* 0.70 ± 0.006
Acc:* 0.85 ± 0.005

AUC: 0.71 ± 0.003
Acc: 0.85 ± 0.002

AUC: 0.69 ± 0.010
Acc: 0.85 ± 0.005

is probably not allowed to use race as an input to prevent the model
from learning to be racially biased. Unfortunately, excluding a
variable like race from the inputs does not prevent the model from
learning to be biased. Racial bias in a data set is likely to be in
the outcomes — the targets used for learning; removing the race
input race variable does not remove the bias from the targets. If
race was uncorrelated with all other variables (and combinations
of variables) provided to the model as inputs, then removing the
race variable would prevent the model from learning to be biased
because it would not have any input variables on which to model
the bias. Unfortunately, in any large, real-world data set, there is
massive correlation among the high-dimensional input variables,
and a model trained to predict recidivism or credit risk will learn to
be biased from the correlation between other input variables that
must remain in the model (e.g., income, education, employment) and
the excluded race variable because these other correlated variables
enable the model to more accurately predict the (biased) outcome,
recidivism or credit risk. Unfortunately, removing a variable like
race or gender does not prevent a model from learning to be biased.
Instead, removing protected variables like race or gender make it
harder to detect how the model is biased because the bias is now
spread in a complex way among all of the correlated variables,
and also makes correcting the bias more difficult because the bias
is now spread in a complex way through the model instead of
being localized to the protected race or gender variables. The main
benefit of removing a protected variable like race or gender from
the input of a machine learning model is that it allows the group
deploying the model to claim (incorrectly) that they model is not
biased because it did not use the protected variable.
When training a transparent student model to mimic a black-box
model like COMPAS, we intentionally include as an input variable
any protected variables that the COMPAS model excluded, but for
which we are interested in detecting bias. We are careful to include
both race and gender as input variables on the transparent student
mimic model, and on the transparent model trained to predict the
true outcomes, specifically because we are interested in examining
what the model learns from these variables. If, when the student
model mimic the COMPAS model, it does not see any signal on
the race or gender variable and learns to model them as flat (zero)
functions, this indicates whether the teacher model (the COMPAS
model) probably did or did not use these variables, but also if the
teacher model exhibits race or gender bias even if the model did not
use race or gender as inputs. In other words, by training transparent

models that use variables like race or gender, we can detect if there
is bias in the data by seeing how the transparent model uses these
variables. If the targets for the training data are scores from a blackbox model, then we can detect if that black-box model is biased,
even if that model did not itself use these variables.
In general, we find it is important when using transparent modeling to include as inputs all variables of interest, particularly those
which are of interest because of potential bias. Including those
variables makes it easier to detect the bias (and may also make it
easier to correct the bias).

5.3

Modeling Pairwise Interactions

It is also possible to include pairwise feature interactions into GAM,
leading to a more accurate model class called GA2M. Pairs are still
intelligible since they could be visualized as heatmaps. Including
pairs usually improves the model accuracy, but we do not pursue
this direction in this paper for two reasons. First, although the
same variance calculation methodology can be applied to pairs as
well, it is harder to visualize the confidence intervals for heatmaps.
Second, the accuracy gap between GAM and random forests is not
very large on our datasets. As random forests model higher order
feature interactions, we do not expect adding pairs to GAM can
significantly improve predictive power in this case.

5.4

Uniform Confidence Intervals

The intervals that we have produced here give are based on the
pointwise variance of the effects that we plot (alternatively, the
pointwise differences). That is, they give a measure of uncertainty
at each point along the x axis, and allow us to test hypotheses
at points such as age=60 or race=black. However, they are not
uniform in the sense of simultaneously capturing the label over the
whole of the range. We leave for future work the construction of
uniform confidence intervals. Uniform confidence intervals take
into account that nearby feature values are correlated by adjusting
the size of the confidence intervals. Specifically, this can be achieved
by changing the usual 1.96 standard deviations (the critical value
from a standard normal distribution) to a larger multiple that takes
into the account this covariance. This work is in progress.

5.5

Problems With the German Credit Data

Although we do not include the results in this paper, we also applied
transparent models to to German Credit Data Set [15] which has
been used by others examining bias in predictive modeling. To our

Black-Box Risk Models Transparent Student-Teacher Distillation
surprise, when we examined the term in the transparent model
for the variable that might lead to bias (native German vs. foreign
national), we noticed that the error bars for the native German
category were much larger than those for foreign nationals. A quick
examination of the data showed that the data comprises 99% foreign
nationals, and only a handful of German nationals, suggesting that
the data set is drawn from a very skewed distribution and probably
should not be used for studies of potential bias. We mention this
result because it shows that transparent models sometimes provide
insight into a data set that would bring into question how that data
should or should not be used.

6

CONCLUSION

We propose a method to detect bias in black-box risk models by
using model distillation to train a transparent student model to
mimic the black-box model, and then comparing the transparent
mimic model to a transparent model trained using the same features
on true outcomes instead of the labels predicted by the black-box
model. Differences between the transparent mimic and true-labels
model indicate differences between how the black-box model makes
predictions, and how a model trained on the true outcomes makes
predictions, highlighting potential biases in the black-box model.
We demonstrate this method on two data sets and uncover a number
of interesting differences (potential biases). The key advantages of
this approach are that the transparent models are very accurate
despite being intelligible, the method generates reliable confidence
intervals to aid interpretation, and one does not to know in advance
what biases to look for.

REFERENCES
[1] Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos Eduardo
Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. 2016. Auditing
Black-Box Models for Indirect Influence. In IEEE 16th International Conference
on Data Mining. 1–10.
[2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. How we
analyzed the compas recidivism algorithm. (2016). https://www.propublica.org/
article/howwe-analyzed-the-compas-recidivism-algorithm Accessed May 26,
2017.
[3] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine
Bias: There’s software used across the country to predict future criminals.
And it’s biased against blacks. (2016). https://www.propublica.org/article/
machine-bias-risk-assessments-in-criminal-sentencing Accessed May 26, 2017.
[4] Susan Athey, Julie Tibshirani, and Stefan Wager. 2017. Generalized Random
Forests. arXiv preprint arXiv:1610.01271 (2017).
[5] Thomas Blomberg, William Bales, Karen Mann, Ryan Meldrum, and Joe Nedelec.
2010. Validation of the COMPAS risk assessment classification instrument.
College of Criminology and Criminal Justice, Florida State University, Tallahassee,
FL (2010).
[6] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model
compression. In Proceedings of the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 535–541.
[7] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie
Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and
hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 1721–1730.
[8] Alexandra Chouldechova. 2017. Fair prediction with disparate impact: A study
of bias in recidivism prediction instruments. arXiv preprint arXiv:1703.00056
(2017).
[9] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq.
2017. Algorithmic decision making and the cost of fairness. arXiv preprint
arXiv:1701.08230 (2017).
[10] William Dieterich, Christina Mendoza, and Tim Brennan. 2016. COMPAS
risk scales: Demonstrating accuracy equity and predictive parity. Technical Report. http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica
Commentary Final 070616.pdf

xxx, xxx, xxx
[11] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and
Suresh Venkatasubramanian. 2015. Certifying and Removing Disparate Impact.
In Proceedings of the 21st ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. 259–268.
[12] Robert D Gibbons, Giles Hooker, Matthew D Finkelman, David J Weiss, Paul A
Pilkonis, Ellen Frank, Tara Moore, and David J Kupfer. 2013. The computerized
adaptive diagnostic test for major depressive disorder (CAD-MDD): a screening
tool for depression. The Journal of clinical psychiatry 74, 7 (2013), 1–478.
[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014. Distilling the Knowledge
in a Neural Network. In Deep Learning and Representation Learning Workshop:
NIPS 2014.
[14] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2017. Inherent tradeoffs in the fair determination of risk scores. In Proceedings of the 8th Conference
on Innovations in Theoretical Computer Science.
[15] M. Lichman. 2013. UCI Machine Learning Repository. (2013). http://archive.ics.
uci.edu/ml
[16] Yin Lou, Rich Caruana, and Johannes Gehrke. 2012. Intelligible models for
classification and regression. In Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 150–158.
[17] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. Accurate
intelligible models with pairwise interactions. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,
623–631.
[18] Francisco Louzada, Anderson Ara, and Guilherme B Fernandes. 2016. Classification methods applied to credit scoring: Systematic review and overall comparison.
Surveys in Operations Research and Management Science (2016).
[19] Edward Neukrug and R Fawcett. 2014. Essentials of testing and assessment: A
practical guide for counselors, social workers, and psychologists. Nelson Education.
[20] Avi Feller Sam Corbett-Davies, Emma Pierson and Sharad Goel. 2016.
(2016). https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/
can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/
?utm term=.bf4ccc27056a Accessed May 26, 2017.
[21] Joseph Sexton and Petter Laake. 2009. Standard Errors for Bagged and Random
Forest Estimators. Computational Statistics and Data Analysis 53, 3 (2009), 801–
811.
[22] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P
Gummadi. 2017. Fairness Beyond Disparate Treatment & Disparate Impact:
Learning Classification without Disparate Mistreatment. In Proceedings of the
26th International Conference on World Wide Web. ACM, 1171–1180.
[23] Jiaming Zeng, Berk Ustun, and Cynthia Rudin. 2016. Interpretable classification
models for recidivism prediction. Journal of the Royal Statistical Society: Series A
(Statistics in Society) (2016).

A

ADDITIONAL FIGURES

Tan, Caruana, Hooker, & Lou

Density of Observations

Score − Outcome

Score and Outcome

xxx, xxx, xxx

Figure 5: Additional features in recidivism risk data. See caption in Figure 2.

