Biases in Retrospective Self-Reports of Time Use: An Empirical Study of Computer Users
Author(s): Fred Collopy
Source: Management Science, Vol. 42, No. 5 (May, 1996), pp. 758-767
Published by: INFORMS
Stable URL: https://www.jstor.org/stable/2634463
Accessed: 09-03-2019 15:40 UTC
REFERENCES
Linked references are available on JSTOR for this article:
https://www.jstor.org/stable/2634463?seq=1&cid=pdf-reference#references_tab_contents
You may need to log in to JSTOR to access the linked references.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide
range of content in a trusted digital archive. We use information technology and tools to increase productivity and
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at
https://about.jstor.org/terms

INFORMS is collaborating with JSTOR to digitize, preserve and extend access to Management
Science

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

Biases in Retrospective Self-reports of Time
Use: An Empirical Study of Computer Users
Fred Collopy
The Weatherhead School of Management, Case Western Reserve University, Cleveland, Ohio 44106-7235

R esearch on information systems (IS) use has often relied upon retrospective self-reports.
One example is when the amount of time spent using a system is reported, often as an

indication of user acceptance. In this study, self-assessments of computer usage are compared

with computer-monitored interactive use and connect time for 401 managers and professionals.
When self-assessed use was compared with logged interactive use, there was a 32% difference

in the average amount of use (3.9 vs. 2.7 hours/day). When the self-assessment was compared
with total connect time, the averages were similar to each other (3.9 vs. 4.0 hours). In both
comparisons, though, there were considerable differences between individual self-assessments

and logged time (with a median absolute percentage difference of 47% when compared with
logged connect time). Individual estimates of use tended to regress toward the population's
mean use. Those whose use of the system was relatively light overestimated their use, while
heavy users underestimated theirs. When a test of the relationship between user satisfaction and

amount of use was conducted using self-assessments, the relationship was not statistically significant. When the same test was conducted using logged connect time, the relationship was

significant. These results suggest that care should be exercised in operationalizing the user acceptance construct through self-reports of time use.

(End-user Computing; Information Systems; Managerial Computing; Research Methods; Response Bias;

System Monitors; System Usage; Time Study)

1. Introduction

There are several motivations for doing so. First, time

How many hours each day do you spend working at

serves as a proxy for the degree of acceptance of a sys-

your computer? Questions like this are often asked in

tem or for satisfaction with the system. It is also some-

is a frequently used construct in IS research. Often, it

studies of information systems (IS) use. Similar ques-

times of interest for its own sake-e.g., to enable re-

tions about time are asked in other areas of management

source planning. Further, it is possible to compare self-

science as well as in psychological research. How con-

reports of time with an external or objective benchmark.

fident should we be of the answers given by subjects?

Not all self-assessments are subject to such clear com-

Critical reviews in psychology (Furnham 1986), organi-

parisons, so studying self-assessments of time could

zational research (Podsakoff and Organ 1986), anthro-

provide insights into self-assessments in general. Fi-

pology (Bernard et al. 1984), and information systems

nally, researchers in other areas, such as marketing and

(Hufnagel and Conca 1994) suggest that there is wide-

organizational behavior, also have an interest in time.

spread concern about the use of retrospective self-

Results from IS research, where computers provide a

reports by researchers in the social sciences.

unique opportunity for measuring time, may provide

In this paper, I examine how an individual's self-

help in devising better theory and operationalizing ap-

assessment of the amount of time spent using a com-

propriate research constructs. By permitting observa-

puter system compares to logged measures of use.

tion of large numbers of subjects under a wide variety
0025-1909/96/4205/0758$01.25
Copyright ? 1996, Institute for Operations Research

758 MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996 and the Management Sciences

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

COLLOPY

Biases in Retrospective Self-reports of Time Use

of conditions, computers make it possible to develop

use "can be taken as a behavioral indicator of values

very detailed appreciations of how people allocate time.

and preferences" (Robinson 1988, p. 134).

The next section of the paper examines the validity of

Some IS researchers have expressed concern that self-

self-assessments of time. The third section discusses em-

assessments of computer use are not valid. Vitalari et

pirical evidence. The fourth and fifth sections describe a

al. (1985), for example, specifically avoided using self-

field study designed to examine biases in self-assessments

reports due to "measurement problems" associated

of time. The discussion section presents limitations and

with them. Others have expressed reservations about

issues for research and recommendations.

having employed perceptual measures, sometimes sug-

gesting that it would be better to rely upon more objec-

2. The Validity of Self-assessments

of Time

tive measures (e.g., Thompson et al. 1991, Pentland
1989, Robey 1979). Rice (1990), on the other hand, has

cautioned that "while computer-monitored data are

In IS research there has been little discussion about pre-

empirically more reliable measures of system usage than

cisely what is being assessed when people are asked to

are self-reported data, diaries, and observations, they

estimate the amount of time they spend using a com-

are not necessarily more valid" (p. 641). In any case,

puter. There is some recognition that different measures

subjective assessments of time are widely used in IS re-

of use might have different meanings and produce dif-

search. Melone (1990) suggested that the preference for

ferent results. Rice and Shook (1988) observed that

perceptual rather than objective measures of use is ex-

computer-monitored measures may be conceptually

plained by several things. These include constraints im-

more valid than self-assessments if they measure spe-

posed by site policies, the unavailability of monitoring

cific things, but that the two types of measures may be

software capable of measuring use at the level of detail

valid representations of different aspects of usage. In a

required by the researcher, and potential influences of

meta-analysis of 80 samples in nearly 40 studies of intra-

direct measurement on user behavior.

organizational media use, Rice and Shook (1990) found

Hufnagel and Conca (1994) summarized potential

that the method of data collection (observation, diary,

threats to the validity of self-assessed measures in the

multiple methods, self-report) was a significant source

context of IS research. Of particular relevance to self-

of variation in studies of time spent using various com-

assessments of time are the threats related to categori-

munication channels. Most researchers, however, have

zation. These include the effects of question wording,

not identified very explicitly the aspect of use that is of

the impact of frequency expressions (such as "Usually,"

interest to them. Indeed, use seems generally to function

"Sometimes," and "Rarely"), the relevance and avail-

as a way of assessing something else that is of interest

ability of instances to the respondent, and lack of knowl-

to the researchers.

edge.

Time estimates of computer use have been employed

There is much variation in the wording of questions

to assess the extent of computerization (Gutek et al.

used to assess the amount of time spent using comput-

1992, Igbaria et al. 1989, Hackathorn 1987, Laudon 1986,

ers. Of the questions used in ten studies for which it

Srinivasan 1985); the extent to which computers are in-

was possible to get the exact wording, about half used

tegrated into the work patterns of respondents (Winter

open-ended questions while the others used scales.

1993, Pentland 1989); how effectively computers com-

Each of the scales was a little different from the others.

pare to traditional information channels (Vandenbosch

Only two of the studies used items from previous stud-

and Higgins 1992); and the extent to which social fac-

ies. Even small differences in wording might affect re-

tors, affect, and perceived consequences influence peo-

sponses. For example, Winter (1993) asked "On average

ple's decisions about computers (Thompson et al. 1991;

how many hours a week do you spend working on a

Howard and Mendelow 1991). User acceptance of com-

computer for your job?" Pentland (1989) asked "How

puters seems to be implicit to some extent in all of these.
many hours per week do you currently use the laptop?"
This seems an appropriate use of time as a research con-

Respondents may well think differently about working

struct because, when it reflects people's choices, time

on a computer and using a computer.

MANAGEMENT

SCIENCE/Vol.

42,

No.

5,

May

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

1996

759

COLLOPY

Biases in Retrospective Self-reports of Time Use

Frequency expressions are particularly prone to mis-

interpretation. Schriesheim and Schriesheim (1974)
found, for example, that people tended to select the

same position on frequency scales regardless of the ver-

bal cues associated with that position or! the scale. The
positions may themselves establish expectations or anchors from which subjects make adjustments by comparing their own behavior with what they see as the
norm. Scales are frequently used to assess the amount

of time spent using a system, and positions on the scales
might imply norms. The scales used by Winter (1993)
and Pentland (1989) are illustrative. One had a top item

of "more than 40" hours; the other a top item of "20 or
more."
It is possible that some respondents will remember

particularly difficult days and treat them as typical, or
that a few recent days will be taken as representative.
This problem is greater when the question allows for

ambiguity in interpreting the time frame.
Finally, there may be variations in the knowledge of

people about the amount of time they spend using a
computer. Some subjects may have a very good sense
of the time they spend using a system (if, for example,
they use it for a well defined set of tasks of known du-

ration). Those with irregular usage patterns might
know less about their patterns of use. Those who use

multiple similar systems might also have more trouble
in recalling the use of a particular system. Also, people's

3. Empirical Evidence Related to

Self-assessments of Time
Two kinds of biases are evident in comparative studies

of self-reports and objective measures. The first has to
do with a tendency of groups as a whole to adjust es-

timates in the socially desirable direction (response
bias). In addition, the amount of the bias sometimes
varies based upon the characteristics of the situation
(structural bias). A few studies suggest that differences
between self-assessments and objective measures of

time spent using a technology may be related to the
amount of use. Light users seem to estimate differently
from heavy users.

A general finding regarding retrospective self-reports

has been that deviations tend to be in the direction of

cultural norms (see Bernard, et al. 1984, pp. 508 ff.). For
work-related tasks, there seems to be a tendency for
people to overstate the amount of time they spend at an
activity.

Hartley et al. (1977) compared estimates by 13 office
workers with observed amounts of time involved in

doing each of 23 activities. Two of eight items for
which there was a significant correlation between the

subjects' estimates and observations had a directional
bias. All of the subjects overestimated the time they
spent typing and 11 of the 13 overestimated the time
they spent on the telephone. In a study of the link be-

general sense of time might vary, with some having a

tween computer system use and productivity of ac-

good feel for the passage of clock time and others hav-

counting professionals from the Internal Revenue Ser-

ing little feel for it (see the discussion in Jones 1988, pp.

vice, Pentland (1989) indirectly compared self-reported
and computer-generated measures of the frequency

26 ff.).
All of these can create biases in self-assessments of

with which professionals used various laptop computer

time, so that even if time represents the construct of

facilities. The two measures did not come from the same

interest, and if the self-assessments are reliable, they

users, but summary results for two populations of sim-

may lack validity. How can we determine if such

ilar users were compared. Pentland concluded that re-

threats do in fact invalidate self-assessments of time in

spondents tended to overstate the extent to which they

IS and similar research? One way is by comparing self-

used the system. He attributed this bias to social desir-

assessments with objective measures.'

ability. Robinson (1985) compared retrospective selfreports to other measures, including beepers and time
diaries. He concluded from four studies that respondents using retrospective self-reports tended to overes-

1 The alternative measures are objective in the sense that they are captured by some external source, rather than being provided by the subjects. They are, of course, still subjective in the sense that the researcher

timate their activity times, particularly when asked

about a single specific activity or about secondary activ-

has determined how they are collected, what they mean, and how they

ities (activities that are in support of another activity).

relate to the research construct of interest.

Even in a study of television viewing, subjects tended

760 MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

COLLOPY

Biases in Retrospective Self-reports of Tinie Use

to overreport time spent. Robinson (1972) compared

electronic messaging system. The measures included

Neilson-type viewing logs with television viewing be-

logged minutes of connect time on a mainframe and

havior that was monitored on videotape. He found that

reported usage of the electronic messaging component

subjects reported 25 percent more viewing than was ob-

of PROFS. They found that reported and monitored

served on the tapes.

measures shared only about 20 percent of their variance.

If there were consistent response biases across the

In general, they found that reported measures were not

population of users-that is, if they consistently over-

associated with as many outcomes as logged measures,

estimated or underestimated-their effects on research

nor were they as strongly associated with the same out-

might be minimal. Such estimates could be adjusted by

comes. In a study of farm operators who used a video-

statistical means. In their study of office workers, Hart-

text system to retrieve commodity price quotations, cor-

ley et al. (1977) found that the overall mean absolute

relations that were evident when using logged mea-

percentage difference between observed and estimated

sures were not evident when self-reports were used in

amounts of time was 24.4%, "a value so large that it

the analysis (Ettema 1985). The self-reports tended to

indicates, in our opinion, an unsatisfactory ability on the

mask the impact of demographics on frequency of use.

part of subjects to judge the amount of time they spend

Further, those who were most interested in agricultural

in various activities" (p. 32). But for most of the activi-

market information tended to overestimate their use of

ties they examined, they did not find a systematic re-

the system.

sponse bias. Rather, they concluded that these inaccu-

In a study of electronic messaging systems (EMS) use

racies were related more to response variability than to

in an R&D organization, Rice et al. (1989) found signif-

a systematic response bias. Other empirical compari-

icant discrepancies between a self-reported usage mea-

sons of self-assessed and objective measures also sug-

sure (minutes/day) and computer-monitored usage

gest that there may be systematic biases beyond any

measures (number of messages sent and received).

response biases in self-assessments.

They suggested that "we must consider the possibility,

What can be said about the nature of the structural

intimated by the lack of significant effect of the

biases that occur in self-reports of use? Most compari-

computer-monitored variables, that when we do use

sons have not attempted to explicitly characterize the

more reliable and valid measures, we may find less ev-

biases. One exception is a study in which diaries of

idence of the generalized outcomes associated with use

phone calls made by 354 subjects were compared with

of EMS than the literature to date implies" (p. 154).

information monitored at their exchanges (Hyett 1979).

Taken together, these studies suggest that there are

One would expect that respondents might underreport

biases in self-reports of time use. In a work context,

telephone use (since they might be concerned that the

these biases have a tendency to inflate the amount of

amount of use reported to the service provider would

activity reported. In addition, there is variation in the

affect charges). Diaries did underreport by an average

biases of individual respondents. One study suggests

of 15 percent. But callers who made fewer than six calls

that this variation may produce a regression to the

were more likely to overreport than to underreport.

mean; others suggest that structural biases exist, but do

Hyett concluded that "biases in diary records appear to

not characterize them. The next section discusses a field

be associated with the number of calls made: it is the

study designed to examine and further characterize

relatively high call-makers who tend to under-record

these biases.

and the relatively low call-makers who tend to over-

record" (p. 138). There is, in other words, a regression
to the mean effect in the reports. Though the effect is

4. Field Study Design

not strong and this is the only direct comparison, other

The field study was one of several conducted at IBM

studies report results that are consistent with such a re-

sites (see Collopy 1988, Durand et al. 1990) to better

gression bias.

understand how IBM's office systems were being used.

Rice and Shook (1988) used both self-reported and

In addition to logging system use automatically, partic-

system-monitored measures of usage in a study of an

ipants were asked to estimate their system use. Most of

MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996 761

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

COLLOPY

Biases in Retrospective Self-reports of Time Use

the 420 employees at the site were systems analysts and

librarians, and editors. "Administrative personnel" in-

programmers who operated and maintained several

cludes specialists involved in security, procedures, fa-

large computer systems that supported the IS require-

cility maintenance, and secretaries.

ments of IBM facilities over a three state region. All of

Respondents were asked for each of twelve items to

the managers, professionals, and clerical workers at the

indicate "On a scale of 1 (extremely significant) to 7 (not

site were asked to participate in a study of office systems

significant) how important is each of the following in
successfully doing your job?" All groups considered

computer use.

A description of the study population's job categories,

communicating and information gathering quite im-

attitudes, and logged computer behaviors is presented

portant in doing their jobs. Planning, scheduling, and

in Table 1. The site was unusual in that there was only

decision-making were also considered fairly important

one manager at a level senior enough to be classified as

across all four groups. For the seven other activities

an executive (in e.g., Durand et al. 1990, Collopy 1988,

there were statistically significant differences among the

Hackathorn 1987, Laudon 1986). He is included in the

groups (using the Kruskal-Wallis test). Programming

"managers, planners, and business analysts" group.

and analysis were, not surprisingly, more important to

Computer system operators are included in the "tech-

programmers and analysts than to the others. Admin-

nical support personnel" group along with engineers,

istrative personnel considered advising/consulting of

Table 1 Characteristics of the Study Population

Managers, Planners, Programmers Tech Support Administrative

& Bus Analysts & Sys Analysts Personnel Personnel Entire Population
(45)
Percent

Years

Males

of

57.8

Service

Importance
extremely

for

(274)
70.1

13.7

63.2

10.7

your

job

significant;

Communicating
Information

Planning

1

=

3

3

3

4

Programming"*
Applications

Document

Communication
Calendar

(%

Preparation

70.4

19.6

Management

Analysis

&

Other

58.0

7.5

2.4

7

3

3

5

3

5

4

3

6

4

4

7

6

7

4

5
5

computer

60.9

33.5
3.3

3.7

3

4

3

6

4

of

35.0

3

5

3

4

3
4

2
2

3

3

5

=

1

3

4

3

3

1

1

2

4

3

Education/Training**

Forecasting*

2

3

3

4

10.4

significant

2.5

Presentations*

Researching

8.0

1.5
2

Advising/Consulting**
Preparing

65.1

2

3

Making

43.2

not

1

3

3

(401)

(medians;

7

2

Scheduling

(44)

6.4

Gathering

Analyzing***

Decision

(38)

60.7

23.6

2.0
3.6

60.0
31.9

13.1
2.6

4.7
3.4

* p < 0.0001.

p < 0.01.

* p ? 0.1.

762 MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

t

COLLOPY

Biases in Retrospective Self-reports of Time Use

less importance than did other groups and for the other

age, how much time do you spend each day actually

activities there was a range across the groups. The fact

interacting with the VM3 system?" Questionnaires were

that the groups varied somewhat contributes to the po-

completed by 401 of the 420 employees at the site, a

tential generalizability of any findings.

response rate of 95%. Keisler and Sproull (1986) found

Several VM (mainframe) systems were in use at the

that responses to close-ended questions on computer-

site, running an interactive operating system called the

administered questionnaires are less affected by social

Conversational Monitor System (CMS). One of these,

desirability than those on written questionnaires. This

VM3, was used to support electronic mail, calendars,

means that the self-assessments in this study may be

word processing, presentation graphics, analytical

less subject to a response bias than surveys that have

tools, and other widely-used general purpose software.

been done on paper, as has been more common in IS

IBM's global communications network was also ac-

research. Computer-administered questionnaires also

cessed through VM3. Virtually everyone at the site had

have higher response variance than written question-

at least one terminal or personal computer that could

naires (Sproull 1986). Again, this suggests that results

directly access VM3. The only site services that were not

in the current study might understate differences be-

delivered on VM3 were some related to product devel-

tween self-assessments and logged measures relative to

opment. It was explained that as part of the computer

studies using written questionnaires.

usage study all use of VM3 by study participants would

The measure of bias used is the difference between

be automatically logged but that no information about

the self-assessment and logged use. Percentage differ-

the content of work being done would be collected.

ences in self-assessments are also reported. In addition,

The CMS Command Monitor/Analysis Package

absolute percentage differences are reported to sum-

(CMAP) was used to gather data about usage behavior.

marize the magnitude of individual differences (with-

CMAP records commands executed, programs run, and

out respect to direction).

related resource utilization data. Use was logged over
three months during the first quarter of 1988. During

that time, each command issued, program run, and operating facility used was automatically logged by the
monitor, along with information about the duration of

5. Results
5.1. Comparison of Self-Assessments and

use and the amount of resources utilized. Participants
were logged an average of 53 days.

Interactive Use

The mean daily self-assessed use was 3.9 hours while

Rice and Shook (1988) concluded that "even computer-

logged interactive use was 2.7 hours. A test of the

monitored log-ons and connect time do not tell much

difference between the two means was significant

about how the system is really used, what functions are

(t = 11.16; p < 0.0001). As a group, users overstated

involved, and the extent to which a user is actively us-

their use of the system by about 32%. This is a bit more

ing the system" (p. 273). Because we were primarily

than the 25% observed by Robinson (1972) for television

interested in assessing the extent to which users were

viewers. The differences in individual assessments were

actively using the system, we defined the measure 'in-

somewhat greater. The median absolute percentage dif-

teractive use' to exclude any period of ninety seconds

ference for the entire population was 69%. Comparisons

or more in which there was no user interaction. Inter-

for each of the groups are presented in the first half of

active use of the various office systems applications was

Table 2. A linear regression relating bias to logged use

classified as supporting one of four activities: document

had a statistically significant negative slope coefficient

preparation, communication, calendar management,

(b = -0.73; p < 0.0001). This regression bias is similar

and analysis. Connect time was also logged.

to what Hyett (1979) saw in subjects' diaries of phone

Near the end of the study period, participants were

calls. Those who used the system little were more likely

asked to complete a brief questionnaire. It was pre-

to overestimate their use than to underestimate it, while

sented in place of the normal greeting screen as they

those who used it more were more likely to underesti-

logged onto VM3. One of the questions was "On aver-

mate.

MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996 763

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

COLLOPY

Biases in Retrospective Self-reports of Time Use

Table 2 Comparisons of Self-Assessments and Objective Measures
Managers, Planners, Programmers & Sys Tech Support Administrative
& Bus Analysts Analysts Personnel Personnel Entire Population

(45)
Self-assessed
Logged
Md
Md

Md

active

Md
Md

time

30.81

57.20

43.32

(hrs/day)

percentage
abs

-0.24

diff

percentage

2.73

1.16

diff

difference

3.05

3.89

0.78

diff

percentage

(38)

4.03

(hrs/day)

percentage

Connect

Md

(hrs/day)

difference

abs

(274)

-8.02

diff

-0.25

33.80

5.2. Comparison of Self-Assessments and Connect

4.48

2.30

2.06

57.68

2.65

1.24

119.68

58.29

119.68

4.13

-0.31

3.39

1.13

-21.24

42.88

3.89

2.35

71.01

4.06

-0.01

(401)

3.04

1.12

67.90

3.95

(44)

66.26

49.22
65.08

69.21
3.98
0.16
5.59
46.57

estimate connect time, and if on average they were able
to do a good job of that, the individual estimates were

Time

One possible explanation for these biases in user self-

still quite different from the logged measures of connect

assessments is that users estimated their connect time

time.

rather than the time they were interacting with the com-

Though the means of self-assessments and connect

puter. The second half of Table 2 suggests that this may

time were close, there was still a significant regression

well have been the case. The mean self-assessments for

effect. A linear regression relating bias to the logged

three of the groups, as well as that for the entire popu-

measures again had a statistically significant negative

lation, are close to the mean connect times. There is a

slope (b = -0.89; p < 0.0001).2

statistically significant difference between the means (t

This regression effect in the biases appears to be quite

= 2.06; p = 0.05) only for the administrative personnel,

strong and applies to all of the groups. As illustrated in

and the average difference across the population is quite

Table 1 the groups differ in what activities they consider

close to zero.

important to their work. They also differ in how they

The ability of the population as a whole to closely

made use of VM3. However, in all of the groups, light

estimate its usage echoes an effect seen in Bernard et al.

users overestimated use, while heavy users underesti-

(1982). They found that while informants did not know

mated it.

with whom they communicated, the informants "en
masse seemed to know certain broad facts about the

communication pattern" (p. 62). In particular, the ag-

5.3. Explanatory Usefulness of Self-Assessments vs.
Logged Connect Time

gregate of what everybody said about their communi-

Studies that have compared self-assessments and

cation with everybody else produced an accurate listing

logged measures of use have sometimes found that the

of the top six people involved in communication.

latter provide more statistically significant relationships

Even though the means of the populations' self-

assessments were quite accurate when compared with
the means of their connect times, the differences in individual assessments were still quite large. The median
absolute percentage difference was almost 47%. Members of all of the groups exhibited large differences,

ranging from 34% to 66%. So if people were trying to

2 Because the distributions of the data are skewed, I also analyzed
transformed versions. Using square roots reduced the skewness of the
connect time data from 2.1 to .38 and that of the self-assessments from
0.29 to -0.26. Using these transformed data, the slope remained significant (p < 0.0001).

764 MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

COLLOPY

Biases in Retrospective Self-reports of Time Use

than the former (Rice and Shook 1988, Ettema 1985).
The regression effect could explain this.

superior replacement for self-reported measures. At the

same time, there is concern that retrospective self-

In the field study, subjects were asked: "On a scale of

reports may contain so much bias as to be nearly useless

1 (very satisfied) to 7 (very dissatisfied) rate the services

(Hartley et al. 1977, Bernard et al. 1984). Computer-

available on VM3." The correlation between satisfaction

monitored data might permit us to more systematically

and connect time was significant (r = 0.14; p < 0.01),3

examine such assessments, but only if we identify mea-

while that between satisfaction and self-assessed use

sures that are convincingly comparable.

was not (r = -0.02; n.s.). The relationship was negative

I am reminded of Meehl's (1986) observation that

in the sense that heavier users tended to be less satisfied

"when you check out at a supermarket, you don't eye-

with services. Similar negative relationships have been

ball the heap of purchases and say to the clerk, 'Well it

observed previously. For example, Weisband (1987)

looks to me as if it's about $17.00 worth; what do you

found that those who used advanced functions of an

think?' The clerk adds it up" (p. 372). In many situa-

executive information system were less satisfied than

tions, this is likely to be true of time as well. Nonethe-

those who used only simple features. Chung and Iacono

less, time perception is a complex construct, and con-

(1994) also found that those who used more features of

sideration should be given in IS research to such issues

software products were less satisfied with the products.

as the subjective nature of time (McGrath and Kelly

In the case of respondents in the current study, users

1986, pp. 67 ff.; Hornik 1984; Graham 1981), the fact that

who are most dependent upon the system may also be

people often do more than one thing at a time (Kaufman

most critical.

et al. 1991, Hall 1983), and the fact that, in general, people's recollections decay fairly rapidly (Bernard et al.

1984). For the purposes of IS research, further thought

6. Discussion
Because the self-assessment procedure used in this
study followed practices like those used in prior IS stud-

ies, it suffers from some of the same problems as those
used in other studies. It was administered at one point

in time, used a single item, and did not clearly specify
the time frame. The study was conducted at an IBM
facility where it is possible that social desirability
played a larger role than it might at other places (e.g.,
computer use may not be socially desirable in all environments). Finally, some of the subjects in the study
population (mostly programmers) also used other computer systems. It is possible that this interfered with
their ability to accurately estimate their use of VM3, but

this probably happens when self-assessments are used
in other IS research as well.

There are at least two points of view regarding what

can be learned by comparing computer-monitored and
self-assessed measures of use. One, argued convincingly by Rice and Shook (1988) and by Rice (1990), is
that computer-monitored data should be viewed as an

addition to a multi-method approach rather than as a

should also be given to the research construct of interest. Perhaps different assessments of time will be required for different purposes. Hesse et al. (1988) con-

sidered some of these issues specifically in the context
of computer-mediated communication, applying a
transactional view that sees time as intrinsic to the
events of interest.
To more exactly characterize the nature of the struc-

tural biases in self-assessments of time use, it will be
necessary to conduct studies in which the questions

used in the self-assessments more exactly reflect what
can be logged using computer monitors. This will require controlling such factors as the applications being

monitored, the time period being estimated, and the relationship to other tasks being performed. The current

results suggest that norms might play a critical role in
self-assessments, and this possibility should be controlled for in future research designs.
Though the current results reinforce concerns about
the validity of self-reports, there may be some things
that researchers can do to improve the accuracy of their
measures. If overall assessments of the amount of use
are of interest, it may be possible to get reasonably good

3When the square root transformation was used the correlation was

estimates for certain populations when their estimates

0.10 (p < 0.05).

are averaged. Hufnagel and Conca (1994, pp. 50 ff.) and

MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996 765

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

COLLOPY

Biases in Retrospective Self-reports of Time Use

Sudman and Bradburn (1982) make some suggestions

Collopy, F., "White Collar Computing: A Field Study Using Auto-

for reducing the random error and response biases in

mated Logging," Proc. Twenty-First Annual Hawaii International

survey data. Further studies will be required to determine the conditions under which structural biases are
likely to have significant impacts.

Conference on System Sci. (Ralph H. Sprague, Jr., Ed.), 21 (1988),
Vol. IV, 236-244.

Durand, D., S. Floyd and S. Kublanow, "How Do 'Real' Managers Use

Office Systems?," J. Information Technology Management, 1 (1990),
25-32.

Ettema, J. S., "Explaining Information Systems Use with System-

Monitored vs. Self-Reported Use Measures," Public Opinion Quar-

7. Conclusions
Large differences were found when self-assessments

of computer use were compared with computermonitored measures. Though the population as a whole
produced summary assessments of use that were quite

consistent with logged measures of connect time, individual assessments had an average difference in excess

of 46%. If one interprets the question as asking people
to estimate the amount of time they spend actively using the system, the differences are even greater. All of
the groups demonstrated these differences.

The self-assessments tended to regress to the mean
logged usage, with those who used the system relatively

little tending to overestimate their use, while those who
used the system a great deal tended to underestimate

their use. Connect times were significantly related to

terly, 49 (1985), 381-387.

Furnham, A., "Response Bias, Social Desirability and Dissimulation,"
Personal Individual Differences, 7 (1986), 385-400.
Graham, R. J., "The Role of Perception of Time in Consumer Research," J. Consumer Res., 7 (1981), 335-342.
Gutek, B. A., S. J. Winter and K. M. Chudoba, "Attitudes Towards
Computers: When Do They Predict Computer Use?," Academy of
Management Best Papers Proceedings, 1992, 253-257.

Hackathorn, Richard D., "End-User Computing by Top Executives,"
Data Base, 19 (1987), 1-9.

Hall, E. T., The Dance of Life: The Other Dimension of Time, Anchor
Press/Doubleday, New York, 1983.
Hartley, C., M. Brecht, P. Pagerey, G. Weeks, A. Chapanis and D.

Hoecker, "Subjective Time Estimates of Work Tasks by Office

Workers," J. Occup. Psychol., 50 (1977), 23-36.
Hesse, B. W., C. M. Werner and I. Altman, "Temporal Aspects of
Computer-Mediated Communication," Computers in Human Behavior, 4 (1988), 147-165.

satisfaction with the system, while self-assessments

Hornick, J., "Subjective vs. Objective Time Measures: A Note on the

were not. This pattern of results is consistent with ear-

Perception of Time in Consumer Behavior," J. Consumer Res., 11

lier studies and may help to explain method-dependent
differences that have been observed in the relationship
of other research variables to use. Until further comparative studies more clearly identify the conditions un-

(1984), 615-618.
Howard, G. S. and A. L. Mendelow, "Discretionary Use of Computers:
An Empirically Derived Explanatory Model," Decision Sci., 22
(1991), 241-265.

Hufnagel, E. M. and C. Conca, "User Response Data: The Potential for

der which self-assessments of time adequately represent

Errors and Biases," Information Systems Res., 5 (1994), 48-73.

the research construct of interest, I think that such self-

Hyett, G. P., "Validation of Diary Records of Telephone Calling Be-

assessments should be used cautiously.4

havior," The Recall Method in Social Surveys, Univ. of London In-

stitute of Education, London, England, 1979, 136-138.
Igbaria, M., F. N. Pavri and S. L. Huff, "Microcomputer Applications:

4Many people have commented on previous versions of this paper. An
I Empirical Look at Usage," Information and Management, 16
especially wish to thank Richard Boland, James Emery, Brian Pent-

land, Betty Vandenbosch, the associate editor, and three anonymous

(1989), 187-196.

Jones, J. M., "Cultural Differences in Temporal Perspectives: Instru-

reviewers.

mental and Expressive Behaviors in Time," in J. E. McGrath (Ed.),

References

CA, 1988, 21-38.

The Social Psychology of Time, Sage Publications, Newbury Park,

Bernard, H. R., P. Killworth, D. Kronenfeld and L. Sailer, "The Prob-

Kaufman, C. F., P. M. Lane and J. D. Lindquist, "Exploring More than

lem of Informant Accuracy: The Validity of Retrospective Data,"

24 Hours a Day: A Preliminary Investigation of Polychronic Time

Ann. Rev. Anthropol., 13 (1984), 495-517.
and L. Sailer, "Informant Accuracy in Social-Network Data
V. An Experimental Attempt to Predict Actual Communication

from Recall Data," Social Sci. Res., 11 (1982), 30-66.

Use," J. Consumer Res., 18 (1991), 392-401.
Keisler, S. and L. Sproull, "Response Effects in Electronic Survey,"
Public Opinion Quarterly, 50 (1986), 402-413.
Laudon, K. C., "From PCs to Managerial Workstations: Organizational

Chung, W. Y. and S. Iacono, "Use of Advanced Features and Percep-

Environment and Management Policy in the Financial Industry,"

tions of Software Quality: An Experimental Study," Proc. Twenty-

in M. Jarke (Ed.), Managers, Micros and Mainframes, John Wiley &

Seventh Hawaii International Conference on Systems Sci., 27 (1994).

Sons Ltd., Chichester, 1986.

766 MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

COLLOPY
Biases in Retrospective Self-reports of Time Use

McGrath, J. E. and J. R. Kelly, Time and Human Interaction: Toward a
Social Psychology of Time, Guilford Press, New York, 1986.

Meehl, P. E., "Causes and Effects of My Disturbing Little Book," J.
Personality Assessment, 50 (1986), 370-375.
Melone, N. P., "Theoretical Assessment of User-Satisfaction Construct
in Information Systems Research," Management Sci., 36 (1990),

Goods, and Well-Being, Institute for Social Research; Ann Arbor,
MI, 1985.

, "Television's Impact on Everyday Life: Some Cross-National Ev-

idence," in E. Rubinstein, G. Comstock, and J. Murray (Ed.), Tel-

evision and Social Behavior, Vol.4, U.S. Government Printing Office,
Washington, DC, 1972.

Schriesheim, C. and J. Schriesheim, "Development and Empirical Ver-

76-91.

Pentland, B. T., "Use and Productivity in Personal Computing: An

ification of New Response Categories to Increase the Validity of

Empirical Test," Proc. Tenth International Conf. on Information Sys-

Multiple Response Alternative Questionnaires," Educational and

tems, Boston, MA, 10 (1989), 211-222.

Podsakoff, P. M. and D. W. Organ, "Self-Reports in Organizational

Research: Problems and Prospects," J. Management, 12 (1986),
531-543.

Psychological Measurement, 34 (1974), 877-884.

Sproull, L., "Using Electronic Mail for Data Collection in Organi-

zational Research," Academy of Management I., 29 (1986), 159169.

Rice, R. E., "Computer-Mediated Communication System Network

Srinivasan, A., "Alternative Measures of System Effectiveness: Asso-

ciations and Implications," Management Information Systems QuarData: Theoretical Concerns and Empirical Examples," Int. J. ManMachine Studies, 32 (1990), 627-647.

, D. Hughes and G. Love, "Usage and Outcomes of Electronic
Messaging at an R&D Organization: Situational Constraints, Job
Level and Media Awareness," Office: Technology and People, 5

terly, 9 (1985), 243-253.

Sudman, S. and N. M. Bradburn, Asking Questions: A Practical Guide to
Questionnaire Design, Josey-Bass, San Francisco, CA, 1982.
Thompson, R. L., C. A. Higgins and J. M. Howell, "Personal Computing: Toward a Conceptual Model of Utilization," Management In-

(1989), 141-161.
and D. E. Shook, "Relationships of Job Categories and Organizational Levels to Use of Communication Channels, Including

formation Systems Quarterly, 15 (1991), 125-143.
Vandenbosch, B. and C. Higgins, "Executive Information Systems: A

Comparison to More Traditional Information Channels," Proc.
Electronic Mail: A Meta-Analysis and Extension," J. Management
ASAC, 1992, 178-187.

Studies, 27 (1990), 195-229.
and , "Access to, Usage of, and Outcomes from an Electronic

Vitalari, N. P., A. Venkalesh and K. Gronhaug, "Computing in the

Messaging System," ACM Trans. on Office Information Systems, 6

Home: Shifts in the Time Allocation Patterns of Households,"

(1988), 255-276.

Comm. ACM, 28 (1985), 512-522.

Robey, D., "User Attitudes and Management Information System

Use," Academy of Management J., 22 (1979), 527-538.
Robinson, J. P., "Time-Diary Evidence About the Social Psychology of
Everyday Life," in J. E. McGrath, (Ed.), The Social Psychology of
Time, Sage Publications, Newbury Park, CA, 1988, 134-148.

Weisband, S. P., "Instrumental and Symbolic Aspects of an Executive
Information System," in S. Kiesler and L. Sproull (Eds.), Computing and Change on Campus, Cambridge University Press, Cambridge, MA, 1987, 150-169.
Winter, S. J., "The Symbolic Potential of Computer Technology Dif-

, "The Validity and Reliability of Diaries Versus Alternative Time ferences Among White-Collar Workers," Proc. Fourteenth InterUse Measures," in F. T. Juster and F. P. Stafford, (Eds.), Time,

national Conf. on Information Systems, 14 (1993), 331-344.

Accepted by John C. Henderson; received April 14, 1994. This paper has been with the authors 10 months for 2 revisions.

MANAGEMENT SCIENCE/Vol. 42, No. 5, May 1996 767

This content downloaded from 132.74.55.202 on Sat, 09 Mar 2019 15:40:31 UTC
All use subject to https://about.jstor.org/terms

