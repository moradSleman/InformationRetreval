Computers in Human Behavior 86 (2018) 235e244

Contents lists available at ScienceDirect

Computers in Human Behavior
journal homepage: www.elsevier.com/locate/comphumbeh

Full length article

Personality biases in different types of 'internet samples' can inﬂuence
research outcomes
Tom Buchanan
Department of Psychology, University of Westminster, 115 New Cavendish Street, London, W1W 6UW, United Kingdom

a r t i c l e i n f o

a b s t r a c t

Article history:
Available online 2 May 2018

There are different ways of recruiting participants for internet-mediated research. Small differences
in personality have previously been documented between participants recruited in different ways.
Three online studies investigated whether such personality biases could affect research outcomes. In
Study 1, volunteers completing an online personality test scored higher on Openness to Experience
than students participating as a course requirement. Study 2 demonstrated that Openness to Experience was associated with political voting preference (Republican v. Democrat). Study 3 found that
volunteers scored higher on Openness to Experience than members of a paid research panel. Among
volunteers, but not paid panellists, Openness to Experience was associated with voting preference. It
is concluded that where research outcomes may be inﬂuenced by personality, particularly Openness
to Experience, researchers conducting online studies should be aware that biases arising from
recruitment techniques could inﬂuence ﬁndings. Recommendations are made for addressing this
issue.
© 2018 Elsevier Ltd. All rights reserved.

Keywords:
Internet-mediated research methodology
Volunteers
Personality biases
Openness to experience
Political preference
Participant recruitment

1. Introduction

1.1. Data quality and generalizability

Over the past two decades, internet-mediated research methods
such as online surveys have become ﬁrmly established in the
repertoire of behavioral scientists. Work using the internet and
associated technologies for the remote acquisition of data, from or
about human participants, is common in many disciplines. For
example, Rife, Cate, Kosinski, and Stillwell (2014) contend that
internet-mediated studies have become widely accepted within
social psychology, and Crone and Williams (2016) note that internet
studies are commonplace. There are a variety of ways of sourcing
participants for such research. These range from participation requests posted on social media or specialised websites, to commercial research panels and crowdsourced labor marketplaces.
Matthijsse, Leeuw, and Hox (2015) argue that most internetmediated research is based on non-probability panels, comprising
self-selected respondents. Given their prevalence, it is important to
consider methodological issues that may affect the outcomes of
such online studies.

There has always been discussion of issues such as data quality
in online data collection. In recent years, the debate has moved on
from consideration of the method as a whole, to examination of
the data quality characteristics for different sources of data. For
example, with the recent popularity of the MTurk online labor
marketplace for participant recruitment, there has been a sharp
focus on data quality (e.g. Hauser & Schwarz, 2016; Lowry, D'Arcy,
Hammer, & Moody, 2016; Thomas & Clifford, 2017) and other
characteristics such as the extent to which MTurk studies may be
repeatedly sampling from a limited participant pool of ‘expert’
respondents (Stewart et al., 2015). Similar themes are seen in
methodological research on other participant sources: for
example, Matthijsse et al. (2015) identiﬁed a subset of ‘professional’ respondents who were members of multiple online
research panels, but whose presence did not seem to affect data
quality.
In addition to data quality, a common theme of discussion has
been the extent to which ﬁndings obtained with different types of
online sample may be generalizable to wider populations. Hays, Liu,
and Kapteyn (2015) discuss the representativeness of both opt-in
(convenience) and probability-based internet respondent panels.

E-mail address: t.buchanan@westminster.ac.uk.
https://doi.org/10.1016/j.chb.2018.05.002
0747-5632/© 2018 Elsevier Ltd. All rights reserved.

236

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

They note that samples drawn from convenience panels can be
weighted to improve demographic representativeness, but this
“does not always yield complete comparability of the outcome
measures to a target population” (p. 687), for example in terms of
self-reported health status. There is considerable other evidence
that self-selected volunteer samples may vary from probability
samples in terms of a number of characteristics, and perhaps
particularly in terms of their status with respect to the topic of a
study (e.g. Khazaal et al., 2014).
1.2. Psychological differences between volunteers and other
research participants
There is evidence going back some time (e.g. Oakes, 1972) that
there may be psychological as well as demographic differences
between self-selected volunteers and people recruited in other
ways. For example, Van Lange, Schippers, and Balliet (2011) found
that people who volunteered to take part in experiments were
more likely to be prosocially-oriented than those who did not. In
the context of internet-mediated research, this raises the possibility
that people recruited in one way (for instance volunteers
responding to an advert circulated via social media) may differ
psychologically from those recruited through other routes (for
instance a paid online research panel). The reason this matters is
summed up nicely by Oakes: “For any behavioral phenomenon, it
may well be that members of another population that one could
sample might have certain behavioral characteristics that would
preclude the phenomenon being demonstrated with that population.” (Oakes, 1972, p. 961). Expressed more crudely: one might ﬁnd
a given effect in one sample, but not another, due to psychological
differences between them.
While this notion has been present in the literature for some
time (e.g. Buchanan, Johnson, & Goldberg, 2005) it has received
little empirical scrutiny. One exception is work by Rife et al. (2014)
who compared personality data for volunteers recruited through
Facebook and through a stand-alone data acquisition website.
While both were volunteer samples, they were different types of
volunteer. The Facebook participants actually paid a fee to take part,
which is very unusual. There were personality differences, of which
the largest was in Openness to Experience (higher in the Facebook
group). However, effect sizes were small, leading Rife et al. to argue
that these differences arising from recruitment method were not
likely to affect research outcomes. Rife et al. also compared Facebook participants with an undergraduate sample, drawn from a
University participant pool. Again, there were personality differences, of which the largest was in Openness to Experience (again
higher in the Facebook group). The personality trait of Openness to
Experience broadly reﬂects an interest in culture, arts and educational experiences; imagination and creativity; and progressive
rather than conventional outlooks. It is known to be associated
with a range of behavioral phenomena (e.g. Buchanan et al., 2005;
Costa & McCrae, 1992).
Rife et al. argued that the differences they found were too small
to have a realistic prospect of inﬂuencing research outcomes, and
that this supported the view that Facebook was a viable medium for
participant recruitment and testing. However, there is scope for
bigger differences to exist between volunteers of the type they
tested, and other participant types (such as ﬁnancially-motivated
research panel members). There were also minor differences between their recruitment routes in terms of the materials used that
could conceivably have attenuated the effect sizes for the differences they did report. Finally, the fact that an effect is small, does
not always mean it has no practical signiﬁcance. For example, even
a small difference in the efﬁcacy of two drugs could lead to differences in the number of lives saved depending on which is

prescribed. Therefore, there is still a question here worthy of
examination.
1.3. Aims, research questions and hypothesis
There are different ways of recruiting samples for research
projects executed via the internet. It is possible that different types
of sample will have different personality characteristics (for
example, volunteers may be more Open to Experience than people
recruited in other ways). Evidence to date suggests that if differences exist, they may be too small to be practically important. The
overall aims of this project were to establish (1) whether there are
personality differences between different types of internet sample,
and (2) if there are, are they likely to affect research outcomes?
These research questions were addressed with a series of three
studies. Study 1 was intended to test for the existence of personality
differences between samples recruited in different ways. Study 2
was designed to establish that a phenomenon of interest to researchers was inﬂuenced by the personality variable(s) in question.
Study 3 tested whether recruiting participants in different ways
could actually make a difference to research ﬁndings. For Study 1, it
was speciﬁcally hypothesised that volunteers recruited via the
internet would have higher Openness to Experience scores than
members of a traditional undergraduate research panel.
2. Study 1
Study 1 used a quasi-experimental design to compare the personality proﬁles of participants recruited either through a personality testing website, or a University research participation scheme.
Ethical approval for the study came from the host University's
Psychology Research Ethics Committee.
2.1. Method
2.1.1. Materials
In both conditions, data was collected using an online 41-item
personality questionnaire (Buchanan et al., 2005) derived from
the International Personality Item Pool (IPIP; Goldberg, 1999). The
measure provides indices of Extraversion, Neuroticism, Openness
to Experience, Agreeableness and Conscientiousness that correlate
well with the domains of Costa and McCrae's (1992) Five Factor
Model, and has been validated for use on the internet (Buchanan
et al., 2005). Participants are asked to rate the accuracy of statements about their typical behaviour on a 5-point scale ranging from
1 ‘‘very inaccurate’’ to 5 ‘‘very accurate’’.
2.1.2. Procedure
In Condition 1 (henceforth, ‘the website condition”), participants were recruited using an established personality testing
website, www.personalitytest.org.uk. This has existed for a number
of years, and attracted over a thousand users per week at the time
the study was conducted. No attempt is made to recruit respondents or otherwise attract them to the site - they are referred
by other sites or ﬁnd it through search engines. Many complete the
test as part of some class, being asked to do so by their teacher or
professor. In Condition 2 (henceforth, ‘the course requirement
condition’), participants were recruited through the host Psychology department's research participation scheme and referred to the
personality testing site described below. Students in their ﬁrst year
of study complete a number of hours of research participation in
return for course credit.
The conditions differed only in the source of the participants:
the experience and materials for participants in both conditions
were the same. Participants ﬁrst saw information about the study.

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

On indicating consent, they moved to a second page with brief
instructions and the questionnaire. Radio button response formats
on a 5-point scale (‘Very Inaccurate - Very Accurate’) were used for
the personality items, while all the others used drop-down menus.
Demographic items comprised age group (in 5-year increments);
current location (a comprehensive list of nations); gender; highest
level of education; main occupational status. Following this, participants were asked how they came to be taking the test (e.g. as
part of a class). Finally, participants were asked whether their data
could be used in analyses (they were instructed to answer ‘no’ if
they had not answered the questions seriously, or did not give
consent). After doing this, those who had completed all the personality items then saw a debrieﬁng page thanking them for their
participation, and providing their scores on each of the scales
(those who had not were sent back to complete the missing items).
In addition, they were given information to help interpret the
scores, including a brief description of the meaning of each of the
scales, and normative information about their scores relative (top
third, middle, bottom third) to others who had completed the inventory to date. Links were provided to contact the researcher, and
to information about personality research elsewhere on the
internet.
The target sample size was determined based on Ferguson's
(2009) ‘minimum practical effect size’ e i.e. sufﬁciently large to

237

detect effects that will have real-world importance, as opposed to
just statistical signiﬁcance. These effect sizes are operationalised as
equivalent to a Cohen's d of 0.41. Power analyses indicated that a
sample size of 300, with 150 in each condition, would have over
90% power to detect such an effect. In actuality, recruitment for the
course requirement condition ceased before this target was
reached. This was because the host University closed the participation scheme through which participants were recruited during a
curriculum redesign. No analyses were performed prior to
recruitment being terminated.
2.1.3. Data screening and processing
In the website condition, participants were drawn from the
general body of people accessing the personality testing website
(19,408 data submissions). A number of checks were performed to
assure data quality. First, any that did not indicate their data could
be used were deleted, leaving 13742 records. Data were then
screened for multiple submissions, where respondents accidentally
or deliberately submit their responses more than once. To guard
against this, an ID code is randomly assigned each time the site is
accessed. Using the SPSS 23 ‘Identify Duplicate Cases’ procedure,
1439 cases were identiﬁed as duplicating another record in the ﬁle
in terms of the randomly assigned ID code. These were deleted,
leaving 12303 cases with unique ID codes. Then, anyone reporting

Table 1
Demographic data for study 1.
Analysis 1

Analysis 2

Condition

N
Sex
Men
Women
Unanswered
Age
Modal age group
Age range
Unanswered
Location
USA
UK
Unanswered
Route to participation
Doing as part of some class
Found through search engine
Got link from a friend
Followed link from another site
Other
Unanswered
Highest level of education
Primary Education
Secondary Education
Vocational/Technical college
Some college/University
College/University Graduate
Some Postgraduate
Postgraduate/Professional Degree
Unanswered
Occupation
Employed for Wages
Self-employed
Unemployed
Home-maker
Student
Retired
Unable to work
Unanswered

Condition

All

1 (volunteers)

2 (course requirement)

All

1 (volunteers)

2 (course requirement)

326

234

92

6693

2512

4181

102 (31.3%)
219 (67.2%)
5 (1.5%)

87 (37.2%)
144 (61.5%)
3 (1.3%)

15 (16.3%)
75 (81.5%)
2 (2.2%)

2226 (33.3%)
4327 (64.6%)
140 (2.1%)

889 (35.4%)
1565 (62.3%)
58 (2.3%)

1337 (32.0%)
2762 (66.0%)
82 (2.0%)

16-20 (60.1%)
16e55
0 (0%)

16-20 (53.4%)
16e55
0 (0%)

16-20 (77.2%)
16e45
0 (0%)

16-20 (49.3%)
16e85
0 (0%)

16-20 (22.0%)
16e85
0 (0%)

16-20 (65.7%)
16e85
0 (0%)

0 (0%)
326 (100%)
0a (0%)

0 (0%)
234 (100%)
0 (0%)

0 (0%)
92 (100%)
0 (0%)

6693 (100%)
0 (0%)
0 (0%)

2512 (100%)
0 (0%)
0 (0%)

4181 (100%)
0 (0%)
0 (0%)

92 (28.2%)
95 (29.1%)
64 (19.6%)
50 (15.3%)
25 (7.7%)
0 (0%)

0 (0%)
95 (40.6%)
64 (27.4%)
50 (21.4%)
25 (10.7%)
0 (0%)

92 (100%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)

4181 (62.5%)
798 (11.9%)
448 (6.7%)
901 (13.5%)
322 (4.8%)
43 (0.6%)

0 (0%)
798 (31.8%)
448 (17.8%)
901 (35.9%)
322 (12.8%)
0 (0%)

4181 (100%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)

8 (2.5%)
90 (27.6%)
29 (8.9%)
110 (33.7%)
64 (19.6%)
13 (4.0%)
11 (3.4%)
1 (0.3%)

6 (2.6%)
61 (26.1%)
17 (7.3%)
81 (34.6%)
45 (19.2%)
13 (5.6%)
11 (4.7%)
0 (0%)

2 (2.2%)
29 (31.5%)
12 (13.0%)
29 (31.5%)
19 (20.7%)
0 (0%)
0 (0%)
1 (1.1%)

628 (9.4%)
2396 (35.8%)
134 (2.0%)
1767 (26.4%)
1016 (15.2%)
324 (4.8%)
417 (6.2%)
11 (0.2%)

85 (3.4%)
496 (19.8%)
83 (3.3%)
606 (24.1%)
670 (26.7%)
205 (8.2%)
362 (14.4%)
5 (0.2%)

543 (13.0%)
1900 (45.4%)
51 (1.2%)
1161 (27.8%)
346 (8.3%)
119 (2.8%)
55 (1.3%)
6 (0.1%)

8 (2.5%)
1 (0.3%)
5 (1.5%)
0 (0%)
311 (95.4%)
0 (0%)
0 (0%)
1 (0.3%)

0 (0%)
0 (0%)
0 (0%)
0 (0%)
234 (100%)
0 (0%)
0 (0%)
0 (0%)

8 (8.7%)
1 (1.1%)
5 (5.4%)
0 (0%)
77 (83.7%)
0 (0%)
0 (0%)
1 (1.1%)

2243 (33.5%)
295 (4.4%)
301 (4.5%)
97 (1.4%)
3543 (52.9%)
66 (1.0%)
65 (1.0%)
83 (1.2%)

1224 (48.7%)
222 (8.8%)
131 (5.2%)
65(2.6%)
712 (28.3%)
61 (2.4%)
56 (2.2%)
41 (1.6%)

1019 (24.4%)
73 (1.7%)
170 (4.1%)
32 (0.8%)
2831 (67.7%)
5(0.2%)
9 (0.2%)
42 (1.0%)

Note. Percentages may not sum exactly to 100% due to rounding errors.
a
One participant in Study 1 Condition 2 did not report location but was known to be in the UK.

238

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

Table 2
Descriptive statistics for personality scales, study 1.
Variable

Extraversion
Agreeableness
Conscientiousness
Neuroticism
Openness to Experience

Analysis 1

Analysis 2

n

M

SD

a

Range
Potential

Actual

326
326
326
326
326

28.97
26.47
31.82
23.13
26.52

7.26
4.44
6.99
6.73
5.11

.87
.71
.83
.85
.75

9e45
7e35
10e50
8e40
7e35

9e45
12e35
15e49
8e40
11e35

Skew

n

M

SD

a

Range
Potential

Actual

0.10
0.64
0.04
0.29
0.41

6693
6693
6693
6693
6693

29.51
27.62
35.56
21.33
26.09

7.43
4.49
6.99
6.72
5.28

.86
.75
.84
.84
.74

9e45
7e35
10e50
8e40
7e35

9e45
7e35
10e50
8e40
7e35

Skew

0.24
0.77
0.29
0.30
0.42

Table 3
Volunteer and participation scheme personality scores, study 1.
Volunteers

Extraversion
Agreeableness
Conscientiousness
Neuroticism
Openness to Experience

Participation
Scheme

M

SD

M

SD

29.06
26.51
31.72
23.16
26.97

7.71
4.71
7.17
6.98
5.05

28.73
26.36
32.08
23.07
25.37

6.00
3.69
6.56
6.07
5.10

t

df

p

0.42
0.30
0.42
0.12
2.56

212.4
210.9
324
190.3
324

.68
.76
.68
.91
.01

95% CI

Cohen's d

LL

UL

1.25
0.82
2.05
1.45
0.36

1.92
1.12
1.34
1.63
2.83

.05
.04
-.05
.01
.32

Note. CI ¼ Conﬁdence Interval; LL ¼ Lower Limit; UL ¼ Upper Limit. For Extraversion, Agreeableness and Neuroticism, adjusted df used due to heterogeneity of variance.

their age group as below 16 years (359 cases) or who did not give
their age (11 cases) was removed due to concerns about their capacity to give valid consent. Finally, anyone in the 16e20 group
claiming to have completed 'some postgraduate' or greater education (10 cases) was removed due to concerns about mischievous
or careless responding. Following these checks, 11923 individuals
remained in the dataset collected from the website. Subsets of
these 11923 who met the inclusion criteria for the analyses that
follow are described in Table 1.
Of 101 individuals who participated in the course requirement
condition, two did not indicate they gave consent for their data to
be used, and seven cases were found where data submissions were
duplicates of other entries in the dataﬁle (e.g. where a respondent
had accidentally or deliberately submitted data twice). These nine
cases were removed, leaving 92.
Two analyses were performed using different sets of participants. For the primary analysis (Analysis 1), a subsample was
created by selecting those respondents who were (a) students, (b)
located in the UK, and (c) not completing the test as part of some
class requirement. This resulted in a ‘website condition sample’ of
234 individuals who matched those in the course requirement
condition in terms of student status and nation, but differed in their
motivation to complete the test (course requirement vs. true
volunteers).
2.1.4. Participants
Demographics for those Study 1 participants meeting the inclusion criteria for the analyses are shown in Table 1. The data
suggest that for Analysis 1, the sample of 326 participants is
comprised of UK-based students, mainly younger than 20 years in
age, and predominantly female. For Analysis 2, the sample of 6693
participants still comprises around half students, this time all based
in the USA. It includes more older people, with higher education
levels, and more people in employment. The sample is again predominantly female.
2.2. Results
2.2.1. Analysis 1
Personality scores for Study 1 participants are shown in Table 2

(left hand side). Independent samples t-tests (Table 3) indicated
that there were statistical differences between the website volunteer condition and the course requirement condition only in terms
of Openness to Experience, with the volunteers being more Open to
Experience than those recruited through the participant pool.
However, Levene's test for equality of variances also indicated
heterogeneity of variance between conditions for Extraversion,
Agreeableness and Neuroticism. Volunteer respondents were more
variable in these characteristics. One reason for this could be
increased variability in the website condition with respect to other
demographic variables that inﬂuence personality. Indeed, c2 tests
indicated that age group (c2 (7, N ¼ 326) ¼ 19.36, p < .005) and sex
(c2 (1, N ¼ 321) ¼ 13.17, p < .0005) were not evenly distributed
across conditions. The main analysis was therefore repeated using
ANCOVAs examining the effect of Condition and Sex on each personality variable, with age group as a covariate. When these variables were controlled for, there were no differences between
conditions in terms of personality scores. In particular, there was no
statistically signiﬁcant difference between volunteers and participant pool members on Openness to Experience (F(1,316) ¼ 0.1.66,
p ¼ .20).
At ﬁrst glance, the implication of this analysis would be that the
two samples do differ in personality (Openness to Experience).
However, further analyses showed this is a function of known demographic inﬂuences on personality, which can be controlled for.
One issue with this interpretation is that samples tested via the
internet are often very much larger than the groups compared here.
There may still be meaningful differences between recruitment
methods that would manifest with larger samples. Given that a
large amount of data was collected but not used in Analysis 1, there
was scope for additional exploratory analyses to be carried out.
2.2.2. Analysis 2
This exploratory analysis drew on the larger dataset
(N ¼ 11,923), of which the Analysis 1 website condition sample was
a subset. To control for any between-nation differences in personality, analysis was restricted to those 6693 individuals who gave
their location as the US. Thus, none of the participants (all from the
UK) in Analysis 1 were also included in Analysis 2. Personality scale
scores for these participants are shown in Table 2 (right hand side).

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

239

Table 4
Comparison of study 1 participants completing online test as a class requirement with those who are volunteers.
Estimates

F(1,

Volunteers

Extraversion
Agreeableness
Conscientiousness
Neuroticism
Openness to Experience

6548)

Estimated Differences

Class
Requirement

M

SE

M

SE

28.29
26.95
34.09
21.98
27.30

0.17
0.10
0.15
0.15
0.11

30.25
27.62
36.18
20.20
25.10

0.13
0.08
0.12
0.11
0.09

79.71
26.71
107.80
85.20
210.10

p

M

.000
.000
.000
.000
.000

0.20
0.67
2.09
1.78
2.20

95% CI

Partial eta' squared

LL

UL

2.39
0.93
2.49
1.40
1.90

1.53
0.42
1.70
2.16
2.50

0.01
0.00
0.02
0.01
0.03

Note. CI ¼ Conﬁdence Interval; LL ¼ Lower Limit; UL ¼ Upper Limit.

Those individuals who reported doing the test as part of a class
requirement (analogous to the course requirement condition in
Analysis 1) were compared to those who reported coming to it in
some other way, using two-way ANCOVAs, with route-toparticipation and sex as independent variables and age group as a
covariate. As there was missing data on sex for 140 respondents, the
group sizes were 4099 (class requirement) and 2454 (nonrequired) respectively.
The results of the route-to-participation main effect in these
ANCOVAs are summarised in Table 4 (Table 5 includes full information on all ﬁve ANCOVAs including the covariates). With this
larger sample, there are statistically signiﬁcant differences between
self-reported routes to participation on all personality variables.
People seeking out the personality website and participating of
their own volition were less Extraverted, Agreeable and Conscientious, but had higher Neuroticism and Openness to Experience,
than those participating as part of class requirement.
Given the large sample size, examination of effect sizes is more
instructive than statistical signiﬁcance. All the effect sizes are in the
trivial to low range. The strongest effect, for Openness to Experience at 0.03, does not meet the benchmark for partial eta squared
(0.04) proposed by Ferguson (2009) for a ‘practically meaningful’
effect.
2.3. Discussion
Analysis 1 suggested that there was a personality difference
between volunteers accessing a personality testing website and a
traditional undergraduate panel, with the volunteers having higher

levels of Openness to Experience. However, the difference was
small and could be attributed to known demographic variables. A
further exploratory analysis, with a larger sample and thus better
power, indicated that volunteers differed from respondents
completing the test as a class requirement on all ﬁve dimensions of
personality. However, the effect sizes were very small. Even the
largest, Openness to Experience, was below the threshold where it
might be considered to be a meaningful difference. These results
closely parallel those of Rife et al. (2014). Taken together, these
ﬁndings suggest that personality biases may be detectable, but it is
questionable whether they are worth worrying about in practice. To
address this question, Studies 2 and 3 were planned to examine
whether the differences observed actually could potentially inﬂuence research ﬁndings.
3. Study 2
Study 1 suggested that samples recruited in different ways could
differ in personality, notably Openness to Experience. However, the
small effect sizes raised the question of whether such biases actually matter. Study 2 set out to explore whether variables such as
Openness could affect phenomena of interest to researchers, and
whether differences of the magnitude seen could potentially affect
research outcomes.
There is considerable evidence that Openness to Experience is
associated with political preference, with people scoring lower on
Openness being more conservative in their opinions, and people
scoring higher being more liberally oriented (e.g. Carney, Jost,
Gosling, & Potter, 2008). Indeed, items asking about voting

Table 5
Analyses of covariance examining effect of class requirement to participate, sex and age group on personality variables.

Extraversion

Agreeeableness

Conscientiousness

Neuroticism

Openness to Experience

Variable

F(1,

Age Group
Sex
Class Requirement?
Sex x Class Requirement
Age Group
Sex
Class Requirement?
Sex x Class Requirement
Age Group
Sex
Class Requirement?
Sex x Class Requirement
Age Group
Sex
Class Requirement?
Sex x Class Requirement
Age Group
Sex
Class Requirement?
Sex x Class Requirement

0.48
0.00
79.71
0.73
169.53
164.13
26.71
0.03
314.46
36.37
107.80
0.87
218.93
222.22
85.20
15.08
37.83
44.16
210.10
3.75

6548)

p

Partial eta squared

0.489
0.957
0.000
0.393
0.000
0.000
0.000
0.859
0.000
0.000
0.000
0.351
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.053

0.00
0.00
0.01
0.00
0.03
0.02
0.00
0.00
0.05
0.01
0.02
0.00
0.03
0.03
0.01
0.00
0.01
0.01
0.03
0.00

240

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

Table 6
Demographic data for studies 2 and 3.
Study 2

Study 3
Condition

N
Sex
Men
Women
Unanswered
Age
Modal age group
Age range
Mean age (SD)
Unanswered
Location
USA
UK
Unanswered
Route to participation
Doing as part of some class
Found through search engine
Got link from a friend
Followed link from another site
Other
Unanswered
Qualtrics panel
Highest level of education
Primary Education
Secondary Education
Vocational/Technical college
Some college/University
College/University Graduate
Some Postgraduate
Postgraduate/Professional Degree
Unanswered
Occupation
Employed for Wages
Self-employed
Unemployed
Home-maker
Student
Retired
Unable to work
Unanswered

All

1 (volunteers)

2 (paid)

1179

10427

247

298

423 (35.9%)
733 (62.2%)
23 (2.0%)

3754 (36.0%)
6571 (63.0%)
102 (1.0%)

111 (44.9%)
136 (55.1%)
0 (0%)

137 (46.0%)
161 (54.0%)
0 (0%)

16-20 (50.9%)
16e80
e
0 (0%)

e
16e75
25.79 (10.55%)
0 (0%)

e
18e25
20.8 (2.19)
0 (0%)

e
18e25
21.7 (2.23)
0 (0%)

804 (68.2%)
133 (11.3%)
7 (0.6%)

6448 (61.8%)
1089 (10.4%)
195 (1.8%)

247 (100%)
0 (0%)
0 (0%)

298 (100%)
0 (0%)
0 (0%)

698 (59.2%)
241 (20.4%)
109 (9.2%)
50 (4.2%)
75 (6.4%)
6 (0.5%)
e

6002 (57.6%)
1627 (15.6%)
876 (8.4%)
833 (8.0%)
781 (75%)
10 (0.1%)
298 (2.9%)

0 (0%)
247 (100%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)

0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
298 (100%)

48 (4.1%)
419 (35.5%)
45 (3.8%)
379 (32.1%)
160 (13.6%)
67 (5.7%)
61 (5.2%)
0 (0%)

1085 (10.4%)
2517 (24.1%)
336 (3.2%)
3332 (32.0%)
1655 (15.9%)
495 (4.7%)
998 (9.5%)
9 (0.1%)

2 (0.8%)
50 (20.2%)
2 (0.8%)
126 (51.0%)
51 (20.6%)
7 (2.8%)
9 (3.6%)
0 (0%)

8 (2.7%)
72 (24.2%)
7 (2.3%)
126 (42.3%)
59 (19.8%)
9 (3.0%)
17 (5.7%)
0 (0%)

393 (33.3%)
44 (3.7%)
50 (4.2%)
11 (0.9%)
643 (54.5%)
5 (0.4%)
4 (0.3%0
29 (2.5%)

3764 (36.1%)
471 (4.5%)
382 (3.75)
138 (1.3%)
5444 (52.2%)
85 (0.8%)
131 (1.3%)
12 (0.1%)

91 (36.8%)
4 (1.6%)
0 (0%)
0 (0%)
152 (61.5%)
0 (0%)
0 (0%)
0 (0%)

142 (47.7%)
16 (5.4%)
0 (0%)
0 (0%)
140 (47.0%)
0 (0%)
0 (0%)
0 (0%)

Note. Percentages may not sum exactly to 100% due to rounding errors.

preference have been included in personality inventories as indices
of Openness to Experience: the Openness scale used by Buchanan
et al. (2005) includes the items “Tend to vote for conservative political candidates” and “Tend to vote for liberal political candidates”.
Thus, it was hypothesised that Openness to Experience would be
positively associated with a tendency towards political liberalism.
In a U.S. political context, participants preferring to vote Democrat
would be higher on Openness than participants preferring to vote
Republican.
Study 2 thus used a quasi-experimental design to compare the
Openness to Experience scores of U.S. based participants, recruited
through a personality testing website, who indicated preferences
for voting for either Democrat or Republican candidates. Ethical
approval for the study came from the host University's Psychology
Research Ethics Committee.

3.1. Method
3.1.1. Materials and procedure
Data were collected using the same materials and procedure as
in Study 1 Condition 1: a Five Factor personality questionnaire
hosted on the website www.personalitytest.org.uk. All participants

were visitors to that website, with no active attempts being made to
recruit respondents. The materials and experience of participants
were identical to those in Study 1, save for addition of a new item at
the end of the questionnaire asking about voting preference: “This
question applies only to people who are entitled to vote in the USA.
In general, do you prefer to vote for Democrat or Republican candidates?”. Response options were (1) Prefer not to answer; (2) Not
applicable e not entitled to vote in USA; (3) Prefer to vote Democrat; (4) Prefer to vote Republican; (5) Prefer to vote neither
Democrat nor Republican.

3.1.2. Data screening and processing
Over a period of 9 days, 1862 responses were logged. Any cases
where respondents did not indicate their data could be used were
deleted, leaving 1299. Twenty-four individuals who reported an
age group of below 16 were excluded. The SPSS 23 Identify Duplicate Cases dialogue was used to identify 96 cases duplicating
another record in the ﬁle in terms of the randomly assigned ID
code. These were deleted leaving 1179 cases. To check for
mischievous or careless responding, cases in the 16e20 group were
checked for any claiming to have completed 'some postgraduate' or
greater education. None were found.

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

241

Table 7
Descriptive statistics for personality scales, study 2.
Variable

Extraversion
Agreeableness
Conscientiousness
Neuroticism
Openness to Experience
Non-political Openness to Experience

N

1179
1179
1179
1179
1179
1179

M

a

SD

29.28
27.21
34.41
22.29
25.74
18.97

7.51
4.63
7.23
6.86
5.57
4.30

Range

.87
.75
.84
.83
.77
.76

Skew

Potential

Actual

9e45
7e35
10e50
8e40
7e35
5e25

9e45
9e35
11e50
8e40
8e35
5e25

0.30
0.66
0.30
0.37
0.40
0.61

for Openness of 0.89 (p < .0005) indicated that for every one-point
increase in Openness to Experience, the likelihood of voting
Republican decreases by around 11% (see Table 8).

3.1.3. Participants
Demographics for Study 2 participants are shown in Table 6 (left
side). The data suggest that while there was considerable variance
among participants, the majority were relatively young female
students based in the United States, who were participating as part
of some educational activity.

3.3. Discussion
Study 2 showed that, consistent with previous research, there is
a relationship between Openness to Experience and political preference. US-based participants preferring to vote for Democrat
candidates scored higher on Openness than those preferring to vote
Republican. The logistic regression analysis showed that the impact
of personality on political preference is modest. However, when
considering the potential effect of sampling differences on research
outcomes, it is pertinent to recall that Study 1 found the mean
Openness to Experience scores of participants recruited in different
ways differed by up to 2.2 scale points (Table 4). With a one-point
difference in the shortened Openness scale used here changing
political preference by around 11%, this may mean there is a real
possibility that research ﬁndings could be affected by recruitment
method. The acid test of whether the phenomenon described here
actually matters in practice, will be to look at whether different
patterns of results are observed in different types of ‘internet
sample’. This was the focus of Study 3.

3.2. Results
Descriptive statistics for the personality scales are shown in
Table 7. Because the Openness to Experience scale included two
items explicitly referring to political preference, an abbreviated
version of this scale was also computed which omitted these items.
It correlated r ¼ 0.91 (n ¼ 1179, p < .0005) with the full Openness
measure. This abbreviated measure was used in the following analyses. All scales had acceptable reliability, with Cronbach's
alphas > .7.
The analysis was restricted to those 804 individuals who reported their location as being in the USA. Of the 777 of those who
answered the voting question, 129 (16.6%) preferred not to indicate
a voting preference; 65 (8.4%) were not eligible to vote, 332 (42.7%)
preferred to vote Democrat, 164 (21.1%) preferred to vote Republican, and 87 (11.2%) did not prefer either. Data was missing for 27
individuals who did not answer the question.
An independent samples t-test was used to compare the
Openness to Experience scores of Republican and Democrat voters,
using the abbreviated (non-political) Openness scale. This indicated
that Democrat voters (N ¼ 332, M ¼ 19.80, SD ¼ 4.08) were more
Open to Experience than Republican voters (N ¼ 164, M ¼ 17.81,
SD ¼ 4.19). The difference was statistically signiﬁcant (t(494) ¼ 5.08,
p < .0005) with an effect size of Cohen's d ¼ 0.48.
In a further exploratory analysis, a logistic regression was performed to examine the effect of (non-political) Openness to Experience, the other personality variables, age group and sex on
classiﬁcation of participants as either Democrat or Republican
voters. This is shown in Table 8. The Cox and Snell R2, at 0.088,
indicated that this set of variables accounted for only a small
portion of variance. However, Openness, Conscientiousness, sex
and age group were all signiﬁcant predictors of voting preference.
The effect of most interest here is that of Openness. The Odds Ratio

4. Study 3
Study 3 adopted a quasi-experimental design, where participants recruited through a personality testing website (as in Study 1
Condition 1, and Study 2) were compared with paid individuals
drawn from a commercial research panel. The goals of the study
were (1) to compare Openness to Experience scores of the two
samples, hypothesising that the scores of volunteer participants
would be higher than those of paid panellists, and (2) to test
whether the relationship between personality and voting preference was replicated across these different types of sample. Ethical
approval for the study came from the host University's Psychology
Research Ethics Committee.
As an aside, the focus of Study 3 is somewhat relevant to current
social concerns. In recent years, there have been concerns about the
accuracy of political opinion polls. There have been several high-

Table 8
Binary logistic regression: Personality and demographic predictors of voting preference.

Openness to Experience (non-political)
Extraversion
Agreeableness
Conscientiousness
Neuroticism
Sex (M ¼ 1, F ¼ 2)
Age group
Constant

B

S.E.

Wald

df

p

Exp(B)

95% CI for Exp(B)

0.12
0.03
0.01
0.04
0.01
0.70
0.13
0.90

0.03
0.02
0.03
0.02
0.02
0.23
0.06
1.15

23.01
3.00
0.05
4.77
0.21
9.47
4.37
0.61

1
1
1
1
1
1
1
1

.000
.083
.825
.029
.645
.002
.037
.435

0.89
1.03
1.01
1.04
1.01
0.50
0.88
2.46

[0.84,
[1.00,
[0.96,
[1.00,
[0.97,
[0.32,
[0.78,

0.93]
1.06]
1.06]
1.08]
1.05]
0.78]
0.99]

242

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

analyses, 299 individuals reporting their age as below 16 years
were removed from the sample due to ethical concerns about
consent, as were 172 who did not answer the age question. Two
individuals giving unrealistically high ages (150 and 269) and 23
who appeared to have given their year of birth rather than age were
also removed. This left 10,147 respondents who gave an age that fell
within these parameters. Data quality was further assured by
examining the dataﬁle for unrealistic combinations of demographic
data (e.g. people claiming to be children with doctoral degrees) that
might indicate mischievous or careless responding. Anyone under
21 claiming to have a doctoral or professional degree (15), anyone
under 19 claiming to have a Masters degree (2), and anyone under
18 claiming to be a college/University graduate (1), was excluded
leaving 10,129 in the sample for the volunteer condition.
In the paid condition, 350 responses were recorded from individuals sourced via the Qualtrics panel. Of these, 49 did not give
consent for data analysis, and were thus excluded. An age was
recorded for 299 individuals (the two who did not respond to this
item were also excluded). All reported ages were between 18 and
25. The dataﬁle was again checked for unrealistic combinations of
demographic data. One individual aged under 21 claiming to have a
doctoral or professional degree was excluded, leaving 298 in the
sample for the paid condition.

proﬁle instances where electoral results varied from predictions,
with one notable example being the 2015 UK General Election.
Analyses of that situation have indicated that unrepresentative
sampling was an important factor (Sturgis et al., 2017). Many
opinion polls are conducted using participants sourced from online
panels, and there is evidence that online panels are typically more
Democrat-oriented (in the US) than samples that are more representative of the population as whole, such as telephone random
digit dialling surveys (Kennedy et al., 2016). Thus, if some types of
sample incorporate biases that could affect the outcomes of political research, it may be important to know about that.
4.1. Method
4.1.1. Materials
In both conditions, data was collected using the same 41-item
personality inventory as in Studies 1 and 2. For this study, it was
hosted on the Qualtrics online research platform (participants
accessing the www.personalitytest.org.uk site were automatically
redirected to the Qualtrics implementation). Qualtrics’ proprietary
technology prevented multiple data submissions. An additional
scale was added to the questionnaire: the ten-item Openness to
Experience subscale from the Big Five Inventory (BFI; John,
Naumann, & Soto, 2008). This was selected for use as the dependent measure in the study, because unlike the Openness scale in the
Buchanan et al. (2005) measure, it does not include any items
referring to political preferences. It can thus be used in complete,
not abbreviated, form for this study. Beyond this, the materials
were as described in Study 2.

4.1.4. Participants
Demographics for Study 3 participants are shown in Table 6
(right hand side). As in the previous two studies, the sample was
largely US-based, young and well educated, with a majority of
participants being female. Typical ages were a little higher than in
Studies 1 and 2.

4.1.2. Procedure
In Condition 1 (henceforth, ‘the volunteer condition’), participants were recruited through www.personalitytest.org.uk in the
same way as in Study 1. They included both true volunteers, and
those participating as part of an educational program. In Condition
2 (henceforth, ‘the paid condition’), participants were supplied by
the Qualtrics research company, drawn from a panel of paid survey
respondents. The amount paid to each participant is not known, but
is likely to be modest. The experience of participants was as
described in Study 2, with the exception that the ten extra BFI
Openness to Experience questions were included in the personality
questionnaire, and age data was collected in exact numbers of years
rather than age groups. Study 1 found an effect size of d ¼ 0.32 for
the difference in Openness between volunteers and non-volunteers
(Table 3). Sample sizes for Study 3 were planned to give over 95%
power to detect such an effect, with a target of at least 255 in each
condition.

4.2. Results
Descriptive statistics for the measured personality variables are
shown in Table 9. While all variables are included for the sake of
completeness, the analyses that follow use only the BFI Openness
scores.
To test the hypothesis that online volunteers differed from paid
panellists in BFI Openness to Experience, a subsample of the
volunteer condition demographically similar to the paid condition
was selected. The selection criteria were that the participants had
been searching for personality tests (i.e. volunteers ﬁnding it of
their own accord), were located in the US, aged 18e25 years, and
were either students or employed (including self-employed). There
were 247 such individuals. Given that demographic variables are
known to affect personality, and could differ between the samples,
the extent to which gender, age and education were related to BFI
Openness to Experience was assessed. Men and women did not
differ in BFI Openness to Experience (t(541) ¼ 1.31, p ¼ .16), and BFI
Openness to Experience did not correlate signiﬁcantly with either
age (r ¼ 0.08, N ¼ 545, p ¼ .065) or education (r ¼ 0.006, N ¼ 543,
p ¼ .89). Thus, analysis proceeded with no controls for these

4.1.3. Data screening and processing
In the volunteer condition, 18,420 data submissions were
recorded from individuals accessing the personality testing website. From the 10,643 who gave consent for their data to be used in

Table 9
Descriptive statistics for personality scales, study 3.
Variable

BFI Openness to Experience
Extraversion
Agreeableness
Conscientiousness
Neuroticism
Openness to Experience

N

10285
10427
10427
10427
10427
10427

M

38.09
28.93
27.65
35.25
21.72
25.90

SD

6.10
7.25
4.32
7.08
6.71
5.21

a

.78
.87
.73
.84
.83
.74

Range

Skew

Potential

Actual

10e50
9e45
7e35
10e50
8e40
7e35

9e45
7e35
11e50
8e40
7e35

0.21
0.73
0.30
0.30
0.39

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

243

Table 10
Big Five Inventory Openness to Experience scores for online volunteers and paid panellists.
Sample

Online volunteers
Paid panellists

Republican voters

Democrat voters

N

M

SD

N

M

SD

40
65

37.42
36.26

6.64
5.84

120
150

39.83
37.61

5.70
5.97

demographic variables being implemented. The BFI Openness to
Experience scores of online volunteers (N ¼ 245, M ¼ 39.28,
SD ¼ 6.06) and paid panellists (N ¼ 298, M ¼ 37.21, SD ¼ 5.64) were
statistically signiﬁcantly different (t(541) ¼ 4.12, p < .0005, Cohen's
d ¼ 0.35).
The principal question addressed by this study was whether
samples recruited in different ways might give rise to different
research ﬁndings. Therefore, the analysis performed in Study 2 e
comparison of the Openness to Experience levels of Democrat and
Republican voters e was repeated within each condition, to test
whether the relationship between political preference and Openness to Experience was the same for paid participants as it was for
volunteers. The same selection criteria as described above were
retained to make the two conditions as comparable as possible,
apart from having been recruited in different ways. Results of this
analysis are shown in Table 10. t-tests performed within each
condition showed that among online volunteers, people preferring
to vote Democrat once again had higher Openness to Experience
scores. However, this phenomenon was not observed among participants recruited through the Qualtrics panel.
4.3. Discussion
Participants recruited through a Qualtrics panel, and volunteers
accessing a personality testing website of their own volition, did
not differ in variables associated with Openness to Experience
other than the manner in which they were recruited. However,
consistent with expectations, the volunteers were more Open to
Experience than individuals who were paid to participate. This
indicates that recruitment method is associated with personality
differences. With respect to the question of whether this relatively
small difference actually matters, the current ﬁndings suggest that
it can. The phenomenon reported in Study 2 e association of
Openness to Experience and voting preference e was found for one
group but not the other.
5. General discussion
This series of studies has shown that samples used in internetmediated research may vary in their personality characteristics
according to the means by which they were recruited. The differences are most notable for Openness to Experience. Volunteers
participating of their own volition score higher on Openness than
either traditional undergraduate samples, or paid research panel
members. The effect sizes for these differences are very small. In
many research projects they will be unimportant, as argued by Rife
et al. (2014). However, where this manuscript goes beyond previous
research is to provide a demonstration that these differences
actually can inﬂuence the outcomes of research. In cases where the
phenomenon or dependent variable of interest is related to
Openness to Experience, there is potential for recruitment method
to affect research outcomes. This is demonstrated by the ﬁndings of
Study 3, where a known psychological effect was detected with one
sample but not another. Thus, researchers should consider possible
biases arising from self-selection when planning and interpreting
studies.

t

df

p

Cohen's d

2.22
1.53

158
213

.028
.13

0.39
0.23

The ﬁnding that volunteers are higher in Openness to Experience is not at all surprising. Such individuals are by their very nature intellectually curious, and attentive to their inner states. It is
thus entirely likely that people seeking out personality tests on the
internetdwhich is how they would have ﬁrst come into contact
with the personality testing websitedwould tend be higher on
Openness. Furthermore, Dollinger and Leong (1993) found that
individuals higher on Openness to Experience were more likely to
give permission for data about them to be used in research. This is
analogous to the volunteer participants in the current study giving
consent for their scores to be used in the project.
5.1. Limitations
The work reported here has a number of limitations. While the
paper refers to ‘online volunteers’ in fairly broad terms, there are
different categories of these. The current project used people who
participated as a consequence of looking for an online personality
test. There are other ‘online volunteers’ who are recruited through
websites that actively seek to recruit research participants. Would
such individuals have similar personality characteristics to the
sample used here? Similarly, there are other types of paid research
participant. Would individuals recruited via online labor marketplaces such as MTurk have similar characteristics to the Qualtrics
panellists used here?
The measure of political preference used here was crude in
relation to the methodology used in real political polling, or
research that focuses speciﬁcally on political psychology. However,
given that Studies 2 and 3 were intended to provide proof of
concept, rather than explore political preference in detail, that
seems relatively unimportant. The key point is that the hypothesised phenomenon was detected, no matter how crude the measure used.
In Study 3, the total number of Republican voters (105) was
markedly lower than Democrats (270), for both the volunteers and
the paid panellists (Table 10). This is potentially indicative of the
very bias documented in this paper; that people lower in Openness
(indexed in this instance by political preference) are less likely to be
engaged in online research.
5.2. Recommendations
Despite the limitations, the work described here does appear to
have implications for practice and a number of recommendations
can be made. The key point is clearly that researchers should take
the source of their participants in to account and consider whether
it could affect research ﬁndings. Fortunately, there are some fairly
clear ways in which this can be done.
Early work on online research methodology (Reips, 2000) suggested that self-selection effects could be detected using the
‘multiple site entry’ technique, where data supplied by participants
from different recruitment websites is compared. This approach
may be a viable one in establishing whether a suspected bias has in
fact affected research outcomes. Researchers conducting online
studies where Openness to Experience may be an important variable should consider using this approach.

244

T. Buchanan / Computers in Human Behavior 86 (2018) 235e244

Another implication is for replication efforts. Psychologists have
recently demonstrated increased attentiveness to the need for
research ﬁndings to be replicated. Some replication efforts have
involved use of internet-mediated research methods. There has
been much discussion of the fact that successful replication rates
appear to be lower than one would desire. The implications here
are twofold: ﬁrst, it may be that some failures to replicate have
been a function of differences between the participants in the
original and replication studies (cf. Study 3). Second, it may well be
that replications should also be conducted using participants from
different sources, to increase conﬁdence in the generalizability of
ﬁndings.
Finally, although the measure of political preference used here
was simplistic, there may be implications for the opinion polling
industry. While sophisticated methodologies exist for recruiting
and weighting samples with respect to demographic characteristics, it is not clear that polling professionals do the same with
respect to personality variables. The current ﬁndings suggest it may
be desirable to do that, at least for Openness to Experience, given
that high Openness scorers may be over-represented in online
panels in comparison to the wider population (irrespective of demographic similarity).
6. Conclusions
It is clear that ‘internet samples’ recruited in different ways may
have different personality characteristics. In particular, volunteers
are likely to have higher Openness to Experience scores than people
being paid to participate or students receiving course credit for
doing so. The differences are relatively small in magnitude, a point
that led previous researchers to dismiss similar ﬁndings as being
unlikely to have important implications for research. In this
instance, it was demonstrated that a personality bias could affect
research outcomes, to the extent that a phenomenon was detected
with one type of sample but not another. On this basis, it is recommended that researchers doing internet-mediated research
where Openness to Experience may be relevant should take this
bias into account.
References
Buchanan, T., Johnson, J. A., & Goldberg, L. R. (2005). Implementing a ﬁve-factor
personality inventory for use on the internet. European Journal of Psychological Assessment, 21(2), 115e127. https://doi.org/10.1027/1015-5759.21.2.115.
Carney, D. R., Jost, J. T., Gosling, S. D., & Potter, J. (2008). The secret lives of liberals
and Conservatives: Personality proﬁles, interaction styles, and the things they
leave Behind: Liberals and conservatives. Political Psychology, 29(6), 807e840.
https://doi.org/10.1111/j.1467-9221.2008.00668.x.
Costa, P. T., & McCrae, R. R. (1992). Revised neo personality inventory (neo PI-r) and
neo ﬁve-factor inventory (neo FFI): Professional manual. Odessa, FL: Psychological
Assessment Resources.
Crone, D. L., & Williams, L. A. (2016). Crowdsourcing participants for psychological

research in Australia: A test of microworkers. Australian Journal of Psychology.
https://doi.org/10.1111/ajpy.12110.
Dollinger, S. J., & Leong, F. T. L. (1993). Volunteer bias and the ﬁve-factor model.
Journal
of
Psychology,
127(1),
29e36.
https://doi.org/10.1080/
00223980.1993.9915540.
Ferguson, C. J. (2009). An effect size primer: A guide for clinicians and researchers.
Professional Psychology: Research and Practice, 40(5), 532e538. https://doi.org/
10.1037/a0015808.
Goldberg, L. R. (1999). A broad-bandwidth, public domain, personality inventory
measuring the lower-level facets of several ﬁve-factor models. In I. Mervielde,
I. J. Deary, F. De Fruyt, & F. O (Eds.), Personality psychology in europe (Vol. 7, pp.
7e28). Tilburg, The Netherlands: Tilburg University Press. Retrieved from
http://projects.ori.org/lrg/PDFs_papers/A_broad-bandwidth inventory.pdf.
Hauser, D. J., & Schwarz, N. (2016). Attentive turkers: MTurk participants perform
better on online attention checks than do subject pool participants. Behavior
Research Methods, 48, 400e407. https://doi.org/10.3758/s13428-015-0578-z.
Hays, R. D., Liu, H., & Kapteyn, A. (2015). Use of Internet panels to conduct surveys.
Behavior Research Methods, 47(3), 685e690. https://doi.org/10.3758/s13428015-0617-9.
John, O. P., Naumann, L. P., & Soto, C. J. (2008). Paradigm shift to the integrative bigﬁve trait Taxonomy: History, measurement, and conceptual issues. In O. P. John,
R. W. Robins, & L. A. Pervin (Eds.), Handbook of personality: Theory and research
(pp. 114e158). New York, NY: Guilford Press.
Kennedy, C., Mercer, A., Keeter, S., Hatley, N., McGeeney, K., & Gimenez, A. (2016).
Evaluating online nonprobability surveys. Retrieved 12th January, 2018 from
http://www.pewresearch.org/ﬁles/2016/04/Nonprobability-report-May-2016FINAL.pdf.
Khazaal, Y., Mathias, van, S., Chatton, A., Achab, S., Zullino, D., Rothen, S., &
Thorens, G. (2014). Does self-selection affect samples' representativeness in
online Surveys? An investigation in online video game research. Journal of
Medical Internet Research, 16(7), e164. https://doi.org/10.2196/jmir.2759.
Lowry, P. B., D'Arcy, J., Hammer, B., & Moody, G. D. (2016). “Cargo cult” science in
traditional organization and information systems survey research: A case for
using nontraditional methods of data collection, including mechanical turk and
online panels. The Journal of Strategic Information Systems, 25(3), 232e240.
https://doi.org/10.1016/j.jsis.2016.06.002.
Matthijsse, S. M., Leeuw, E. D. D., & Hox, J. J. (2015). Internet panels, professional
respondents, and data quality. Methodology, 11(3), 81e88. https://doi.org/
10.1027/1614-2241/a000094.
Oakes, W. (1972). External validity and the use of real people as subjects. American
Psychologist, 27(10), 959e962. https://doi.org/10.1037/h0033454.
Reips, U.-D. (2000). The Web experiment method: Advantages, disadvantages, and
solutions. In M. H. Birnbaum (Ed.), Psychological experiments on the internet (pp.
89e118). San Diego, CA: Academic Press.
Rife, S. C., Cate, K. L., Kosinski, M., & Stillwell, D. (2014). Participant recruitment and
data collection through Facebook: The role of personality factors. International
Journal
of
Social
Research
Methodology.
https://doi.org/10.1080/
13645579.2014.957069.
Stewart, N., Ungemach, C., Harris, A. J. L., Bartels, D. M., Newell, B. R., Paolacci, G.,
et al. (2015). The average laboratory samples a population of 7,300 Amazon
Mechanical Turk workers. Judgment and Decision Making, 10(5), 479e491.
Retrieved
from
http://www.ucl.ac.uk/lagnado-lab/publications/harris/
StewartEtAl_JDM_MTurk.pdf.
Sturgis, P., Kuha, J., Baker, N., Callegaro, M., Fisher, S., Green, J., & Smith, P. (2017). An
assessment of the causes of the errors in the 2015 UK general election opinion
polls. Journal of the Royal Statistical Society: Series a. https://doi.org/10.1111/
rssa.12329 [RSSA12329].
Thomas, K. A., & Clifford, S. (2017). Validity and Mechanical Turk: An assessment of
exclusion methods and interactive experiments. Computers in Human Behavior,
77, 184e197. https://doi.org/10.1016/j.chb.2017.08.038.
Van Lange, P. A. M., Schippers, M., & Balliet, D. (2011). Who volunteers in psychology
experiments? An empirical review of prosocial motivation in volunteering.
Personality and Individual Differences, 51(3), 279e284. https://doi.org/10.1016/
j.paid.2010.05.038.

