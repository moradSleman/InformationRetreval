Process
TracingMethods

ProcessTracing
Methodsin Decision
Support Systems
Research:Exploring
the Black Box
By: Peter Todd
Assistant Professor of Business
Administration
College of Business Administration
University of Houston
Houston, Texas 77004
By: Izak Benbasat
Professor of MIS
Faculty of Commerceand Business
Administration
University of British Columbia
Vancouver, Canada V6TIY8

Abstract
Anoverviewis providedof the appficability to
DSSresearch of process tracing methodologies in generaland verbal protocol analysis in
particular. Rationaleis developed
for whyprocess tracing methodsare an important addition to the inventory of methodologies
available to researchers,stressing the needto explore the "black box" of decision processes.A
variety of processtracing methodsare presented,along with their relative strengthsand
weaknesses.
Verbal protocol analysis is discussedas a particularly promisingmethodfor
use in DSSresearch.Thenature of verbal protocols is outlined andmethods
for analysis are
reviewed. The criticisms of this methodand
the controversy surroundingits use are discussed, and an assessment
of the validity of
these criticisms is provided.Finally, areasof
DSSresearch in which protocols mayprove
valuableare identified andrelevantstudies are
examined.
Keywords: Protocolanalysis, data collection,
data analysis, researchmethods,
decision support systems
ACMCategories: H.O, J.O

Introduction
Muchof the prescriptive literature in the area
of Decision Support Systems(DSS)places
considerableemphasison the needto properly understandthe processof decision making
in order to designeffective systems
[54,82]. In
spite of this, .few studies that examinethese
relationships haveemerged
in the DSSliterature. Interestingly, early DSSresearch[76]
emphasizeddecision processeswhile subsequent empirical work has concentrated, for
the most part, on a moretraditional inputoutput approachto analysis. This is likely
due, in part, to the mostcommonly
knownresearchtechniquesnot being well suited to the
investigation of process at the required
amountof detail and rigour.
In order to moveinto a moreadvancedstage
of researchin DSS,one that permits not only
a demonstrationof the effects of computerbased systems but also provides an understanding of whyand howsuch systemsare effective, researchtechniquesare neededthat
permit us to examinethe pre-decisional behavior that takes place whengroups and individuals use these systems. A numberof
such techniques have been proposed and
usedby researchersin the decision sciences
[69], accounting[59], and marketing[11]. A
brief descriptionandevaluationof thesetechniques are provided in Section 3. Themajor
emphasis
in this paperwill be specifically on
protocol analysis which has been well
groundedin the study of humaninformation
processing.Wewill assert that protocol analysis is a useful tool for researcherspursuing
questions pertaining to both the design and
use of DSS.
First, a rationale is developedshowingwhya
processtracing methodology
is an important
addition to the inventory of methodologies
available to DSSresearchers.This is followed
by an overviewof the different processtracing
methodsused by researchersin the administrative sciences.Thenext three sections discuss the nature of protocol analysis, howto
¯ apply it, and someof the controversiessurroundingits use. Thelast section discusses
the potential contributions of protocol analysis to DSSresearch.

MIS Quarterly~December1987 493

Process
TracingMethods

The Needfor Process
Tracing Methods
A substantial bodyof researchhasinvestigated the effects of various forms of decision
aids and information presentation methods
on managerial decision-making effectiveness. Among
the dependentvariables used to
measure
th’ese effects are: profit, time taken
to makedecisions, confidence in decisions
madeandperceptionsof the quality of the information system[6,7,24,60,62].
Theseexperiments essentially use a dependent measureto represent the outcomeof the
decision-making process based on the influence of someindependenttreatment variable, suchas the availability of a decisionaid,
differences in presentationformat, or structure of the
information presented
[5,6,7,32,33,62].
In general, the purposeof this type of research is to examinethe values assumedby
the dependent variables based on a systematic variation of the independentvariables, with the intervening processconsidered
as a "black box" that is left unexplored.In order to be able to makesomea priori statementsaboutthe direction of the causeandeffect relationships, researchers rely on the
characteristics of the task, of the decision
makers, and of the decision environment. A
large proportion of the experimentalresearch
in information systems(IS) within the last
years has relied on psychological theory to
formulate and test hypotheses[10, 93]. Recently, subsequentto Huber’s [47] severe
criticism of the overemphasison cognitive
style variables in IS research, there appears
to be an increasedemphasison using task [9,
49] and environmentalcharacteristics [8] to
formulate hypotheses.
In summary,the research to date can be
characterizedas deductive; researchershave
attemptedto infer the causesof the outcomes
effected by different information systems
basedon the contextural factors described
above, while paying little attention to the
processes which intervene betweenthe independentvariable and the dependentoutcome.
In our opinion, there is a needfor methods
that "openthe black box," and allow research-

494

MISQuarterly~December
1987

ers to understandthe changestaking place in
the decision processdue to the influence of
DSS.Thesemethodsshould reveal a greater
variety of outcomeseffected by the antecedent condition without necessarily being restricted to a limited set of responsemeasures
(Swieringaand Weick,1982). Whiletraditional experimental research techniques demonstrate that the use of decision aids influence
decision-makingquality, whyor howthis occurs is unclear.
In the domainof cognitive psychologymuch
of the work donefollows the standardinputoutput paradigm and has had somesuccess
in developingprocessmodelsto describe empirical results. (Lachman,Lachman
and Butterfield [56] and Anderson
[3] providegeneral
overviewsof input-output oriented researchin
cognitive psychology.) Whydoes this approach not work in DSSresearch? The answerto this likely lies in differencesin the type
and complexity of the problemsbeing investigated. The input-output approachin cognitive psychology
is typically applied to small,
highly structured tasks where performance
and responseis measuredin secondsor milliseconds. However,in studying DSSweare
typically interested in highly complex
decision
situations that persist over time. In realistic
settings, input (problemstatement)and final
choice (output) can be separatedby a matter
of hoursor days. Evenwhensuchtools are investigated for researchpurposesin laboratory settings, the tasks are typically complex
andunstructured(e.g., budgetallocation, investmentdecisions, or market forecasting).
Theseproblemsand settings tend to be much
richer than those dealt with in moststudies of
decision makingundertakenby cognitive psychologists.As a result it is not surprisingthat
studies which manipulatesomeinput variable
and measureoutput in terms of performance,
confidence,satisfaction or decision time give
little insight into the influenceof a particular
DSS.Thereare simply too manypotential interveningvariables. As a result there is a tendency to movefrom input-output analysis to
data collection techniques which capture
someaspects of what occurs betweenstimulus and response.Payne,Brownskinand Carroll [69] havearguedthat the studyof process
anddecision strategy are essential in order to
properly understanddecision behavior, and
have demonstrated
the value of this approach
in the decision sciences.

Process
TracingMethods

While different approachesto solving the
same problem maylead to different outcomes, the sameoutcomemaybe generated
through different problemsolving approaches
or strategies (i.e., there are manyroutes to
the sameconclusion). Unless we have some
wayto assessthe strategies employed,evaluating the influenceof a DSSis difficult. In examiningDSSuse, determining the quality of
the analysis maybe easier than determining
the quality of the final decision. Decision
processes can be examinedby contrasting
observedbehavior with normativemodels,by
comparing
the scopeof the searchfor alternatives relative to the numberof alternatives
available, or by examiningthe numberof simplifying assumptionsexplicitly madeduring
an analysis. Theserepresent someapproaches to determiningthe quality of the decision
makingprocess that maynot be evident by
examining decision outcomesalone. In this
sense processis important.
But how can DSSresearchers capture the
components
of a decision process?A host of
techniquesfalling under the general label of
process tracing methods,have been used by
researchersinterested in understandingdecision makingprocesses. Theseare discussed
below.

Methodsfor Process
Tracing
Twoprincipal approacheshave beenused to
assessindividual judgementand choice within the Human
Information Processing framework: regression (mathematical) modelling
and processtracing methods.~Wewill briefly
discuss regression modelling and explain
whywebelieveit is not particularly applicable
to DSSresearch before providing a moredetailed review of commonlyemployedprocess
tracing methods.

Regression modelling
Regressionmodellingattempts to find mathematical modelsto predict an individual’s output basedon the input cues being processed
rather than attempting to gain accessto individual problemsolving processes[28, 29,
80]. While this approach has been widely
usedin accountingresearch[59], it has not,

to the best of our knowledge,beenemployed
in any study of decision makingin the DSS
area.Wedo not viewit as a particularly useful
tool for furthering our understanding
of howto
design DSSbecauseit does not provide any
clear insight into decision-making
processes.
While the modelmaylead to an outcomeconsistent with that of the decisionmaker,the underlying cognitive processesby which the individual arrives at the decision maybe radically different fromthat depictedby a particular mathematicalmodel[78, 91].
Anothercriticism of this approach
is that it is
applicableto the studyof tasks that are well
defined, havingperfectly reliable information,
and dealing with a single dependentvariable
[35, p.200]. To the extent that DSSresearch
is concerned
with problemsthat typically violate oneor moreof thesecharacteristics there
wouldseemto be little value in using mathematical modelling approaches.

Process tracing
Process tracing methodsprovide someaccessto activities that occurbetweenthe onset
of a stimulus and the eventual responseor
choice. As a whole,theseare likely to offer a
more comprehensive meansof evaluating
and understanding the behavior of DSSso
that extracting appropriateinformationfor design and for the evaluation of DSSperformancecan be done. There are a variety of
methodsthatcan be groupedinto the process
tracing categoryincluding information display
boards, tracing of eye movements,computer
logs, written protocols, andverbal protocols.
Wewill examinethe strengths and weaknesses of each of these methodsbefore proceeding with a moredetailed analysisof the verbal
protocol methodwhich wewill argue is the
most comprehensivetechnique.
Information Display Boards
Information display boards are used to observe the search patterns exhibited by individuals whenselecting amongalternatives
in typical choice problems.By specifying a
particular alternative (e.g., vendornumber)

1Einham,KleinmutzandKleinmutz[36] providea
detailedanalysisof therelationship
between
mathematicalmodels
andprocess-tracing
models.

MIS Quarterly~December1987 495

Process
TracingMethods

andparticular attribute (e.g., cost) the subject
uncoversa particular cell of the alternativesattributes matrix to obtain information. Data
collected from these studies can be used to
assessinformation utilization by measuring
the types of attributes selected and the frequencyof their selection. Moreover,the sequenceof selections can be used to test
different modelsof the decision process. It
has beena widely usedtechniquein the study
of consumerchoice [67, 68].
Theprincipal weakness
of this methodis that
it only provides the researcher with data
about the initial use of the information requested; the experimenterknowsvery little
aboutits subsequentuse and influence on the
decision strategy oncethe information is revealedto the subject. In somestudies [68] this
problemis overcomeby covering the information displayed after it has been accessed.
Subjects requiring accessto the sameinformation again have to reselect the same
attribute-alternative pair. Evenunder these
conditionsthe data collected is limited to information acquisition and insights about
processingmust be inferred.
Tracing of Eye Movements
Onemethod that overcomesthe deficiency
foundin information boardsis the tracing of
eye movements
[71, 72, 74]. The objectives
are similar to thoseof the explicit information
search techniques--sequencesof eye fixations andthe duration of eachfixation provide
data on different processing strategies and
relative information use, respectively. This
providesan indication of whatinformation is
being examined at the outset and subsequently during analysis. Datacan be extracted fromthe analysis of videotapes or fromdirect measurementof eye movementsusing
electronic monitoring devices. The latter
tends to be moreaccurate but also moreobtrusive [69]. Thedeficiency with this means
of
capturingprocessdata is that it doesnot provide any direct measureof the weight or importancewhich individuals attach to particular items of information. Furthermore, eye
movements
do not necessarily tell whatinformation a subject acquires, processes, or
evaluates [74]. The primary advantagesattributed to eyefixation are that theyprovidea
great deal of detail andare difficult for subjects to censorthus giving a highly veridical
processtrace [71].

496

MISQuarterly/December1987

Computer Logs
Theuse of computerlogs to monitor interactive decision makingactivity is particularly
relevant to DSSresearch since data can be
collected unobtrusively while the subject is
using the system [76, 82]. The weaknessof
this method
is that it providesno specific insight into an individual’s evaluationor assessmentof the availableinformation.It also indicates only actions a subject did take, but not
paths which were explicitly considered but
ruled out.
Written Protocols
Written protocols, not a commonly
usedtechnique, provide increased detail on specific
operations that individuals undertake while
manipulating information. However,because
of the relatively lowrate at whichwritten informationis recorded,the densityof data collected is quite low. Bartlett [4] andCell6rier [23]
provide examplesof the use of written protocols.
Verbal Protocols
The final methodto be consideredis verbal
protocol analysis [39, 40]. Here the thought
processes of the decision makerare taperecorded and subsequently analyzed. This
approach provides access to what information is examined,the manipulationsconducted on the input stimulus and, additionally,
what evaluations or assessmentsare made
by the problemsolver. It mayalso point out
the use of information external to the task,
i.e., retrieved from long term memory
during
problemsolving. Thecapture of protocols has
traditionally beenmade
by recordingaudio information only. However, complete videotape traces can be madethat give accessto
the additional insight available from nonverbal cues.
The techniques discussed above are not
mutually exclusive. Payne, Brownstein and
Carroll [69] andRusso[71] arguethat a multimethodapproachin processtracing research
not only providesgreater suppSrtfor the theory being tested, but also servesas a source
of validation for eachof the individual methods. Examplesof multi-method approaches
can be found in worksby Botkin (verbal protocols and computerlogs) [15], Payne(verbal
protocols and information boards) [68], and
Russoand Dosher(eye fixations and verbal

Process
TracingMethods

protocols) [72]. Johnson, Payne, Schkade
and Bettman[50] have developeda software
tool to collect data that combines
the basic attributes of information display board, computer log, and eyefixation techniques.In the
IS field, Jarvenpaa, Dickson and DeSanctis
[49] argue for the supplementary
use of protocol analysis in traditional experimental
studiesas part of the processof validating the
researchdesign.
The remainderof this paper wilt concentrate
specifically on verbal protocol analysis which,
in our opinion,is the mostpowerfulof all process tracing tools for discoveringthe dynamics
of problemdefinitions, hypothesisformation,
and informationsearchin less structured contexts. Verbal protocols provide the greatest
data richness [71] and information value per
data point. Informationboardsidentify initial
accesspatterns, while eye movements
give a
morecompletepicture of information attention, and computerlogs showboth what information is accessedand the manipulations
used on that information. Verbal protocols
havethe potential to yield all the information
obtained in the preceding. While techniques
such as eye fixations maygeneratea greater
amountof data per unit of time than do verbal
protocols, weargue that eye fixation data
does not take on the range of information
providedin a verbal protocol. Its focus is on
information acquisition while the verbal protocols provide indications as to whyinformation was accessed and the importance attachedto the information and the evaluation
techniquesused to analyze the information.
In this sense,the verbal protocol providesthe
richest sourceof data for analyzi.ngdecision
activities.

Methodsfor Collecting
Protocol Data
A summary
of the historical developmentof
protocol analysis is providedby Ericssonand
Simon[40]. Duncker[34] and DeGroot[30,
31] were amongthe first to utilize protocol
analysis to measureproblemsolving from an
HIP (HumanInformation Processing) perspective. Theformalization of protocol analysis as a methodologycamewith the publication of Human
Problem Solving by Newell and
Simon[64]. The most complete coverage of

protocol analysis to date is providedby Ericsson and Simon [40]. Russo, Johnston and
Stephens[73] provide the mostup-to-date informationon the validity of verbal protocols.
Theterm "protocol analysis" is often defined
poorly and used loosely to refer to various
methodswhichutilize different means
of both
capturing and analyzingverbal data. Theprecise nature of howprotocols are collected is
important since the various methodsto be describedin this section havedifferent strengths
and weaknesses.Theconfusion in terminology has sometimes
led to unfair criticisms of
the protocol analysismethodin generalas will
be describedlater in the section on criticisms
and controversies.

Retrospective vs. concurrent
protocols
Bouwman
[17] attempts to clarify the nature
of verbal protocols and to demonstratethat
varioustypesof protocols canbe collected for
differing researchobjectives. Oneof the main
discriminating features amongtypes of protocol analysisis the idea of concurrentversus
retrospectiveprotocols.
Concurrentprotocols involve having subjects
verbalize, or "think aloud," while engaged
in a
specific problemsolving task. Retrospective
protocols require individuals to recall their
processes,after having performeda particular task. Thisretrospectionresults in difficulties of memory
distortion, interpretation, and
inability to recall facts whichwerenot internalized in long term memory.Retrospectiveprotocols havethe advantage,however,of not interfering with the ongoing problemsolving
process. Concurrentverbalization is thought
to be a moreobtrusive method
of collecting information on problemsolving than are retrospective protocols [71]. Therelevant question
to ask of the concurrent protocol methodis
whetherthe use of the data collection technique hasa significant impacton the decision
processbeing traced. This issue will be addressedlater in the section on criticisms and
controversies.

Structured probing vs. neutral
probing
Thesecondfeature used to discrimate among

MIS Quarterly/December1987 497

Process
TracingMethods

protocol collection techniquesis whetherthey
are structured or neutral-probingprotocols. In
structured-probingthe subject is askedspecific questionsaboutthe problemsolving process, whereasin neutral-probingthe subject is
simply instructed to describe the process
without any structure imposedby specific
questions from the researcher. The advantage of structured-probing
is that it results in
a moreconcise protocol that is amenableto
analysis and easily comparableacross subjects. However,
to the extent that this probing
(particularly in concurrentprotocols)requires
subjects to attend to information not normally
available in short-term memoryor heeded
during problemsolving, processesmaybe altered by the act of verbalization [40]. In a
neutral-probing approach,the lack of structure in the data makescategorization more
difficult. This problemis oneof subjectivity.
Bias of an individual researcherin codingthe
data mayaffect the validity of the protocol.
However,
as will be describedin the next section, this bias maybe reducedby controls
placed on the analysis.
Out of the four options concurrent neutralprobing verbalizations are thought to be the
mostvalid andreliable method
utilized for formalprotocol collection [40]. In using concurrent neutral-probing protocols the subject is
simply askedto "think aloud" while simultaneously engagingin a problem-solvingtask, The
only intervention by the experimenteris to
promptthe subject to verbalize if there is an
extendedperiod of silence (typically, more
than 10 seconds).Thepromptis neutral, that
is the experimentersimply asks the subject to
speak or describe what he or she is doing.
The advantageof this type of procedure is
that the measureis relatively unobtrusive,
providedthe informationthat is verbalizedis
attendedto during problemsolving and is in a
form that does not require excessive encoding to be verbalized [40]. Theprimary disadvantage of this methodis the volumeof unstructured data produced. Muchof the data
containedin a protocol mayhavelittle relevanceto the problembeing investigated. This
meansthat researchers have to go through
the long and tedious process of separating
the useful informationin the protocol fromthe
irrelevant data whichis not related to the research question. Throughoutthe remainder
of the paper,all usesof the term"protocol"will

498

MISQuarterly/December
1987

specifically refer to concurrentverbal protocols.

Methodsof Analyzing
Verbal Protocols
In addition to the manytypes of verbal protocols, there are various methodsby which
protocols can be analyzed.Theprinciple discrimination amongthese analytical methods
is the depth to which the problem solving
processis investigated. Bouwman
[17] identifies four majorcategoriesof protocol analysis. Theseare:
Scanning:examining the protocols for
(frequently anecdotal)information that assists in interpretingother(usually quantitative) observations.
Scoring:tabulating the frequenciesof certain key itemsof interest.
Global Modeling: formulating flowcharts
and algorithms that capture the decision
making process.
ComputerSimulations: developing executable computerprogramsthat simulate
the detailed, step-by-stepdecision making
behavior, and that reproducethe process
flow reflected in the protocol.
Each subsequent method involves delving
with increasing depth into the problemsolving process. Scanning is the most
straightforward method,and can be used to
aid interpretation of statistical modelsand
maytake little time or effort. Scoringusually
involves developing a coding scheme by
which the protocols will be broken down,
tabulating the frequency of specific occurrencesand performingstatistical testing on
the results of the aggregation. Accordingto
Bouwman,
this is an efficient and rapid method of protocol analysis. Global modellingand
computersimulation are dependentuponthe
initial scoringof the protocols.
Simon[79] has raised a numberof theoretical
and methodological problems associated
with the use of scoring, including the objectivity andreproducibility of the codingof protocol data andthe amountof effort required in
coding. This difficulty is generally offset by

Process
TracingMethods

having two (or more) coders and insistingon
high intercoder agreementprior to undertaking any analysis of the scoring [69, 87].
Hastie, Penrodand Pennington[42] provide
an excellent illustration of the methods
avail2
able for assessing intercoder agreement.
Anotherdifficulty arises with the development
of the codingsystemitself. In codingthe protocols, it is essentialto ensurethat all informationrelevant to specific hypothesesbeing
tested is capturedand codedaccurately with
no irrelevant informationbeingincluded[16].
This implies that protocol coding schemes
shouldbe developed,
at least in part, a priori.
A priori determination ensures that the
"findings" of the study are not data-driven.
Strict independenceis maintained between
hypothesis formation and data analysis. The
dangerof this procedureis that someimportant, unanticipated results maybe overlooked.
Global modelling involves a direct examination of the processesof problemsolving. This
approach showsnot only which actions are
taken during problemsolving, but also the sequenceof their execution. A flowchart of the
problem-solvingprocessis generally the result of this activity (see [18] for an example
in
the areaof financial diagnosis).
Computersimulation involves developing a
modelthat will accurately represent and reproducethe decisions madeby an individual
subject from whoma protocol has been collected. Theobjective is not merelyto produce
a similar output but also to follow the same
"reasoning" pattern as the individual. The
successful implementation of a computer
simulation, which reproducesthe individual
protocol, automatically validates the accuracy of the protocol data by showingthat the information contained in the protocol is
sufficient to reproducethe decision process
and outcome
as it took place. This validation
givesa reliable indication that the individual’s
processing has beencaptured at a reasonable level of detail. In order to fully determine
if the simulationmodelis a valid representation of the problemsolving process,additional tests are needed.The computersimulation
should be comparedagainst the behavior of
the original subjecton the same
task for different casesto test the model’spredictivevalidity [25]. A single test canbe basedon comparing both the decision outcomesand the pro-

cessesthat led to these outcomes.Multiple
tests can be accomplished
in a fashion similar
to a Turingtest [46, p. 595-99].

Criticisms and
Controversies
This sectionof the paperwill discussthe criticismstypically leveled at the use of protocol
analysis (namely,the veracity of protocols),
the impactof collecting protocols on the decision process, and the completeness
of verbal
protocols.
Veracity of protocols
Majorcriticisms of the useof verbal protocols
in the study of cognitive processeshavebeen
put forth by Nisbett and Wilson [65]. They
suggestthat subjects do not have accessto
their higher order mentalprocessesand cannot give accuraterepresentationsas to what
they do, and moreimportantly as to whythey
do it. Theyclaim that peopletend to tell more
than they can know.
Thus,Nisbett andWilsonseelittle valuein the
useof suchintrospective methods
as a tool to
collect data about cognitive processes.However, all studiesthat wereanalyzed
in their review used retrospective verbalizations, and
their general condemnationof the protocol
methodology
can only be applied to this particular typeof data collection [69].
Furthermore, somecognitive psychologists
arguethat Nisbett and Wilson’scriticisms apply only undercertain conditions.Theissue is
not whetherindividuals haveaccessto cognitive processesbut underwhatconditions this
accesswill be accurate. Nisbett and Wilson’s
criticisms appear to only apply to those
retrospective protocols, in whichsubjects are
not motivatedto maintain accurate reporting
as a primarygoal [81, 89, 92].

2 Hastie, PenrodandPennington
[83] also demonstratetheuseof procolsin a groupdecision
setting
as a means
to captureandanalyzegroupinteraction. Thistreatment
wouldbeof interest to those
contemplating
researchin the areaof Group
Decision SupportSystems.

MIS Quarterly/December1987 499

ProcessTracingMethods

Impact of collecting protocols on
the decision process
According to Ericsson and Simon[40], under
specific conditions the useof concurrentverbal protocols will alter neither the problem
solving process nor the time taken to solve
the problem if collected in an unobtrusive
"think aloud" manner.Theseconditions are
that the subject is required to report only the
contents of short term memory
(i.e., they are
not required to explain why they are doing
something), and that the task in which they
are engagedlends itself to reporting in oral
form without recoding. Taskswith essentially
pictoral representationsare not consideredto
be goodcandidatesfor protocol analysis.
Russo, Johnston and Stephens[73] provide
an up-to-datereviewof the literature assessing the reactivity andveridicality of verbalprotocols. Theyreport on sevenstudies that had
a primary goal of testing whetherconcurrent
verbal protocols significantly changea primary decision process(i.e., are reactive). The
only changefound in these studies wasin
someinstances longer responsetimes. Outcomemeasuresof performancewere not significantly affected. Theseand other studies
which have included checks for decision
process changes, typically by comparing a
protocol condition to a silent problemsolving
treatment, havebeenthe basis for the conclusion that unobtrusivethink-aloud protocols do
not result in fundamentalchangesto process.
However, Russo, Johnston and Stephens
provide additional empirical evidence that
verbalization does influence a numberof
tasks that the Ericsson and Simon model
would otherwise predict to be good candidates for protocol analysis. Their paperprovides a detailed set of instructions to use in
protocol studies to filter out these influences
and is recommended
for anyonecontemplating the useof protocol analysis.
Schweiger [75] tested the contention that
simultaneousverbal protocols impact decision processesand performancein a complex
managerial decision making task, and concluded that there was no such impact. He
suggeststhat verbal protocol analysis is an
accurate and unobtrusive methodologyfor
studying individual problem-solving and
decision-making processes for managerial

500

MISQuarterly~December
1987

problems.This study is particularly relevant
to DSSresearchsince it usedan unstructured
business decision makingtask.

Completenessof protocols
Ericsson and Simon[40] cite three factors
that could affect the completeness
of verbalization: the degreeto whichtask processingis
automatic, cognitive strain imposedby the
task, and needto recall information fromlong
term memory
in order to completethe task.
An automatic(i.e., overlearned)processhas
no intermediate results stored in short-term
memoryand thus the process steps cannot
be verbalized. Cognitive processesrequire
that the outputsof intermediateoperationsbe
available in short-term memory,which allows
themto be verbalized. To the extent that a
subject has automated processes prior to
testing, the protocols will give an incomplete
representation of the intermediate stages of
problemsolving. This is whythe trace of an
expert’s decision process appearsless complete than that of a novice[40, 57]. Similarly,
if a subject gains significant experiencein a
task over the courseof an exercise, the protocols will deteriorate, becomingless complete over time.
Whenan individual is engagedin a task causing excessivecognitive strain, they maystop
verbalizing or give very incompleteprotocols.
Attemptsto verbalize under these conditions
will usually be evidencedby a gradualdecline
in performancerather than a completeoverload of the processingsystem[66].
A final factor that maylead to incompleteness
in verbal protocolsis the needto recall information from long term memory.According to
Ericsson and Simon[40], such retrievals are
fallible and highly dependentuponthe cues
that are givento initiate the recall. Accordingly, a task that relies extensively on detailed
unaided memoryrecall maybe a poor candidate for protocol analysis.
In short, information containedin verbal protocols is baseduponwhat is available from
short-term memory.At best the protocols
yield only "glimpses"of the underlyingmental
processes [43]. However, simply because
protocols provide incompleterepresentations
of problem-solvingprocessesis no reason to

ProcessTracingMethods

dismiss themas either irrelevant or inaccurate.This criticism canbe appliedto virtually any methodof self-reported data, including
questionnaires and interviews both of which
are acceptedas valid data collection tools. In
fact, verbal protocols arguably provide more
completerepresentationsthan mostif not all
other self-reporting data collection techniques.

Costsof protocolanalysis
Whileprotocols provide an important addition
to the inventory of methodsavailable to DSS
researchers, they should not be seen as a
panacea.
As is the casewith virtually everyresearchmethodology,there are certain tradeoffs that mustbe evaluatedprior to makinga
decision to engagein protocol research. Even
if protocolsare administered
carefully (i.e., in
a concurrent, non-probingmanner),there are
certain drawbacksassociated with their use
as the primary data in any study. Thesedrawbacksstem from the nature and volumeof the
data generatedin a protocol. Dueto the high
density of data that will be foundin a single
verbalization, samplesare usually very small,
commonlybetween2 and 20. The coding and
analysis of a protocol are labor-intensive activities and are estimated to take several
hours for each minuteof verbalized information [63]. Naturally, the smallsamplesizes also makethe application of standardstatistical
proceduresdifficult, and there are problems
of subjectivity in coding the protocol data.
Nevertheless,protocols are probablythe best
methodavailable for providing accessto individual cognitive processes.

Useof Protocol Analysis in
DSS Research
In this section of the paper,howprotocolanalysis canbe utilized by researchersinterested
in developing and evaluating the impact of
DSSis discussed.Since there is no paradigm
or theoretical modelyet developed
in the field
of DSSto assist in identifying important researchissues[83, p. 10] wewill haveto settle
for no morethan a definition of DSS.A commonly accepted definition of DSSfound in
authorativearticles will be usedfor this purpose.In particular, wewill identify thoseissues of interest to researchers and de-

velopersof DSSfor whichthe use of protocol
analysis could enhancethe current level of
knowledge. Wewill then examine how researchusing protocol analysis hasinvestigated these issues and offer somesuggestionas
to whereit might be usedfurther.
Decision support systems are defined as
computer-basedsystems that help decision
makers confront ill-structured
problems
throughdirect interaction with data analysis
and models[84, p. 1]. Thecomponent
parts of
a DSSare a data base, a modelbase, and a
human-computer
interface subsystem, which
managesthe interaction betweenthe user
and the computersystem.
A numberof research issues are associated
with those characteristics of a DSSthat are
uniqueto it as a computer-based
information
system:
1. The emphasison supporting and helping
the decision makercalls for a general understandingof decision-makingbehavior.
This is necessary
to identify biasesandinconsistencies in the process so that the
designer knowswhat kind of support tools
to devisein order to alleviate the problems
anddeficiencies associatedwith the limited capacity of humaninformation processing systems.
2. Since DSSis concernedwith semi- or illstructured problems, the lack of welldefined methodsfor handling these problemsnecessitatesthe elicitation of knowledgefrom problem-solversso that the designer is awareof the heuristics, methods,
and information sources used to solve a
particular problem.
3. DSSis an interactive systemused mainly
by discretionary users. As such, muchof
its power,usability, andflexibility are derived from the quality of the interface between the user and the computer.
Giventhat a DSSwith a poor interface is
not likely to be usedregardlessof the quality of its data and modellingcapabilities,
the design of a good human-computer
interface is of high importancein developing
DSS[21, 82, 85].
4. Improvingthe effectiveness of a decision
makeris a key evaluation criterion for a
DSS.Effectiveness is associatedwith adaptation, learning, and respondingto the
changing nature of the problems being

MIS Quarter/y/December1987 501

Process
TracingMethods

handled. Experiencehas Shownthat these
benefits of a DSSare mostly qualitative
anddifficult to define andestimatea priori
[53]. The evaluation of a DSSis thus
basednot only on simple output measures
but also on understandingthe changesin
the process of decision making.
The four research issues identified are associated with building the component
parts of
a DSS.Understandingthe principles of humaninformation processingassists in the developmentof modelbases that have a set of
generic support tools for problems common
to mostapplications. Elicitation of the knowledge and methodsused to solve particular
problemshelps in the design of both data and
modelbases for specific DSSapplications.
The study of human-computer
interaction is
necessaryfor the design of a goodsysteminterface. Finally, the issue of evaluationis an
examinationof the contribution of each component individually, and as a group, to improving the effectiveness of decision makers.
Thewaysin which protocol analysis can contribute to our understandingof the issues of
particular interest to the DSSfield are discussedbelow. In addition, for each area we
will identify howprotocol analysis canbe used
as a research tool basedon Payne, Braunstein andCarroll’s [69] categoriesof: exploration and hypothesis generation, hypothesis
testing, building computer models of behavior, or supplementingother methodsof
data collection.

Understanding the general
principles of humaninformation
processing
In developing a DSSthere are two types of
aids that can be providedto the users: those
that are associatedwith the specific application and those that provide generic support
mechanisms.
Examplesof the latter are whatif andsensitivity analysisfeatures, as well as
alternative formatsfor the display of information. Since the key design emphasisis on
supporting and extending a manager’sdecision-makingcapabilities, an examinationof
how the humancomponentfunctions is necessary to developthe generic components
of
DSS.This examinationis necessaryto identi-

502

MISQuarterly~December
1987

fy the strengths and weaknesses
of the humandecision maker, the types of heuristics
commonlyused, and the biases in judgement. Basedon these observations, decision
aids that alleviate the limitations associated
with humaninformation processing and improving judgementcan be designed. By identifying the specific requirementsof decision
makers, the mismatchbetweenthe capabilities of the DSSand the decision maker’s
needs can be minimized.
The best knownexampleof the use of protocol analysis to understand
the principles of
humaninformation processingis found in the
workby Newell and Simon[64]. Their studies
havehelpedto identify the basic capabilities
and limitations of the humanas an information processor.Humans
havea relatively slow
serial processorwith small short-term memory whichlimits the waysan individual can approachthe solution to a problem.Thoseareas
where the decision maker faces a binding
constraint also indicate whereDSScapabilities should be focussedto augmentthe capabilities of the individual as a problemsolver.
For example,Carlson [20], basedon Newell
and Simon’s work [64], concludesthat problem solvers need memoryaids and suggests
several types of DSStools, including workspacesto store intermediateresults, and triggers to reminddecision makersof the operations that mayneed to be performed.
Studies by Payne[68], Elstein, Shulmanand
Sprafka [38], Bouwman
[16], Biggs [12], and
Biggs and Mock[13] using protocol analysis
provide a general idea about the decision
makingstrategies usedby experts in complex
task domains.For example,in general, expert
decision-makersrely on a standard list of
questionsfor information search, relate evidence to prototypes of problem solutions
stored in associative memory,and use a
small numberof hypothesizedsolutions to organize patterns of data in short-term memory
and to guide the searchof additional information [59]. However,usually a small numberof
alternative hypothesesare evaluated, and
evidence that does not support the current
hypothesis being considered is ignored. A
numberof decision support aids are recommended to remedy such problems [59].
Theseaids include standardchecklists to ensure that certain hypotheses
and data related
to themare considered,"fault trees" that force

Process
TracingMethods

the decision makerto consider all possible
causes in diagnosis problems, and models
that augmentthe human’sestimation ability
with that of the computer’sto combinemultiple sourcesof information.
In pointing out howprotocol basedworkin humaninformation processing can provide insight into generic DSSdesign issues, weare
not advocating that DSSresearchersconduct
pureinformation processingstudies, but rather that they attempt to properly evaluateand
integrate the large bodyof researchdonein
this areaby decision scientists and cognitive
psychologists. In this sense, muchof the
process tracing work conductedprovides a
useful basis for the development
of empirically basedresearchquestionsinto the utility of
manygeneric DSScomponents.
Anotherrationale for couplingprocesstracing
research in DSSwith general studies in humaninformation processingis to extendcurrent knowledgeabout the capabilities of human processors. The computer has been
used extensively in psychologyas a tool for
modelling cognitive functions. Conducting
studies on human-machine
systems can provide an understandingof the humanprocessing system under conditions wherethe constraints on the system,especially with regard
to computational efficiency and short-term
memory,are relaxed by the introduction of
computerizedsupport tools.
Theprotocol studies conductedin this area
havebeenmainly exploratory with theory developmentas a major goal. Clearly, the work
of Newelland Simonhas led to the generation
of a theory of human
problemsolving. Similarly, other studies mentioned[38] have explored the differences betweennovice and
expert problem-solvingstrategies. Somerecent workhoweverhas dealt with testing specific hypotheses
aboutthe behaviorof expert
andnoviceproblemsolvers [87, 88]. In general, the body of knowledgegenerated from
these exploratory studies can provide a basis
for the design of generic DSScomponents.

Elicitation

of domain specific

knowledge
Bonzcek,Holsapple and Winston[14, p. 70]
associate the powerof a DSSwith its degree

of knowledge about the decision maker’s
problem domain. Protocols can be used for
the extraction of expert knowledge
to provide
a basis for the developmentof a DSSor expert system(ES) [70, 90]. Johnson[51] emphasizesthat "authentic methods"of reasoning capturedusing protocol analysis are the
most appropriate bases for the development
of expert systems. Using this method,process traces of one or moreexperts taken from
active problemsolving sessionscan be used
to form a knowledgebasethat contains rules
and facts for developing the modeland data
basesof the DSS.Notethat this is for the purposeof developinga specific application and
not for identifying generic tools for a wide
range of DSSas discussed in the previous
section. Thereare few explicit examplesof
the use of protocols in ESdevelopment;Cohen, Mayand Pople [27] is one exception.
Hansen
and Messier[41 ] report on an attempt
to useprotocol analysisto developa rule base
for an ESto assist in conductingEDPaudits.
In contrast to the lack of concreteexamples
in
the ESarea, the decision-making
literature is
replete with processstudies of expertise in a
variety of domains, such as medicine and
finance[25].
Bouwman
[18] used protocol analysis in a
study of studentfinancia~ analysts attempting
to diagnosethe financial position of a company. Heidentified the decision-making
processes involved, the strategies usedby the subjects, and the task specific knowledgethat
wasrequired. Basedon the analysis of 30 protocols, Bouwman
provides a general modelof
the diagnostic process, which includes the
phasesof problemdetection, selection of the
potentially significant findings for diagnosis,
integrating these findings, the formulation of
hypothesisabout potential problems,and the
selection of one problemas the final diagnosis. Of interest hereis that in orderto proceed
with its diagnosis,the computer,programbuilt
by Bouwman
requires an understanding of
the specific decision context to simulate the
humananalyst. Bouwman
states that protocol
data provided a wealth of information about
the kind andextent of task specific knowledge
usedby the subjects. Basedon this information, he developedan internal modelof the
firm showinga re~ationa~ network of cause
and effect betweenkey decision variables,
such as demand,market share and planned
production, and the type anddirection of the

MIS Quarterly~December1987 503

ProcessTracingMethods

influence of various variables on eachother.
This internal modelcoupledwith the general
diagnostic proceduresprovides a modelbase
for the task of diagnosingfinancial problems.
In other words, the generic processes for
problemdiagnosis, coupled with domainspecific rules and data, provide the basis for
building a DSSto help in the solution of this
task.
The modelwasthen "tuned" to replicate the
decision strategy of one particular subject. In
this waythe modelis basedupon authentic
reasoning methods [51]. Bouwmanspeculates that developingmodelswhich performin
this manner
will havetwo basic benefits. First,
such modelswill be moreacceptable to the
usersas they can be explicitly linked to their
own decision-making approach. Second, the
general weaknesses
of such modelsin dealing with ill-defined environments
will be overcomeby incorporating the capabilities of humandecision makersin this area.
Theobjective of the protocol studies in this
category has been exploration coupled with
the developmentof modelsabout the problem
solving behaviorof decision makersin narrow
but semanticallyrich domains,typically in the
form of computersimulations. Very little of
this workhas involved hypothesistesting.

Evaluating the human-computer
interface
Heafner [44] and Lewis [58] have proposed
the use of "think aloud"protocolsin interface
design as an effective meansof capturing
both a user’s approach to a task and why
problems occur when users interact with
computer systems. Theseare factors that
cannot be easily determined based upon
after-the-fact reports by users.Theuseof protocols wouldhelp identify wherethe user is
constrained by the system, thereby pointing
to new features for augmentingindividual
problem-solving
capabilities andfor facilitating use of systems.
A numberof process tracing studies have
beenundertakento examinehowindividuals
use computer systems. For example, Mack,
Lewis and Carroll [61] studied howpeople
learned to use text processingsystems.This
study contributed in determining the novice
usersbehaviorthat causeddifficulties in us-

504

MISQuarterly~December
1987

ing interactive computer systems. Novices
tended to makead hoc interpretations or excuses about whythe systemperformedin unexpectedwaysin order to explain conditions
that appearedto be out of the ordinary. They
also tended to makestrict mappingsfrom
their previous experiencewith typewriters to
the wordprocessingsystem.This use of analogyoften resultedin errors. Finally, theytended to exploresystemcapabilities without following the online tutorials andinstructions.
Thesefindings help to isolate the reasonsfor
novices’ difficulties in interacting with systems and demonstratethe value of protocol
analysis in the study of human-computer
interaction. Theprotocols provideddirect evidenceof the specific complexitiesof the user
interface that wereresponsiblefor difficulties
in learning.
In a similar vein, Carroll, Mack, Lewis,
Grischowsky and Robertson [22] evaluated
two types of tutorials for learning the useof
word processors: one commercially developed packageof self-study materials, the other
called a "guidedexploration"tutorial developed by the researchers to encourageactive
learning. In addition to collecting time and
performance measures, concurrent non-probing protocols were collected to examine
the type of behaviors exhibited by the subjects. For example, protocol analysis revealed the proportion of time subjects spent
in dealingwith the task in contrastto the times
spentreferring to the training manuals
for assistance. It also indicated the processof how
subjects proceeded
fromsetting a goal to performing a series of nested operations (subgoals) to accomplishthat goal, whichprovided information about how discovery and
learning took place in the different treatment
groups.
Aucellaand Ehrlich [1 ] usedprotocol analysis
to supplementdata collected through inter,
views and usagestatistics in evaluating computer-basedvoice messagingsystems. Protocol data, among
other things, identified which
terminal keys the users werehavingdifficulty
in locating andth.e points in the dialogueat
which the users were having problems. This
informationwashelpful in improvingthe interface by makingspecific parts of the user instructions moreexplicit and by adding"HELP"
facilities.

ProcessTracingMethods

The research to date on interface design
utilizing protocol analysis hasmainlyexamined those issues concernedwith the mechanics of systemuse, suchas abbreviation methods, error correctionoptions, andhelp facilities [19]. A neglectedarea of study has been
howto utilize the capabilities of the systemas
a managerialtool such as understandingthe
meaning,powerand use of the different commandsavailable at the different stages of a
manager’sproblem-solvingprocess, an issue
of greater importanceto DSSuse. Webelieve
that thereis considerable
needto test the utility of variousfunctionsbuilt into existing DSS.
Howcan interfaces be configured in such a
wayas to augmentthe problem-solvingprocess? For example, what are the differences
betweenpositional (e.g. LOTUS)
and procedure-based(e.g. IFPS) DSSgenerators?Protocol analysis can be useful for determining
how users interpret various functions and
what makesthese functions useful.
In the categoryof the human-computer
interface weseethe majoruseof protocol analysis
has beento supplementother data collection
methods.It has beenusedfor exploration, to
observethe type of difficulties decision makers face in interfacing with a DSS,andfor hypothesistesting to evaluatethe quality of contrasting interface designs.Webelieve there is
a large role for the useof protocol analysisin
testing specific hypothesesto determinedifferences betweenbasic interface types and
specific characteristicsof interfaces.

Evaluating decision-making support
Froma researchperspectiveone of the key issues in DSSis simply to determine, in some
concrete, demonstrablemanner, the influenceof a specific types of DSSor of generic
DSSdesign features on the quality of decision making. Keen[52] and Hurt, Elamand
Huber[48] state that this issue hasbeenoverlooked in the DSSfield and strongly recommendthat more work be undertaken in the
area of evaluation. Current literature often
refers to the impactof decision aids on decision processes[55]. However,webelieve that
there is no empirical base in DSSto back
these assertions. Weknowof no published
studies subsequentto Scott Morton[76] that
carefully evaluatethe impactof DSSon decision processes. Only through such evalua-

tions cantruly valid prescriptionsfor the use
of DSSbe derived.
In semi-structuredtask settings, whereit is
difficult to determine
the objectivefunction, it
maybe moreappropriate to try to determine
the degreeto which a decision aid improves
or affects the process,rather than to focus on
outcomesthat can only be ambiguouslydefined. As discussedpreviously, the value of
the decision aid mayaccrue not simply from
altering the final decision, but fromimproving
the manager’sdecision strategy by encouraging a morecompleteanalysis. In this sense,
we mayview the decision outcomeas a composite result of the problem-solvingstrategy
being employed. The examining process is
essentially a mechanism
for taking a complex
problem,in this case understandingthe impact of DSSon decision making, and decomposingit into sub-problems.
By looking at the
componentsof the decision process the researchercan moreeasily imposesomestructure on the evaluation of DSSimpact. Simon
[77] hasarguedthat oneof the primarydifferencesbetweenstructured and semi-structured problems is the degree of fineness to
which the problem has been decomposed.In
DSSresearch, greater insight might be
gained by examininghowthe system effects
sub-problems or episodes in a decisionmakingprocess,as opposedto trying to identify overall effects basedon a holistic evaluation of decision outcomes.In this sense, the
researcheris testing hypotheses,not simply
about differences in outcomevariables, but
also about changesin the decision process.
Weare awareof a numberof studies in progress that examinethe influence of a DSSon
the decision process by utilizing protocol
analysis, in somecases together with other
process tracing methods. The remainder of
this section will discuss the one completed
study, whichwehaveidentified in this area.
Elam and Mead[37] have examinedthe influenceof softwareon creativity. Twotypesof
decision support software that were designed
to aid creative decision makingwerecompared against a "no support" condition. Datawas
collected on both decision outcomesand processes,the latter by utilizing both verbal protocols and computerlogs. The process data
wasanalyzed to determine whether DSSuse
causedsubjects to adopta linear or nonlinear

MIS Quarterly~December1987 505

Process
TracingMethods

andsingle or multi-step decision process.Linear decision processeswere defined as those
whichfollowed a series of stepsincluded in a
decision-making model developed by Amabile [2]. Non-linearmodelswerethose that exhibited stages in the decision makingprocess, but did not follow a strict sequencing
of
steps. The software was expected to reinforce the use of a linear process.Single step
decision processes were defined as those
wheresolutions were reachedimmediately, in
an "off the cuff" fashion. Multi-step processes
representeddistinct stages in the decision
processsuchas the gathering of data, generation, andtesting of alternatives.
Eachprotocol wasbroken into episodesthat
represent predeterminedstages in the decision process. Subjects using the DSSadopted a linear approachand usedmultiple-steps,
whereasthe control groupdid not use a linear
procedureand often leapt immediatelyto decisions. Thetype of decision processinvoked
by the DSSis thought to be moreconducive
to creative problemsolving than the standard
approach. Unaided decision makingis less
conduciveto creativity than either.
Theuse of processtracing in this study appears to have beendone in a relatively efficient mannerby comparingsections of the
protocol to stages in the decision process.
Further exploration of the protocols might
have beenuseful to identify differences in
performancebetweenthe two treatment conditions and to determinehowthe systemcapabilities wereutilized in addressingboth the
qualitative andquantitative aspectsof the decision process. Also useful, wouldbe a more
direct comparisonbetweenexisting theoretical modelsof decision makingand the techniques used by the subjects in solving the
problem.
The testing of hypotheses concerning the
changesin decision processesand outcomes
effected by different support mechanisms
or
betweensupport and no-support conditions
shouldbe the majorapplication of protocolsin
the evaluationcategory. Giventhe scarcity of
DSSresearch in the area of evaluation and
the lack of formal evaluation of existing DSS
[45] this wouldseemto be the proper emphasis. Protocols can be utilized as supplementary to collecting morestructured time and
performancedata. thoughwethink that major

506

M IS Quarter/y/December
1987

findings are likely to be basedon protocol data. Theycanalso be usedfor exploratorypurposes, such as developing descriptive
models of the process of human-computer
problemsolving.

Concluding Comments
Theobjective of this paper has beento provide DSSresearcherswith a thorough review
of processtracing methodswith specific emphasis on protocol analysis. Thoughprotocol
analysis has not beenwidely used in DSSresearch it should be: Onearea that has been
virtually ignored is the examinationof the
effect of DSSon problem solving, an area
wherewebelieve the protocol methodis especially.applicable. In fact, researchinto DSS
seemsto have lost sight of the emphasisthat
its fundamentalstudies have placed on understandingprocess[76]. Apparentlythere is
a clear needfor the use of process tracing
methods
in the DSSarea in order to better examinethe impact of these systemson the decision process[82]. Specifically, verbal protocol analysis could provide knowledgeabout
howto design both generic and domain-specific models and data bases including the
human-computerinterface. Without such
knowledge, developing a comprehensiveset
of designguidelinesis difficult.
Wehave arguedthat DSSresearch has failed
to makereal progressbecauseof a lack of appropriate researchtechniquesto addressthe
complex problems facing researchers. The
basic difficulty is that the gapbetween
the introduction of a stimulus and the measurement
of outcomeis so large as to obscureany impact whentraditional input-output measures
are taken. Processtracing methodsallow for
the more refined measurementof what occurs duringthis time. Verbalprotocol analysis
is likely the richest, mostinformativeof the
process tracing techniques [71]. The basic
benefit of protocol analysis in the DSSenvironmentis the ability to answerquestions
concerning howand why the use of decision
aids improvesindividual problemsolving.
Verbal protocol analysis is a controversial
technique and has manydetractors (as well
as advocates).A balancedpicture of the work
doneon the validity of verbal protocols appears to showthat under specific conditions

Process
TracingMethods

protocolsare valid anddo providedata that is
not available from other methods[40]. Most
common
criticisms, suchas the failure to capture all processes,apply not only to verbal
protocol analysis but to all data collection
techniques. Wetherefore advocatethe use of
verbal protocols in DSSresearchin order to
better understandthe relationship of these
systemsto decision process.

6.

Coding is usually simplified by making
multiple passes through the data when
scoring. Examination
of the protocol once
for each operator in the coding scheme
rather than makingsingle passesto look
for all data is suggested.

7.

Data analysis will be time consuming,
possibly requiring hours for eachminute
of verbalization [63]. Our experienceis
that sessions of one hour can generate
between500 and 1000lines of verbal data. In order to achievelarge samplesizes,
there might be a preferencefor simpler,
reasonablyshort tasks.

8.

Theprotocol methodis controversial, and
the use of an unfamiliar researchtool is
boundto be provocative. For this reason
researchersmust be preparedto justify
the use of the methodand demonstrate
whyit is moreappropriate than another
methodfor answeringa given question.

To summarizethe discussion in preceding
sections,the following list of prescriptiveadvice is providedfor those actually contemplating the use of verbal protocol analysis:
1.

2.

Multi-methoddata collection approaches
are important, particularly whenthe
methods
in use are perceivedto be highly
subjective[69, 71].
If protocols are the primarydata source,
they should be concurrent and non-probing. Retrospective protocols are mainly
useful for gaining supplementarydata
[40].

3.

Thetype of analysis of the protocol data
should relate to the type of research
question being examined.If changesin
decision processesare of interest, both
scoring and global modellingare appropriate. If the frequencyof occurrenceof
specific operationsare of interest, simple
scoring maysuffice [17].

4.

Coding schemesshould be identified a
priori. This is importantto add rigour to
the researchandto facilitate the efficient
analysis of data. Collection of protocol
data without first identifying a scoring
schemewill undoubtably slow downthe
research process. Protocols tend to be
overwhelming,and unless a structure for
analysisexists theymaybedifficult to interpret.

5.

When
doing the codingthe use of at least
two codersis suggested.Preferably neither one of themshould be the principal
researcher.Validity of the analysisis increased by reducing researcher bias.
High inter-coder agreementis essential
before makinginterpretations basedon
the data [69, 87]. Cohen’s Kappacoefficient [26] is usually used to measure
inter-coder agreement.

Thefocus of this paper has beenon research
applicationsof verbal protocol analysis. It is
also possible that protocols can be used by
practitioners in the development
of applications systems.Thereis little difference betweenthe application of protocols for acquiring domain-s.pecificknowledge,or the testing
of interface quality by researchersor practitioners, exceptfor the formality with whichthe
techniqueis likely to be employed.Wewould
expect practitioners to makeless rigorous
useof thesemethods,
but they are still potentially useful tools in the systemsdevelopment
andevaluationprocess,especially for acquiring feedbackin an iterative design approach.
To summarize,
the lack of appropriatetools is
one of the majorstumblingblocks to progress
in the DSSarea. Webelieve verbal protocol
analysis to be a viable researchtool that addresses many basic questions for those
studyingDSS.It is hopedthat the explanation
of the benefits of processapproaches,along
with the guidelines on howto use verbal protocols, will encourageresearchers to make
properuse of this technique.

MIS Quarterly~December1987 507

Process
TracingMethods

Acknowledgement
This work wassupported by operating grant
A2421from the Natural Sciences and Engineering ResearchCouncil of Canadaawarded to Izak Benbasat.A preliminary version of
this paper appearedin Information Systems:
Proceedingsof the Thirteenth AnnualConference, Administrative SciencesAssociation of
Canada,June 1985. Wewould like to thank
GerardineDeSanctis, Jane FedorOwicz,Sirkka Jarvenpaa, Melissa Mead, Gary Moore,
JohnSviokla, Iris Vessey,Nick Vitalari, Yair
Wand,RonWeberand Don Wehrungfor their
helpful comments
on earlier drafts of this
paper.

References
[1] Aucella, A.F. andErlich, S.F. "Voice Messaging: Enhancing the User Interface
Basedon Field Performance,"in CHI ’86
Conference Proceedings, M. Mantei and
P. Orbeton (eds.), Boston, Massachusetts, 1986.
[2] Amabile, T.M. The Social Psychologyof
Creativity, Springer-Verlag NewYork,
NewYork, 1983.
[3] Anderson,J.R. Cognitive Psychologyand
its Impfications, 2nd ed., W.H.Freeman
and Co., SanFrancisco,California, 1985.
[4] Bartlett, F.C. Thinking, Basic Books,New
York, NewYork, 1958.
[5] Benbasat,I. and Dexter, A.S. "Value and
Events Approaches to Accounting: An
Experimental Evaluation," The Accounting Review, Volume54, Number4, October 1979, pp. 735-749.
[6] Benbasat,I. andDexter, A.S. "Individual
Differences in the Useof Decision Support Aids," Journal of Accounting Research, Volume 20, Number1, Spring
1982, pp. 1-11.
[7] Benbasat,I. andDexter, A.S. "An Experimental Evaluation of Graphical and Color-EnhancedInformation Presentation,"
ManagementScience, Volume 31, Number 11, November1985, pp. 1348-1364.
[8] Benbasat,I. andDexter, A.S. "An ExperimentalInvestigation of the Effectiveness
of Color-Enhancedand Graphical Information Presentation under Varying Time
Constraints," MISQuarterly, Volume10,
Number1, March1986, pp. 59-83.

508

MISQuarterly~December
1987

[9] Benbasat,I., Dexter, A.~5., and Todd,P.
"An Experimental ProgramInvestigating
Color-Enhancedand Graphical Information Presentation: An Integration of the
Findings," Communications
of the ACM,
Volume29, Number11, November1986,
pp. 1094-1105.
[10] Benbasat,I. and Taylor, R. "Behavioral
Aspectsof Information Processingfor the
Design of Management
Information Systems," IEEEJournal of Systems,Manand
Cybernetics, Volume12, Number4, JulyAugust 1982, pp. 439-450.
[11] Bettman,J.R. An Information Processing
Theory of ConsumerChoice, AddisonWesley, Reading, Massachusetts, 1979.
[12] Biggs,S.F. "AnEmpiricalInvestigation of
the Information Processes Underlying
Four Models of Choice Behavior," Behavioural Experiments
in AccountingII, T.
Burns(ed.), OhioState University Press,
Columbus,Ohio, 1979.
[13] Biggs, S.F. andMock,T.J. "Auditor Information Search Processesin the Evaluation of Internal Controls," WorkingPaper
2-80-6, University of Wisconsin, Madison, Wisconsin, February 1980.
[14]Bonczek, R.H., Holsapple, C.W. and
Whinston, A.B. Foundationsof Decision
Support Systems, AcademicPress, New
York, NewYork, 1981.
System:
[15] Botkin, J. "An Intuitive Computer
A Cognitive Approach to the Management Learning Process," unpublished
doctoral thesis, Harvard University,
1974.
M.J. "Financial Diagnosis: A
[16] Bouwman,
Cognitive Processof the ModelInvolved," unpublishedPh.D. thesis, University
Microfilms #7818922,Carnegie-Mellon
University, Pittsburgh, Pennsylvania,
1978.
[17] Bouwman,
M.J. "The Useof Protocol Analysis in Accounting Research," unpublished working paper, University of Oregon, Eugene,Oregon, 1983.
[18]Bouwman, M.J. "Human Diagnostic
Reasoningby Computer:An Illustration
from Financial Analysis," Management
Science, Volume 29, Number 6, June
1983, pp. 653-672.
[19]Card, S., Moran, T. and Newall A. The
Psychologyof HumanComputerInteraction, Lawrence Erlbaum Associates,
Hillsdale, NewJersey, 1983.

ProcessTracingMethods

[20] Carlson, E.D. "An Approachfor Designing Decision SupportSystems,"Building
Decision Support Systems,J.L. Bennett
(ed.), Addison-Wesley, Reading, Massachusetts, 1983, pp. 15-39.
[21] Carlson,E.D. "Developingthe UserInterface for Decision Support Systems,"
Building Decision SupportSystems,J.L.
Bennett (ed.), Addison-Wesley,
Reading,
Massachusetts,1983, 65-88.
[22]Carroll, J.M., Mack, R.L., Lewis, C.H.,
Grischowsky, N.L. and Robertson, S.R.
"Exploring a WordProcessor," HumanComputerInteraction, Volume1, Number
3, 1985, pp. 283-307.
[23]Cellerier, G. "Guidance of Action by
Knowledge,"in Methodsof Heuristics, R.
Groner, M. Groner and W.F. Bischof
(eds), Lawrence Erlbaum Associates,
Hillsdale, NewJersey, 1983.
[24] Chervany,N. and Dickson,G. "An Experimental Evaluation of Information Overload in a ProductionEnvironment,"ManagementScience, Volume20, Number1,
June 1974, pp. 1335-1344.
[25] Clarkson,G.P.E., Portfolio Selection: A
Simulationof Trust Investment,PrenticeHall, EnglewoodCliffs, NewJersey,
1962.
[26] Cohen,J. "Coefficient of Agreementfor
Nominal Scales," Educational and Psychological Measurement, Volume 20,
Number1, 1960, pp. 37-46.
[27] Cohen,R.M., May, J.H. and Pople, H.E.
"An Intelligent Workstationfor Electrocenter Design," Working Paper #624,
GraduateSchool of Business, University
of Piitsburgh, Pittsburgh, Pennsylvania,
October 1985.
[28] Dawes,R.M. and Corrigan, B. "Linear
Modelsin Decision Making," Psychological Bulletin, Volume81, January 1974,
pp. 95-106.
[29] Dawes,R.M. "The Robust Beauty of Improper Linear Modelsin Decision Making," AmericanPsychologist, Volume34,
July 1979, pp. 571-582.
[30] DeGroot, J. Het DenkenVanDenSchaker, Noord-HollandscheUitgevers Maatschappij, Amsterdam,1946.
Ver[31] De Groot, J. "Learning and Memory
sus Thought: SomeOld Ideas and Recent Findings," Problem Solving: Research Methodand Theory, B. Kleinmutz
(ed.), John Wiley and Sons, NewYork,

~
NewYork, 1966.
[32]Dickson, G.W., DeSanctis, G. and
McBride, D.J. "Understandingthe Effectiveness of ComputerGraphicsfor Decision Support: A CumulativeExperimental
Approach," Communications
of the ACM,
Volume29, Number1, January 1986, pp.
40-47.
[33] DosSantos, B.L. "Aids for ModelOriented DSS-AnExperimental Evaluation of
Their Impact on Decision MakingEffectiveness," unpublished Ph.D. thesis,
CaseWesternReserveUniversity, Cleveland, Ohio, 1982.
[34] Duncker,K. "On ProblemSolving," Psychological Monographs, Volume 58,
Number
5, 1945,-entire issue.
[35] Einhorn, H.J. "Synthesis: Accountingand
Behavioral Science," Studies on Human
Information Processing in Accounting,
supplementto Journal of AccountingResearch, supplement, Volume14, 1976,
pp. 196-206.
[36] Einhorn, H., Kleinmuntz, D. and Kleinmuntz, B. "Linear Regression and Process-Tracing Modelsof Judgment,"Psychological Review, Volume86, Number
5, 1979, pp. 465-485.
[37] Elam, J. and Mead,M. "CanSoftware Influence Creativity," WorkingPaper, Harvard University, Boston, Massachusetts,
1986.
[38]Elstein, A.S., Shulman,L.E., Sprafka,
S.A. Medical ProblemSolving: An Analysis of Critical Reasoning,
HarvardUniversity Press, Cambridge,Massachusetts,
1978.
[39] Ericsson, K.A. and Simon, H.A. "Verbal
Reportsas Data," Psychological Review,
Volume 87, Number3, May 1980, pp.
215-251.
[40] Ericsson, K.A. and Simon,H.A. Protocol
Analysis MIT Press, Cambridge,Massachusetts, 1984.
[41]Hansen, J.V. and Messier, W.F. "A
Preliminary Test of EDP-XPERT,"
unpublished paper, BrighamYoungUniversity,
Salt LakeCity, Utah, 1985.
[42] Hastie, R., Penrod, S. and Pennington,
N. Inside the Jury, Harvard University
Press, Cambridge, Massachusetts,
1983.
[43] Hayes,J.C. The CompleteProblemSolver, Franklin Institute Press,Philadelphia,
Pennsylvania, 1981.

MIS Quarterly~December1987 509

Process
TracingMethods

[44] Heafner, J.F. Protocol Analysis of ManComputerLanguages:Design and Preliminary Findings, NTISAD-A013568,Defense Advanced Research Projects
Agency, 1975.
[45] Hogue, J.T. and Watson, H.J. "Management’s Role in the Approvaland Administration of Decision Support Systems,"
MISQuarterly, Volume7, Number2, June
1983, pp. 15-26.
[46] Hofstadter, D. Godel, Esher, Bach: An
Eternal Golden Braid, Vintage Books,
NewYork, NewYork, 1980.
[47] Huber,G. "Cognitive Style as a Basis for
MIS and DSS Designs," Management
Science, Volume 29, Number 5, May
1983, pp. 567-579.
[48] Hurt, M.E., Elam, J.J. and Huber, G.P.
"An Examinationof DSSContentin Major
IS Conference Proceedings (1980-1985)," Proceedings
of the SeventhInternational Conferenceon Information Systems, SanDiego, California, December
1986, pp. 27-45.
[49]Jarvenpaa, S.L., Dickson, G.W. and
DeSanctis,G. "MethodologicalIssues in
Experimental IS Research: Experiences
and Recommendations,"MIS Quarterly,
Volume 9, Number2, June 1985, pp.
141-156.
[50]Johnson, E.J., Payne, J.W., Schkade,
D.A. and Bettman,J.R. "MonitoringInformation Acquisitions and Decisions:
MouseDecision Laboratory Software,"
Technical Report #85-2, ONRContract
N000-14-80-C-0114, Fuqua School of
Business, Duke University, Durham,
North Carolina, 1985.
[51]Johnson, P.E. "What Kind of Expert
Should an Expert SystemBe?" Journal of
Medicine and Psychology, Volume 8,
1983, pp. 77-97.
[52] Keen, P.G.W. "Decision Support Systems and the Marginal Economicsof Effort," Working Paper 79-03-16, Department of Decision Sciences, The Wharton
School, University of Pennsylvania,
Philadelphia, Pennsylvania,1979.
[53] Keen,P.G.W."Value Analysis: Justifying
Decision SupportSystems,"MISQuarterly, Volume5, Number1, March1981, pp.
1-15.
[54] Keen, P.G.W. and Scott Morton, M.S.
DSS: An Organizational Perspective,
Addison-Wesley, Reading, Massachu-

510

MISQuarterly~December
1987

setts, 1978.
[55]Klienmuntz, D.N. "Cognitive Heuristics
and Feedbackin a DynamicDecision Environment," Management
Science, Volume31, Number6, June 1985, pp. 680702.
[56] Lachman,R., Lachman,J.L. and Butterfield, E.C. Cognitive Psychologyand Information Processing, LawrenceErlbaum
Associates, Hillsdale, NewJersey, 1979.
[57] Larkin, J., McDermott,J., Simon,D.P.
and Simon,H.A. "Expert and Novice Performancein Solving Physics Problems,"
Number208, June 1980, pp. 1335-1342.
[58]Lewis, C. "Using the Thinking Aloud
Methodin Cognitive Interface Design,"
IBM ResearchReport RC9265 (#40713),
1982.
[59] Libby, R. Accounting and HumanInformation Processing: Theory and Applications, Prentice-Hall, Englewood
Cliffs,
NewJersey, 1981.
[60] Lucas, H. "An ExperimentalInvestigation
of the Use of Computer-Based
Graphics
in Decision Making," Management
Science, Volume 27, Number7, July
1981, pp. 757-768.
[61] Mack,R., Lewis, C.H. and Carroll, J.M.
"Learning to UseText Processing Systems," ACM
Transactions on Office Automation, Volume1, Number3, July 1983.
[62] Mclntyre, S.H. "Experimental Study of
the Impact of Judgement-Based
Marketing Models," Management
Science, Volume 28, Number1, January 1982, pp.
17-33.
[63] Newell, A. "On the Analysis of Human
ProblemSolvingProtocols," International
Symposiumon Math and Comparative
Methodsin Social Sciences,Rome,Italy,
1966.
[64] Newell, A. and Simon,H.A. Human
Problem Solving, Prentice-Halt, Englewood
Cliffs, NewJersey, 1972.
[65] Nisbett, R.E. and Wilson, T.D. "Telling
More Than We Can Know: Verbal
Reports on Mental Processes," Psychological Review, Volume84, Number3,
May1977, pp. 231-259.
[66] Norman,D.A. and Bobrow,D.B. "On Data
Limited and ResourceLimited Processes," Cognitive Psychology, Volume7,
1975, pp. 44-64.
[67] Painton, S. and Gentry, J.W. "Another
Lookat the Impactof InformationPresen-

Process
TracingMethods

tation," Journal of Consumer
Research,
Volume12, Number2, September1985,
pp. 240-244.
[68] Payne, J.W. "Task Complexity and Contingent Processing in Decision Making:
An Information Searchand Protocol Analysis," Organizational Behavior and HumanPerformance,Volume16, Number2,
August 1976, pp. 366-387.
[69] Payne,J.W., Braunstein, M.L. and Carroll, J.S. "Exploring Predecisional Behavior: An Alternative Approachto Decision Research,"Organizational Behavior
and HumanPerformance, Volume 22,
Number1, August 1978, pp. 17-44.
[70] Prietula, M. and Hassebrock,F. "Knowledge Capturing Issues in Software
Design-Analysisof VerbalProtocols," paper presented at the 7th Annual Honeywell International SoftwareConference,
Minneapolis, Minnesota,1983.
Can Savethe
[71] Russo, J. "Eye Movements
World: A Critical Evaluation and Comparison BetweenEyeFixations and Other Information ProcessingMethodoligies," in
Advancesin ConsumerResearch, J.K.
Hunt(ed.), Associationfor Consumer
Research, AnnArbor, Michigan, 1978.
[72] Russo,J. and Dosher,B. "Strategies for
Multiatribute Binary Choice," Journal of
Experimental Psychology: Learning and
Memory,Volume9, Number4, 1983, pp.
676-696.
[73] Russo,J., Johnston,E. and Stephens,D.
"Whenare Verbal Protocols Valid?" working paper,Cornell University, Ithica, New
York, March1986.
[74] Russo, J.E. and Rosen, L.D. "An Eye
Fixation Analysis of Multialternative
Choice," Memoryand Cognition, Volume
3, Number3, 1975, pp. 267-276.
[75]Schweiger, D.M. "Is the Simultaneous
Verbal Protocol a Viable Method for
Studying Managerial Problem Solving
and Decision Making," Academy of
ManagementJournal, Volume 26, Number 1, August1983, pp. 185-192.
[76]Scott Morton, M.S. ManagementDeck
sion Systems: ComputerBased Support
for Decision Making, HarvardUniversity
Press, CambridgeMassachusetts,1971.
[77] Simon,H.A. "TheStructure of Ill-Structured Problems,"Artificial Intelligence,
Volume4, Number3, 1973, pp. 181-201.
[78] Simon, H.A. "Discussion: Cognition and

Social Behavior," Cognition and Social
Behavior, J.S. Carroll and J.W. Payne
(eds.), LawrenceErlbaum Associates,
Hillside, NewJersey, 1976,pp. 253-267.
[79] Simon, H.A. "Information Processing
Modelsof Cognition," Annual Reviewof
Psychology,AnnualReviews, Inc., Palo
Alto, California, 1979,pp. 363-396.
[80] Slovic, P. andLichtenstein, S. "Comparison of Bayesian and Regression Approachesto the Studyof InformationProcessing in Judgement,"Organizational
Behavior and HumanPerformance, Volume6, Number6, November1971, pp.
649-744.
[81]Smith, E.R. and Miller, F.D. "Limits on
Perceptionof Cognitive Processes:A Reply to Nisbett andWilson," Psychological
Review, Volume85, Number4, 1978, pp.
355-362.
for the Devel[82] Sprague,R. "A Framework
opmentof a DSS,"MISQuarterly, Volume
4, Number4, December
1980, pp. 1-26.
[83] Sprague,R. and Carlson, E.D. Building
Effective Decision Support Systems,
Prentice-Hall, Englewood
Cliffs, NewJersey, 1982.
[84] Sprague,RIH. and Watson,H.J. Decision
Support systems: Putting Theory into
Practice, Prentice-Hall, Englewood
Cliffs,
NewJersey, 1986.
[85] Stabell, C.B. "A Decision-Oriented Approach to Building DSS,"Decision Support Systems, J.L. Bennett (ed.), Addison-Wesley, Reading, Massachusetts,
1984.
[86] Swieringa, R.J., Weick,K.E. "An Assessment of Laboratory Experimentsin Accounting," Journal of Accounting Research, supplement, Volume20, 1982.
[87] Vessey,I. "An Investigation of the Psychological ProcessesUnderlying the Debugging of ComputerPrograms," unpublished Ph.D. thesis, University of
Queensland, St. Lucia, Queensland,
Australia, 1984.
as a Basis for Ex[88] Vitalari, N. "Knowledge
pertise in Systems
Analysis: An Empirical
Study," MISQuarterly, Volume9, Number
3, September1985, pp. 221-241.
[89] White, P. "Limitations on Verbal Reports
of Internal Events:A Refutationof Nisbett
and Wilson and of Bem," Psychological
Review, Volume87, Number1,1980, pp.
105-112.

MIS Quarterly~December1987 511

Process
TracingMethods

[90] Winston, P.H. Artificial
Intelligence,
Addison-Wesley, Reading, Massachusetts, 1984.
[91]Wright, P. "Financial Information Processing: An Empirical Study," The Accounting Review, Volume52, Number3, July
1977, pp. 676-689.
[92]Wright, P. and Rip, P. "Retrospective
Reportson the Causesof Decisions: Retrieved Memories,Public Theories, or Impression Management,"working paper,
Centerfor Decision Research,University
of Chicago,Chicago,Illinois, 1980.
[93]Zmud,R. "Individual Differences and MIS
Success," Management
Science, Volume
25, Number10, October 1979, pp. 966979.

512

MISQuarterly~December
1987

About the Authors
Peter Toddis a Ph.D. candidateat the Faculty of Commerceand Business Administration, University of British Columbia.Hehas
recently joined the faculty of the Collegeof
BusinessAdministration at the University of
Houston.His general research interests are
in the area of human-computer
interaction.
Heis particularly interested in the impactof
DSSon managerial decision making and in
the evaluation of systeminterfaces.
Izak Benbasatis Professor and Chairmanof
the MISDivision at the Faculty of Commerce
and Business Administration, University of
British Columbia.Hespent the 1985-86academic year as a Marvin BowerFellow at the
Harvard Business School. His current research interests include evaluating humancomputerinterfaces, knowledgeacquisition
methodsfor expert systemsdesign, and comparative evaluation of MIS research
strategies.

