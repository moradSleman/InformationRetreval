Academic contribution

Economy-on-demand
and the fairness of algorithms

ELLJ
European Labour Law Journal
2019, Vol. 10(1) 3–16
ª The Author(s) 2019
Article reuse guidelines:
sagepub.com/journals-permissions
DOI: 10.1177/2031952519829082
journals.sagepub.com/home/ell

Claudia Schubert
University Hamburg, Germany

Marc-Thorsten Hütt
Jacobs University Bremen, Germany

Abstract
Algorithms are the key instrument for the economy-on-demand using platforms for its clients,
workers and self-employed. An effective legal enforcement must not be limited to the control of
the outcome of the algorithm but should also focus on the algorithm itself. This article assesses the
present capacities of computer science to control and certify rule-based and data-centric (machine
learning) algorithms. It discusses the legal instruments for the control of algorithms and their
enforcement and institutional pre-conditions. It favours a digital agency that concentrates
expertise and bureaucracy for the certification and official calibration of algorithms and promotes
an international approach to the regulation of legal standards.
Keywords
Economy-on-demand algorithms, discrimination, certification enforcement, digital agency

Employment in the economy-on-demand and the influence
of algorithms
Platform-based business models change the working world and the ongoing digitalisation presents
opportunities and risks, which are currently being addressed by economists, social scientists and
legal scholars.1 It creates new job opportunities and removes market barriers, allowing businesses
to recruit their workers from a larger pool of candidates. For certain categories of persons, the

1. E.g. Aneesh, A., ‘Global Labor: Algocratic Modes of Organization’, Sociological Theory, vol. 27, 2009, 347, 354 ss.;
Benner, C., Crowdwork – zurück in die Zukunft, Frankfurt a.M., Bund-Verlag, 2014; Däubler, W., ‘Digitalisierung und

Corresponding author:
Claudia Schubert, Universität Hamburg, Fakultät für Rechtswissenschaft, Rothenbaumchaussee 33, Hamburg, 20146,
Germany.
E-mail: claudia.schubert@uni-hamburg.de

4

European Labour Law Journal 10(1)

demand economy enables them to enter the market, in particular people providing nursing care for
family members, people with disabilities, as well as people living in poorly served regions. In
addition, digital labour platforms provide another tool to facilitate the reconciliation of family and
working life. At the same time, critics view the platform economy as a new type of Taylorism –
low-paid wage labour, barely above the minimum subsistence level, undermining the legal and
social standards of good work.2
Jurists move between these two narratives. For labour law experts, the starting point of the
discussion was the employment status of these workers. The scholarly debate, in all, follows
accustomed patterns: an analysis of the working conditions leads to an assessment of the necessary
standards of social protection and to suggestions of remedial measures. Further, trade union
initiatives (e.g. fair crowdwork) – with the support of the European Parliament3 – are demanding
guidelines for digital platforms. They have established their own rating platform, where platform
providers and platform clientele can be evaluated.
On the other hand, so far, the concrete implications of algorithmic platform management and
platform management by artificial intelligence are rarely discussed. Social scientists and economists have accurately described the phenomenon of algorithmic management or algocracy:4
Workers are permanently being monitored, tracked and rated. The continuous and automated
recording of usage habits, work results and customer reviews, often combined with rewards for
favourable user behaviour are characteristics of the platform economy. A worker’s data track leads
to a built-up of virtual reputation, which in return results in more (lucrative) assignments. Instead,
if previous results did not meet the clients’ or platforms’ expectations, workers may algorithmically get rejected from future jobs. If it is known what kind of behaviour leads to more job offers
and/or higher wages (e.g. work within certain time-frames, working speed or quality), algorithms
trigger behavioural effects. At the same time, the comprehensive access of platforms to these user
data aims at a customer-oriented selection of workers as well as an economical optimisation up to
the gradual exclusion of less profitable platform users.5 This phenomenon is not limited to platforms. Other actors, in all sectors of the economy, also implement this mechanism if they make use
of algorithms in their management (management-by-algorithms).6 Therefore, the following explanations on legal risks and regulation may also be considered in other cases.
In his influential article ‘Code is Law’ in Harvard Magazine, Lawrence Lessig considers ‘code’,
i.e. the set of algorithms and their implementations, as the main regulator of cyberspace.7 Here we

2.
3.
4.

5.
6.

7.

Arbeitsrecht’, Soziales Recht, vol. 2, 2016, 8 s.; Prassl, J., Human as a service, Oxford: OUP 2018; Warter, J.,
Crowdwork, Vienna, ÖGB-Verlag 2016.
E.g. Däubler, W., ‘Digitalisierung und Arbeitsrecht’, Soziales Recht, vol. 2, 2016, 8 s.
European Parliament, Call for Guidelines for digital platforms, 16.7.2017, 017/2003(INI), No. 40.
Aneesh, A., ‘Global Labor: Algocratic Modes of Organization’, Sociological Theory, vol. 27, 2009, 347, 354 ss.; Lee, M.
K., Kusbit, D., Metsky, E., Dabbish, L., ‘Working with Machines: The Impact of Algorithmic and Data-Driven Management on Human Workers’, ACM, 2015, 1, 2 ss., https://www.cs.cmu.edu/*mklee/materials/Publication/2015-CHI_
algorithmic_management.pdf.; see also Danaher, J., ‘The Threat of Algocracy: Reality, Resistance and Accommodation’, Philosophy & Technology, vol. 29, 2016, 245, 247 ss.
See Li, H., Liu, Q., ‘Cheaper and Better: Selecting Good Workers for Crowdsourcing’, arXiv preprint arXiv:1502.00725
(2015).
See De Stefano, V., ‘Negotiating the algorithm: Automation, artificial intelligence and labour protection’, ILO,
Employment policy department: EMLOYMENT Working paper No. 246 (2018), https://www.ilo.org/wcmsp5/groups/
public/—ed_emp/—emp_policy/documents/publication/wcms_634157.pdf.
Lessig, L., ‘Code Is Law. On Liberty in Cyberspace’, Harvard Magazine, vol. 102, 2000, 37-39.

Schubert and Hütt

5

address the question, whether in the light of an increasing number of autonomous decisions made
by computer programmes, the ‘code’ itself requires additional and novel forms of regulation. The
effects of algorithms on the working world are not limited to the intentional supervision and
selection of workers (by applying a scoring function). Their design (due to its construction or the
data used for training) has side effects platform operators may neither want to achieve, nor be
aware of. This is particularly true, where contingencies, such as incomplete data sets or distorted
information, lead to falsified rankings because the algorithm does not know how to adequately
deal with misinformation. This may result in a worker’s ranking substantially deteriorating or
improving for no apparent reason. The same applies, where job opportunities are only presented
to a pre-defined number of platform users, selected by an algorithm from a group of workers with
identical rankings.
These malfunctions of algorithms must not only interest platform managers, looking to recruit
the best-qualified workers, but also jurists. Such effects result in the exclusion of workers from
certain job offers or ban them from the platform entirely. This complicates or hinders market
access, especially where platforms develop into dominant undertakings. Analyses by economists
and legal scholars specialising in competition law suggest that these effects are occurring. In short:
from an economic standpoint, a platform is particularly interesting for its users, if it connects a
wide audience of people. This causes suction effects, which bear the risk of unlawful market
concentration. It is already observable, that larger platforms are starting to control the market.

Legal relevance of algorithms
The fairness of algorithms is an ethical, an economic, but also a legal, category. From a legal
perspective, their fairness must depend on the legal framework, which defines certain actions or
results as legally relevant. Despite the different national legal backgrounds, certain issues regarding the fairness of algorithms are relevant in most jurisdictions. They originate from
anti-discrimination law8 and competition law. Both, notwithstanding their diverging objectives
(protection of personality rights on the one hand, strengthening of competition on the other), have a
common denominator: discriminatory as well as anti-competitive conduct complicate or limit
market access and impede appropriate pricing.
The specific effects of algorithms this article focuses on, are the legally relevant effects of the
design of algorithms. Firstly, this is the general rule of the algorithm (scoring function), which
defines the deciding criteria for a worker’s ranking (e.g. cheaper and better). Such a general rule
can result in direct as well as indirect discrimination, but in this context indirect discriminatory
conduct is more interesting, as the resulting manipulations are subtle and clandestine. For example,
if it proves advantageous for contractors to work on Sundays, this may be discriminatory based on
religion. The same is true, where algorithms favour workers, who are available at times at which
people with family commitments are less likely to be able to render services. Typically, this leads
to the disadvantageous treatment of women.9 Cases of indirect discrimination also occur where
pace of work is important, as this tends to exclude people with bodily impairments. Not in all cases,
unequal treatment is unjustifiable, and indirect discriminations in particular can be objectively
8. See also Edwards, L./Veale, M., ‘Slave to the Algorithm? Why a ‘‘Right to an explanation’’ is probably not the remedy
you are looking for’, Duke Law & Technology Review, vol. 16, 2018, 18, 27 ss.
9. On sex discrimination, see Kullmann, M., ‘Platform Work, Algorithmic Decision-Making and EU Gender Equality
Law’, IJCLLIR vol. 34, 2018, 1, 3 s., 7 ss.

6

European Labour Law Journal 10(1)

justified. To what extent the full inclusion of people with disabilities or the reconciliation of family
and working life require the restriction of such effects are valuation issues we need to debate on
national and international level but should not leave them to the company and the coder.
Besides the general rule, the specific construction of an algorithm may cause legally relevant
effects. Its functioning can discriminate against groups of people or limit market access. It can
disadvantage people based on personal characteristics, if regulation defects result in a systematically less favourable treatment of women, persons with disabilities or people of different ethnic
origins. Additionally, algorithms can be the cause of discriminatory conduct or structural abuse in
the sense of the competition law concept, if they favour certain contractors without objective
grounds or deny groups of workers access to the platform10 or prevent the change between platforms by incompatibility.11 The vertical integration of users by the platform and the lacking access
to information can also distort competition (e.g. platform users receive detailed information on
clients and its preferences that are otherwise not available on the market). Data possibly allow
platform price discrimination with regard to the behaviour and needs. This may affect the platform’s clients and crowdworkers as well, but not every discrimination is relevant as it depends on
the negative effects on the competition and the consumer surplus.
The following contribution examines, how far it is possible, to identify discriminatory effects
based on the algorithm rather than its output. This would enable the optimisation of algorithms, but
also allow for their supervision and lead to the question of whether algorithms must be subject to an
ex ante control.

Fairness of algorithms – Bias of algorithms
Starting point – Technical part
In the broad field of e-commerce, algorithms on a very general level perform various forms of
ranking tasks. Database entries (e.g. products or service providers) are ranked according to an
objective function (cost function or scoring function). In our examples and in the discussion of
algorithmic properties, we will restrict ourselves to algorithm-centric data processing (as opposed
to the data-centric data processing, which is characteristic of machine learning approaches). This
restriction allows us to be clearer and more specific. In the outlook we will address some additions
to the debate arising from machine learning.
In the case of algorithm-centric data processing, deviations from fairness (and, hence, discrimination) enter this process on two levels:
1. The definition of the scoring function (usually either requiring the challenge of mapping a
high-dimensional feature vector onto a real number OR defining a distance measure (or
metric) for two such high-dimensional feature vectors).
2. The tie-breaker criteria that decide on the order of the output, when equal values of the
scoring function are encountered.
In the following we will (A) introduce the relevant terminology using a few simple examples;
(B) summarise the existing literature on algorithmic fairness, particularly in computer science; and
10. Compare Federal Cartel Office, Competition Law and Data, 10.5.2016, 14-19.
11. Federal Ministry of Economy and Industry, White Paper ‘Digital Platforms’, March 2017, 43.

Schubert and Hütt

7

Issues impacng
fairness

Method

Types of algorithms

Rule-based algorithms

Data-centric algorithms
(machine learning)

Scoring funcon

Training

▪ choice of the scoring
funcon

▪ lack of transparency

▪ systemac biases from
e-breaking criteria

▪ architecture of the
machine learning
approach

▪ biases in training data

Figure 1. Summary of algorithmic paradigms, together with typical algorithmic tasks and the
issues, which may have a systematic impact on fairness.
(C) embed the ideas derived from the literature in a broader sense with a special emphasis on their
legal implications.

Terminology
In addition to the ranking tasks mentioned above, algorithms also perform decision tasks and
clustering tasks. As discussed above, ranking tasks use a scoring function to produce an ordered
list of data entries. A decision task produces a decision about an individual data entry (e.g. a ‘yes’
or ‘no’ result or the assignment of a data entry to a category from a list of pre-defined categories).
A clustering task summarises all algorithmic attempts to find groups in data. Clustering can be
used, for example, to derive the categories, which are required as input to a decision task, from
existing data. Currently, most implementations for ranking tasks (as encountered in the context of
platform economy) are still rule-based. The application area of clustering tasks is dominated by
data-driven (machine learning) algorithms, while decision tasks are typically addressed by both
algorithmic paradigms. Figure 1 summarises this overview of algorithmic paradigms.
In a (binary) decision task, the algorithm receives an input and computes a yes/no answer. In a
clustering task, the algorithm receives a large set of inputs and groups them into categories,
called clusters.
It should be noted that the distinction between ranking, classification and decision tasks, though
helpful, is not always clear. Mapping inputs into categories can in most cases be viewed as a
classification task. On the other hand, when the same task is rephrased as finding groups in the
data, which – if the initial categories are meaningful – should correspond to these categories, one
can also consider it a clustering task. When formulated slightly differently, one can also regard this
as a decision task: for each input one decides, whether this input is mapped to a given category or
not (with the subsidiary condition that no input can be mapped to more than one category). We can
also formulate this task as a ranking task, by using a scoring scheme (e.g. a distance metric from

8

European Labour Law Journal 10(1)

other members of the category), in order to assess, to which of the given categories the input at
hand is closest.
All these algorithmic tasks revolve around the processing of available data. Even though the
technical and conceptual details behind each algorithmic task may vary, it is instructive to look at
the general features of such algorithms using a simple example. Imagine an online marketplace
connecting customers to service providers. Based on the information she provided in her service
request query a user, Alice, will receive a ranked list of potential service providers. The rank is
determined by an algorithm converting Alice’s information vector and the information vector of a
potential service provider into a score quantifying the agreement of the two information vectors
and then sorting all potential service providers according to this score.
Here a vector means an ordered list of inputs (e.g. parameter values of the job being offered and
to be evaluated by the scoring function). In the example outlined above, if we assume a home
decoration job offer, Alice might characterise the job in the online platform using parameters like
size of the room, time frame for the work to be completed, current surface conditions of the walls,
etc. or any other type of categorical or numerical information required to characterise the offered
job, often using categories and terminology provided by the online marketplace. The companies
then have, for example, tolerance windows for some of these parameters, indicating, whether or not
they are likely to accept such an offer. These parameters are then put together in the scoring
function, which often contains features proprietary to the marketplace, and then in turn leads to the
ranking of companies displayed to Alice.
Let us assume that two potential service providers for Alice’s query, company B and company
C, have the same score. This could be because the information vectors of B and C are nearly
identical or because the scoring function employed by the algorithm is insensitive to the differences between the two information vectors (e.g. by having very small weights assigned to the
vector components, in which the two information vectors differ) or, lastly, because the positive and
negative contributions to the score happen to yield the same score for both, B and C, with respect to
Alice’s service request query. The latter is more frequent in the case of short vectors and mostly
categorical information (e.g. simple ‘yes’ and ‘no’ entries) contained in the information vector.
Such cases require a tie-breaker criterion, in order to determine, whether B or C are ranked higher
in the list received by Alice in response to her database query. Typical outcomes could be that B is
ranked above C, because alphabetic sorting is used in the case of an equal score. It could also be
that the company, which has been in the database longer, is returned above the other (first in, first
out, FIFO), or, conversely, the company, which has last joined the database, is returned above the
other (last in, first out, LIFO).
Repeated application of such tie-breaker criteria over a large number of queries leaves a pattern
of rank deviations. For example, the average rank of a company, whose name starts with Z, could
be systematically higher than the average rank of a company with an A.
Furthermore, the scoring function itself has a strong impact on the pattern of ranks produced by the database operation and the algorithmic generation of service provider lists in
response to a query. As already outlined above, it is easy to envision situations, where flaws
of the scoring function are exploited by users by modifying their information vectors in
alignment with the specifications of the scoring functions, in order to obtain systematically
better rankings.
It is a challenge in its own right to define the fairness of algorithms. In the case of decision
algorithms typical definitions of algorithmic fairness involve the expectation that the assigned
categories on average are equally probable with respect to outside attributes, which do not directly

Schubert and Hütt

9

enter the algorithm. This can be quantitatively assessed by computing the difference of the conditional probability of an outcome given the one characteristic, for which discrimination is
expected, (e.g. ‘female’ as gender information or ‘foreign’ as ethnicity information) and the other
characteristic (e.g. ‘male’ or ‘native’).12
In the case of ranking algorithms, a fairness requirement could include the robustness of the
rank against small variations of the scoring system13 or a similar long-term distribution of ranks for
all database entries, when exposed to a broad range of queries.

Literature review
There is a rich debate in computer science about the fairness of algorithms and algorithmic
discrimination. Even though this debate is still far from a convergence towards universally
accepted standards and methods, the recent literature can be classified into attempts to measure
discrimination,14 algorithm enhancements preventing discrimination and increasing fairness,15 and
the auditing or control of algorithms with respect to discrimination.16 Contributions to this debate
exist for both algorithm-centric and data-centric (machine learning) approaches.17
Summarising, this debate has shaped our view on algorithmic fairness by providing illustrative
examples, by contributing novel mathematical concepts and novel classification schemes of fairness enhancing methods.
Regarding the question, which elements of this debate can have direct implications for the
legal framework of algorithmic decisions, a few investigations are of particular interest.
Žliobaite_ and Custers discuss an interesting toy model example, where they generate, and then
analyse, data with an embedded ethnicity bias.18 The fact that the data model trained with these
12. Žliobait_e, I., ‘On the relation between accuracy and fairness in binary classification’, arXiv preprint arXiv:1505.05723
(2015).
13. Asudeh, A., Jagadish, H., Miklau, G., Stoyanovich, J., ‘On obtaining stable rankings’, 2018, arXiv preprint
arXiv:1804.10990 (2018).
14. Žliobait_e, I., ‘Measuring discrimination in algorithmic decision making’, Data Mining and Knowledge Discovery, vol.
31, 2017, 1060–1089; Berendt, B., Preibusch, S., ‘Toward accountable discrimination-aware data mining: The
importance of keeping the human in the loop—and under the looking glass’, Big data, vol. 5, 2017, 135–152.
15. Singh, A., Joachims, T., ‘Fairness of exposure in rankings’, arXiv preprint arXiv:1802.07281 (2018); Asudeh, A.,
Jagadish, H., Miklau, G., Stoyanovich, J., ‘On obtaining stable rankings’, arXiv preprint arXiv:1804.10990 (2018).
16. Sandvig, C., Hamilton, K., Karahalios, K., Langbort, C., ‘Auditing algorithms: Research methods for detecting discrimination on internet platforms. Data and discrimination: converting critical concerns into productive inquiry’,
(2014), 1–23, http://www-personal.umich.edu/*csandvig/research/Auditing%20Algorithms%20–%20Sandvig%20–
%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf; Yang, K., Stoyanovich, J., Asudeh, A.,
Howe, B., Jagadish, H., Miklau, G., ‘A nutritional label for rankings’, arXiv preprint arXiv:1804.07890 (2018).
17. See Asudeh, A., Jagadish, H., Miklau, G., Stoyanovich, J., ‘On obtaining stable rankings’, arXiv preprint
arXiv:1804.10990 (2018); Stoyanovich, J., Yang, K., Jagadish, H., ‘Online set selection with fairness and diversity
constraints’, EDBT 2018, 241-252; Žliobait_e, I., ‘On the relation between accuracy and fairness in binary classification’, in arXiv preprint arXiv:1505.05723 (2015); Gajane, P., ‘On formalizing fairness in prediction with machine
learning’, arXiv preprint arXiv:1710.03184 (2017); Friedler, S. A., Scheidegger, C., Venkatasubramanian, S.,
Choudhary, S., Hamilton, E. P., Roth, D., ‘A comparative study of fairness-enhancing interventions in machine
learning’, arXiv preprint arXiv:1802.04422 (2018); Williams, B. A., Brooks, C. F., Shmargad, Y., ‘How algorithms
discriminate based on data they lack: Challenges, solutions, and policy implications’, Journal of Information Policy,
vol. 8 (2018), 78–115 for examples of data-centric approaches.
18. Žliobait_e, I., Custers, B., ‘Using sensitive personal data may be necessary for avoiding discrimination in data-driven
decision models’, Artificial Intelligence and Law, vol. 24, 2016,183 ss.

10

European Labour Law Journal 10(1)

fictitious data indirectly perpetuated the bias, even though ethnicity information was not
included in the data, illustrates in a clear and transparent fashion, how discrimination enters
data-centric algorithms (i.e. algorithms based on learning patterns in training data). The main
result of this study is that formulating a data model (i.e. learning decision rules via data-centric
data processing) requires the sensitive, discriminatory information to be included in the training
data: ‘[c]ollecting sensitive personal data is necessary in order to guarantee fairness of algorithms, and law making needs to find sensible ways to allow using such data in the modelling
process.’19
In the legal system in the US, one application area of algorithms – risk assessment in
criminal sentencing – has received particular attention over the last years.20 This development
had already been anticipated almost a decade ago by Susskind.21 Corbett-Davies et al. use the
Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) algorithm
as the main example in a broad discussion of optimal decision rules under various definitions
of algorithmic fairness.22 The algorithm has been under scrutiny recently, because (1) a study
could demonstrate that even though race is not an explicit input to the algorithms, the output
shows a clear racial bias;23 and (2) it was shown that a ‘crowd computing’ approach, where
the decision is made by laypersons, achieves a similar performance as the COMPAS
algorithm.24
A trend in the literature, for example, is to find ways of excluding biasing information (e.g. race)
from the feature set (e.g. the questionnaire filled out by applicants for a loan). A remaining
challenging problem is then to ensure that the excluded information cannot be statistically reconstructed by other features. In contrast to this general approach, in Kleinberg et al. (2018) it has been
argued to explicitly include such information in the feature set and then distinguish between an
‘efficient planner’ algorithm (that just maximises a score and hence can lead to an effective bias)
and an ‘equitable planner’ algorithm (that avoids discriminatory biases by requiring equal distribution across the relevant features).25

19. Žliobait_e, I., Custers, B., ‘Using sensitive personal data may be necessary for avoiding discrimination in data-driven
decision models’, Artificial Intelligence and Law, vol. 24, 2016, 183 ss.
20. Christin, A., Rosenblat, A., Boyd, D., ‘Courts and predictive algorithms’, in Data & Civil Right 2015; Monahan, J.,
Skeem, J. L., ‘Risk assessment in criminal sentencing’, Annual review of clinical psychology, vol. 12, 2016, 489–513;
Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., Huq, A., ‘Algorithmic decision making and the cost of fairness’,
Proceedings of the 23rd ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, 2017,
797–806.
21. Susskind, R. E., The end of lawyers?: rethinking the nature of legal services, Oxford: OUP, 2010.
22. Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., Huq, A., ‘Algorithmic decision making and the cost of fairness’,
Proceedings of the 23rd ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, 2017,
797–806.
23. Angwin, J., Larson, J., Mattu, S., Kirchner, L., ‘Machine bias: There’s software used across the country to predict
future criminals and it’s biased against blacks’, ProPublica, 23 May 2016.
24. Dressel, J., Farid, H., ‘The accuracy, fairness, and limits of predicting recidivism’, Science Advances, vol. 4, 2018, no.
1, eaao5580.
25. Kleinberg, J., Ludwig, J., Mullainathan, S., Rambachan, A. ‘Algorithmic Fairness’ AEA Papers and Proceedings, vol.
108, 2018, 22–27.

Schubert and Hütt

11

The debate in computer science: Discussion and outlook
The topic of algorithmic fairness needs to be distinguished from another virulent topic, algorithmic regulation, which exists in two variants: regulation by algorithms26 and regulation of
algorithms.27
Currently we see a shift from algorithm-centric to data-centric data processing, i.e. decision
procedures are ‘learned’ via training on large data sets and then, after test and calibration phases,
employed to autonomously perform classification, decision and ranking tasks. The computational
structures, in these ‘machine learning’ approaches are, for example, neural networks.28
Any type of bias on this level can be linked, in principle, to two basic types:
1. biases in the training data; and
2. limitations (or even artefacts) generated by the predefined computational structure (e.g. the
number of layers in a multi-layer neural network).
One challenge is the lack of interpretability of the internal representation of the data within the
algorithm (e.g. within the pattern of weights in the trained neural network). This ‘opacity that
arises from the characteristics of machine learning algorithms’29 limits the assessment of algorithmic fairness.

Certification of algorithms – from a control ex post to an examination
ex ante?
Legal instruments
Legal violations resulting from the specific characteristics of algorithms call for the further development of legislation and law enforcement. From the perspective of the right owner, key barriers to
law enforcement are the lack of transparency and comprehensibility of algorithms as well as the
pace of technological change. The development of new algorithms and machine learning systems
is likely to proceed faster than the development of high-quality test methods. As a result, the
26. Hildebrandt, M. ‘Algorithmic regulation and the rule of law’, Phil. Trans. R. Soc. A, vol. 376, 2018, 20170355; Yeung
K. ‘Algorithmic regulation: A critical interrogation’, Regulation & Governance, vol. 12, 2018, 505-523.
27. Tutt, A. ‘An FDA for Algorithms’, Administrative Law Review, vol. 69, 2017, 83-123.
28. In order to illustrate the difference between rule-based and data-driven algorithms, we resort to a minimal example of a
classification task, where numerical input is mapped to categories A and B: 1.1!A, 2.1!A, 2.2!A, 2.7!B, 3.1!B,
5.2!B. In a rule-based approach one would derive (either from knowledge of the underlying system or from a detailed
analysis of the data) the following algorithm: If input is smaller than 2.5 then output ¼ A, else output ¼ B (where the
exact value of the threshold, 2.5, is to a certain degree arbitrary). A data-centric (machine learning) algorithm would
start with a network connecting the (numerical) input to the (categorical) output via intermediate nodes. The connections among the nodes have numerical weights and each node has a function, how the weighted input is converted
into an output. Initially, the weights are random. Then one iteratively changes the weights such that more and more
inputs are mapped to the correct (known) category. The network has then ‘learned’ to reproduce these input-output
relations. One can then hope that also data outside the original data set are classified correctly. In order to estimate this
capability, the initial data set is typically separated into a training data set, which is used to adjust the weights, and a test
data set, which is used to quantify the accuracy of the classification achieved by this network. Such networks are called
‘neural networks’, because the learning procedure is motivated by the learning mechanisms of biological neurons.
29. Burrell, J., ‘How the machine ‘‘thinks’’: Understanding opacity in machine learning algorithms’, in Big Data & Society,
vol. 3, 2016, no. 1, 2053951715622512.

12

European Labour Law Journal 10(1)

detection of legal infringements becomes significantly less likely, which – according to economic
analysis – will provoke an increase in rights violations. Therefore, the legal system needs to
counteract the undermining of the laws by technology.
Approaches for the improvement of legal protection must be technically possible as well as
practically enforceable. Suitable legal reactions to the impact of algorithms on the economy-ondemand do have to reflect the available instruments and its enforcement but also the institution
necessary in order to keep up with the technical progress and to protect the interests of enterprises
and workers as well as the business secrets of the platform owner. In addition, one must consider
that platforms operate internationally. Thus, solutions on national level are likely to fall short.
In cases of lack of transparency, the respect of rights and the improvement of legal protection
are usually ensured by creating transparency and improving comprehensibility, in particular data
protection and consumer protection law, but also employment law. However, regarding the issue at
stake, an increase in transparency does not remedy the described deficiencies; adverse effects have
their roots in the algorithms themselves. The disclosure of said algorithms is thinkable, but rather
useless, as only computer experts can read and decipher them. This area is the preserve of experts.
But even they encounter their limits when dealing with machine learning algorithms, and protected
business secrets may be concerned. Hence, from the perspective of those seeking justice, solutions
through data protection law are worthless.30 Regarding platforms, the right not to be subject to a
decision based solely on automated processing contained in Art. 22 General Data Protection
Regulation (GDPR) is also not an adequate remedy.31 The platform management has to respond
to the worker’s request individually. Doing so, requires a solution that is more expensive and by no
means more transparent.
As rule-based and data-centric algorithms are technical tools the certification or calibration by an
independent institution is an alternative path to prevent intentional and unintentional unfairness of
algorithms. Despite the need for more research in computer science, the legal response can and shall
keep up with the technical development. Summarising the detailed technical discussion above, a
suitable requirement for rule-based algorithms could be the robustness of the rank against small
variations of the scoring system. In the case of data-centric algorithms one may impose the use of
multiple methods and classifiers, as well as monitoring the results under variation of the training data.
For both algorithmic paradigms, the long-term distribution of ranks for all database entries under a
broad range of queries could be monitored and statistically assessed with respect to intrinsic biases.
Certification or calibration as an instrument for legal control of algorithms has two main
preconditions. First, there is a need for a stable and reliable testing method. Second, the legal
standards have to be feasibly transferable in a testing tool. The development of calibration and
certification methods requires more research at the interface of computer sciences, social sciences
and data analytics, embedded in an ongoing dialog with legal experts. In general, rule-based
algorithms may be easier to audit and control with respect to algorithmic fairness than datacentric algorithms.
30. Less critical Edwards, L., Veale, M., ‘Slave to the Algorithm? Why a ‘‘Right to an explanation’’ is probably not the
remedy you are looking for’, Duke Law & Technology Review, vol. 16, 2018, 18, 38 ss.; see their proposed use of a right
to erasure (Art. 17, para. 2 GDPR) and a right to data portability (Art. 20 GDPR) as suitable instruments (see pages 71
ss.). It cannot address the antidiscrimination and anti-trust cases adequately. Only the right of erasure might be useful to
avoid ongoing effects of discriminating data.
31. Blacklaws, C., Algorithms: transparency and accountability (2018), Phil. Trans. R. Soc. A 376: 20170351, p. 6, http://
dx.doi.org/10.1098/rsta.2017.0351.

Schubert and Hütt

13

Furthermore, certification needs the transformation of legal standards in decision trees that
allow the detection of discrimination and its proper justification. Whilst the detection of an intentional or unintentional unequal treatment according to one of the inadmissible criteria is very
formal and therefore easy to identify if the data are available, the proper justification of discrimination is much more difficult. The main obstacle for certification as a legal instrument is the
justification of discrimination that is regulated by general clauses in anti-discrimination law. When
designing a certification tool one can either limit the test to unequal treatment as such, and check
the detected discrimination by the administration, or integrate justifications in the testing tool as far
as possible and thereby certify that the algorithm is up to some extent free of discrimination.
Despite these pre-conditions, certification or calibration is superior to the existing dummy job
applications (test persons).32 The assessment of the algorithm itself is more effective, it provides a
more intensive and precise scrutiny. Evaluations by test persons can lead to mistakes. Calibration
by a specific homogenous data set can have a homogenous output. Hence, big data sets allow
proving biases by statistical methods.

Enforcement of certification or calibration
Formation of an agency – Independent accumulation of expertise, administrative solution. These are
strong indicators that the control of algorithms should be conducted by a body or an agency, as
such a specialised institution could accumulate the necessary expertise and monitor new technological developments.33 It could participate in the development of testing standards and their
evaluation and certify algorithms, whilst at the same time ensuring the adequate protection of
business secrets. Legislation is no alternative means. The rapid development and the technical
complexity make it impossible to have detailed legislation on the certification of algorithms. Under
constitutional law it is sufficient if legislation provides the legal framework for such an agency
(e.g. aim, procedure, and authorisation).
First developments in this direction can be observed in the USA, the UK and the Netherlands: In
these countries, competition authorities have special departments for competition and consumer
law protection in the digital economy. The House of Lords’ Select Committee on Artificial
Intelligence makes additional proposals on expert guidance.34 In late 2016, the advisory council
for the German Federal Ministry for Justice and Consumer Protection proposed the establishment
of a digital agency.35 In 2017, the Federal Ministry for Economic Affairs and Industry endorsed
this proposal.36

32. See Cediey, E., Foroni, F., Discrimination in access to employment on grounds of foreign origin in France, International Migration Papers 85E (2008), https://www.ilo.org/wcmsp5/groups/public/—ed_protect/—protrav/—migrant/
documents/publication/wcms_201429.pdf.
33. See also Scherer, M., ‘Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies’,
Havard Journal of Law & Technologies, vol. 29, 2016, 353, 381 ss.; recommending the use of GDPR for the certification of algorithms Edwards, L., Veale, M., ‘Slave to the Algorithm? Why a ‘‘Right to an explanation’’ is probably
not the remedy you are looking for’, Duke Law & Technology Review, vol. 16, 2018, 18, 77 ss.
34. See Artificial Intelligence Select Committee. 2018 AI in the UK: ready, willing and able? 16 April 2018, HL 100 201719, paras 105 s., 121, 366 ss., 420, recommendation 16, https://publications.parliament.uk/pa/ld201719/ldselect/ldai/
100/100.pdf.
35. Expert Committee on Consumer Issues, Verbraucherrecht 2.0, December 2016.
36. Federal Ministry of Economy and Industry, White Paper ‘Digital Platforms’, March 2017, 97-100.

14

European Labour Law Journal 10(1)

Surprisingly, these advances have not yet been taken up in the discussion on platform-based
business models and crowd work. Still, participating in this discourse is important because (most)
crowd workers cannot be classified as employees, but are independent contractors. Moreover,
unfair competition laws and anti-trust laws also apply to employers, at least beyond the exception
for collective agreements. Therefore, addressing the specific problems of algorithms through a
digital agency simultaneously offers a consistent solution for crowd work.
Such an agency is an administrative body and should be under the control of the highest
administration, e.g. the Federal Cartel Office (Bundeskartellamt) is controlled by the Federal
Ministry of Economic Affairs and Energy. To what extent the social partners should participate
in such an agency or its administrative procedure has to be decided based on various factors, but it
has to be borne in mind that only a part of the affected crowd workers are employees and organised
by social partners. It is of utmost importance that when an agency exercises administrative power,
it observes and respects the rule of law. Social partners can support such an agency by providing
information and exercising and/or enhancing private enforcement. Anti-discrimination law already
promotes that organisations with legitimate interests may engage within the legal framework of the
Member States supporting complaints (see Art. 10, para. 2 Direction 2000/78/EC). Anti-trust law
has its own elements of private enforcement (e.g. disgorgement of benefits by association see para.
34 Gesetz gegen Wettbewerbsbeschränkungen, Competition Act).
Administrative enforcement or Legal incentives. When creating such an agency and defining its responsibilities, one should consider whether algorithms should be controlled ex post or need to be
certified ex ante. The latter would lead to a fundamental change of paradigm: As a rule, competition law, anti-discrimination law and data protection law rely on an ex-post control. A control ex
ante highly depends on the threat for a person’s rights by the algorithm. From a systematic point of
view one can introduce a repressive ban with an exemption option or a preventive ban with an
authorisation option but if the technical development is reasonable and desirable in general it shall
be allowed but certified by the administration.
All these options might come into use. As lethal autonomous weapons systems (‘killer robots’)
will be repressively banned, a preventive ban is typical for areas of the law guaranteeing technical
safety (product and equipment safety), where failures can lead to irreversible damage to lives,
bodies and health. Only those algorithms that have potential effect on the health and safety or life
of a person will preventively be banned. There might be fields of application that create comparable risks (e.g. autonomous cars, medical treatments) but there is no need for a general preventive
ban with an authorisation option. In the economy-on-demand, the inherent dangers of algorithms
are of a different kind. Algorithms (machine learning and rule-based) are a commonly used
technique. This technical revolution has its ambiguity, but as long as we regard it as promising
and desirable for society, economy and public welfare, the use of algorithms should be permitted in
legal categories.
However, the lack of transparency measurably complicates ex post law enforcement. Effective
remedy through associations seems unlikely, as they are facing the same technical obstacles. This
is an argument in favour of a certification ex ante. Therefore, certification could be used to make
legal protection ex post more effective. Legal responses are not limited to the alternatives of
prohibited and allowed behaviours or products. An ex ante control may also be exercised for
behaviours and products that are allowed in general but should be monitored by the administration
due to their potential dangers for rights and freedoms.

Schubert and Hütt

15

Still, to some extent, a race between technical advancement and the development of testing
standards is inevitable, leading to a protection gap between legislation and law enforcement. The
administration has to keep up with the technical development. This can be facilitated through
incentives for the companies, first, to ask for certification, and second, to keep the agency informed
about the technical development. Beyond labour law, Scherrer proposed a national regulation of
strict liability for the users of specified algorithms,37 which can be reduced to fault liability, if the
algorithm is subject to an ex-ante control by the national agency. This proposed regulation has two
advantages: According to international private law, the law applicable to obligations arising out of
a tort is the law of the country in which the damage occurred.38 Hence, national legislation can
offer protection, even if the algorithm was constructed, tested and implemented elsewhere.
Furthermore, the company that implemented the algorithm has a strong incentive to contact the
agency and ask for the certification of the algorithms.
This concept can be transferred to labour law issues. Even if all EU Member States and many other
states world-wide allow a choice of law for the employment contract, there are overriding mandatory
provisions in the national laws, which govern the employment relationship without the choice of law.39
Anti-discrimination law in particular contains such overriding mandatory provisions.40 The strict liability
for discrimination could be a starting point for a comparable regulation. The range of damages might be
too low for a strong incentive, but the sum of liability can make the regulation finally effective.
National and/or international solutions. In implementing such a strategy, attention should be paid to
the coordination of national, supranational and international laws. In general, states are competent
to establish obligatory, national digital agencies. As a result, platforms, which operate internationally, would be subject to multiple checks. That would create different, perhaps even conflicting
standards and costs. Simplifications could be achieved through international certification standards
or cross-recognitions of certificates. Within the EU cross-recognitions follow the principle of the
country of origin; in the wider context of international law they are governed by international
agreements.
The question, which international organisation shall be responsible, is not easy to answer, as
there is the ILO for employment law, the WIPO for intellectual property law and the WTO for
international trade law. One suitable player could be the OECD, since it has already enacted
guidelines for corporate governance and multinational enterprises and has a digital agenda. The
development of a code of conduct in form of international soft law and the establishment of a
certification body would be sensible additions to the current efforts to uphold legal and ethical
standards in the digital economy.

37. Scherer, M., ‘Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies’, Havard
Journal of Law & Technologies, vol. 29, 2016, 353, 395 ss.
38. See Art. 4, para. 1 Rome II Regulation; Junker, A., in Münchener Kommentar zum BGB, 7. Ed., 2018, Art. 4 Rome II
Regulation rec. 18.
39. See Art. 8, para. 1, Art. 9 Rome I Regulation.
40. Entscheidungen des Bundesarbeitsgerichts [BAGE] (German Supreme Labour Court), 153, 138 (rec. 96), obiter
dictum; Deinert, O., Internationales Arbeitsrecht, Tübingen, Mohr Siebeck, 2013, para, 12 rec. 88 with further
references.

16

European Labour Law Journal 10(1)

Conclusion
Our article has shown that an algorithm-based platform economy carries the risk of undermining
current legislation. Law enforcement through individual legal protection should be complemented
by state support, where individual protection is limited by reason of technicity and collective
representation is not able to remedy this. It is our task to cooperate with physicists and computer
scientists to further develop our legal system. Particularly, self-learning systems will present new
challenges, not only of a technical, but also of a legal nature.
Algorithmic decisions are by now ubiquitous.41 We would like to emphasise that the overarching topic of the fairness of algorithms goes far beyond the specific focus selected here, the
platform economy. The risks of input-output biases, and thus breaches of algorithmic fairness,
which have been enumerated above (choice of the scoring function and tie-breaking criteria for
rule-based algorithms; choice of methods and biased training data in data-centric algorithms can
and should be delineated in a wide range of domains of application.
Businesses are already aware of the necessity of such developments. In the 2017 Founders’
Letter, published on 30 April 2018, the President of the Google parent company Alphabet, Sergey
Brin, wrote about algorithms (in particular in the context of machine learning): ‘However, such
powerful tools also bring with them new questions and responsibilities. How will they affect
employment across different sectors? How can we understand what they are doing under the hood?
What about measures of fairness?’42
Declaration of Conflicting Interests
The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or
publication of this article.

Funding
The author(s) received no financial support for the research, authorship, and/or publication of this article.

ORCID iD
Claudia Schubert
https://orcid.org/0000-0003-4206-5526
Marc-Thorsten Hütt
https://orcid.org/0000-0003-2221-423X

41. See the recent issue on this topic in the Philosophical Transactions of the Royal Society, summarised in Olhede S. C.,
Wolfe P. J., The growing ubiquity of algorithms in society: implications, impacts and innovations (2018), Phil. Trans.
R. Soc. A 376: 20170364, http://dx.doi.org/10.1098/rsta.2017.0364.
42. Brin, S., 2017 Founders’ Letter.

