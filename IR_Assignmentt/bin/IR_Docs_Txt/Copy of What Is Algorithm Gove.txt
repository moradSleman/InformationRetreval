Internet Governance

Editor: Virgilio A.F. Almeida, virgilio@dcc.ufmg.br

What Is Algorithm
Governance?
Danilo Doneda • Rio de Janeiro State University
Virgilio A.F. Almeida • Harvard University

With algorithms’ increased use to fulfill complex tasks comes the risk of algorithms’ use for manipulation, biases, censorship, social discrimination, violations
of privacy and property rights, and more. To address such risks, the process of
algorithm governance should be considered.

A

lgorithms are basically sets of instructions to perform a task, producing an output from a given input. Today, algorithms
embedded into systems and electronic devices
are increasingly trusted to make decisions, evaluations, and analysis with a concrete impact on
our lives.
Algorithms’ vocation to penetrate several realms
of our routine is now regarded as a fact of life. They
perform tasks we could hardly imagine accomplishing without a human in charge. As their sophistication and utility grows, the more they seem
“autonomous” — even evoking the notions of a
“thinking machine” present in some arcane thoughts
that date back to the early age of computer science.
In fact, often the term “algorithm” is being used or
referred to as a synonym for a computer, machine,
code, software, and so on.
The availability of ever-increasing computing power and datasets make it possible for algorithms to perform tasks with a magnitude and
complexity that’s unbearable by human standards. Often their results can hardly be anticipated or explained — even by their designers.
At the same time, although they provide valuable output, algorithms can also take humans
out of the loop in their series of decisional processes, which can be risky. Thus, to foster their
integration into several social and economic processes in which they can be valuable, perhaps
we should design instruments that permit some
sort of governance of algorithms. This could prevent them from affecting the balance of power
60

Published by the IEEE Computer Society

negatively, in favor of those who are able to exercise actual power regarding how they’re used, as
well as maximizing their benefits and reducing
their potential risk. An example of such a shift in
the balance of power is given by Frank Pasquale
when he mentions that a woman who had taken
antidepressants as a sleep aid had her requests for
health insurance denied by some companies. The
record of her use of these medications, that could
eventually help her if it was kept for strict medical
usage, was used against her on the basis of presumptions regarding her use of these medicines.1

Algorithms’ Potential Issues
and Elements

The complexity of algorithms’ work is increased
by the growing use of machine-learning techniques. With these techniques, an algorithm can
rearrange and morph itself and its inner workings
based on the data it analyzes. As Pedro Domingos
once described, learning algorithms are “algorithms that make other algorithms… so we don’t
have to.”2 Often it isn’t a simple task for the data
scientist or the algorithm’s designer to describe
the steps an algorithm has taken to produce a certain output, if not in merely abstract terms.
Hence, algorithms add a new element to the
information processing chain — their opacity — that
often is associated with the difficulty of decoding
their output. Humans are increasingly unable to
understand, explain, or predict algorithms’ inner
workings, their biases, and eventual problems.
This is cause for rising concern in situations where

1089-7801/16/$33.00 © 2016 IEEE

IEEE INTERNET COMPUTING

What Is Algorithm Governance?

algorithms are trusted to make important, if not fundamental, decisions that
affect our lives, to the point that a call
for more transparency and accountability of algorithms is being increasingly
noticed in academic works as well as
public campaigns.3
At the same time, there are also
non-technical justifications for their
opacity. Some of them are based on
issues over competition. Having an
open algorithm could put a company
at a disadvantage regarding its competitors. Others are based on intellectual property: in some countries,
the law protects a company’s commercial secret or intellectual property.
Another reason for not opening certain algorithms is the possibility that
some people — once they’re aware of
their characteristics — could be able
to better “game” the algorithm.4 So,
the opacity of algorithms is a tendency sustained by elements of both
technical and non-technical nature.
Opacity, though, hasn’t been a barrier
to the widespread adoption of algorithms
in several realms. In fact, algorithms are
no longer seen as the trick behind search
engines or something that helps e-commerce gather customers’ preferences —
instead, they’re now essential parts
of self-driving cars, crime-prediction
frameworks, and tests for many diseases, along with a growing list of other
important applications. And several of
these applications have a direct impact
on society, from their employment to
make sense of data for development and
humanitarian action (see www.unglobalpulse.org), to aiding doctors in finding the right diagnosis, or even enabling
more rationality in judicial decisions.
Algorithms have risen to perform
an ever-increasing number of tasks,
due not merely to their own development per se, but to the occurrence of
conditions that transformed the whole
environment they’re settled in. Indeed,
“the algorithm isn’t an algorithm
because it executes (the instructions);
it’s an algorithm because it’s enacted
as such by a heterogeneous assemblage
July/August 2016

of actors, imparting to it the very action
we assume it to be doing.”5
This environment contains elements
of major relevance to algorithm’s governance — in fact, their governance can
often be based on tools that work not
on the algorithm itself but on elements
of their environment. Out of these elements, datasets are probably the most
fundamental ones. Algorithms became
much more useful as a function of the
availability of data, which is relevant
for its inner work. As Tarleton Gillespie
notes, “Algorithms are inert, meaningless machines until paired with databases upon which to function.”4
Datasets are built up from the
data that are collected in a fast-paced
rhythm, as more and more of our activities leave a trace (think of our actions
on the Internet) or are monitored. As a
consequence, the amount of relevant
data available grows dramatically.
This is indeed central to the idea of Big
Data, the paradigm for data that often
“feeds” algorithms, with characteristics that are often referred as the 3 V’s:
volume (more data are available), variety (from a wider number of sources),
and velocity (at an increasing pace,
even in real time).6
If datasets are used as central parts
of the tasks performed by algorithms,
it’s important to emphasize the need
to verify if they’re being lawfully and
even ethically used — in short, if the
data are legitimate, correct, updated,
and don’t show any type of bias. For
instance, data mining and other methods used to refine the datasets can
eventually result in discrimination.
Furthermore, selection, classification,
correlation, and other techniques
many times tend to replicate environmental bias, as they can mimic
social and personal conditions. This
isn’t exactly a novelty, as statistical
discrimination (the drawing of stereotypes based on the “average” behavior
of a discriminated group) has been an
issue for more than four decades, but
this is a problem which algorithms are
constantly making more salient.7

Governing Algorithms

Several potential risks of the use of
algorithms have been identified in the
literature, such as the risks of manipulation, biases, censorship, social discrimination, violations of privacy and
property rights, abuse of market power,
effects on cognitive capabilities, and
growing heteronomy. To address these
risks, the process of algorithm governance should be considered.
Algorithm governance can vary
from the strict legal and regulatory
viewpoints to a purely technical standpoint. Its focus is often on accountability, transparency, and technical
assurances. The resource to a certain
governance path can be based on factors such as the nature of the algorithm, its context, or risk analysis.8
Generally, when a governance
option is made it aims to reduce
problems caused by the algorithm. It
should try to preserve its effectiveness
and reduce undesirable outcomes.
Some governance tools act not on
the algorithm but on the data they
need in order to work. This is true for
several tools already present in dataprotection legislation that, in some
countries, have measures regarding
transparency and fairness that apply
directly to algorithms and the platforms that support their functioning.
For instance, the provision that automated decisions shall be grounded
on transparent criteria is commonly
present in several pieces of data-protection legislation. The same happens
with the right to ask for a human revision of automatically taken decisions.
Using algorithms to regulate datasets is at the core of most data protection
legal frameworks, which also command
these datasets to be legitimate and rightful, undergoing several requirements to
meet this criteria. An example would
be consent for the use of personal data
on various occasions, as ownership is
another rising issue, and identifying
specific datasets — in a manner to allow
consent over the treatment and use of
data, whether for personal use or just
61

Internet Governance

originated by a citizen — should also be
an issue for regulation.
The need for accountability and
transparency of algorithms is often mentioned as another possible approach.
Transparency, as we’ve already mentioned, isn’t natural to many algorithms
in use, for technical and non-technical
reasons, so we need governance instruments to foster the adoption of certain
transparency levels or open algorithms.
Accountability, which is linked to
the notion of responsibility, fairness,
and due process on the use of algorithms, is also fundamental and calls
for another question that will be faced
with the widespread use of algorithms:
who’s liable for their use? In which
situations will algorithm designers be
deemed liable versus when an enterprise or a government body which
employs the algorithm will?
Technical assurances are another
fundamental resource, to provide the
design options for data mining and
analytics with considerations that
aim to evade prejudice, inequality, or
other biased outcomes. In this realm,
engineers and researchers are developing techniques for guaranteeing that
algorithms and their implementations
shall satisfy design, performance, and
even liability standards. In a further
step, there are auditing techniques that
can be useful to determine if the algorithm meets the technical required
standards.
A tool closely related to self-regulation is the development of principles
regarding the ethical use of personal
data — which is being mentioned
sometimes as Big Data Ethics. Even if
it’s a variation of the self-regulatory
approach, some governmental bodies
have mentioned that perhaps these
principles should be developed into a
new regulatory framework.7
Another important element is that
algorithms are constantly working on
the fly, facing new and unprecedented
situations that require answers, thereby
necessitating our constant monitoring
of their outcome for evaluation. This
62

www.computer.org/internet/

is even more important in the case of
machine-learning techniques.

I

mplementing governance instruments for algorithms can occur
at multiple levels. Here, we describe
some of these levels, taking into
account that some of them would
only be considered if the risk that certain algorithms present is substantial
and concrete. Algorithm governance
processes can range from market-oriented solutions to government-based
mechanisms.
An ensemble of oversight bodies is required for structuring and
implementing algorithm governance
on a variety of instruments. It’s evident that there’s no one-size-fits-all
solution.
Private companies should approach
the use of algorithms with given
standards (if their customers are in a
position to refrain from using risky
algorithms built into their software,
services, and products), as long as
there are adequate levels of transparency and accountability in place.
For this private sector approach to
work systematically, it should be built
on a company’s internal organization, where it defines standards that
reflect public interest and establishes
a reviewing process and an internal
body to guarantee the integrity and
compliance with public interest values when using algorithms.
It can also rely on industry-wide
self-regulation processes where, for
instance, collective standards and
public interest values are defined for
a specific sector — as happens, for
example, with the auto industry defining quality and security standards for
car-embarked software. A specific
industry oversight body, which can
take the form of a multistakeholder
committee, would be responsible for
demanding information from software makers about algorithms.
Finally, a governmental oversight
body in charge of algorithm regula-

tion is another possibility for the
future, focusing on requirements such
as the level of transparency or quality of service in terms of errors, risks
of death, or injuries caused by algorithms or software, along with security breaches and other concerns.
Acknowledgment
We thank Yasodara Córdova for her valuable
insights and suggestions.

References
1. F. Pasquale, The Black Box Society: The
Secret Algorithms That Control Money and
Information, Harvard Univ. Press, 2015.
2. P. Domingos, The Master Algorithm, Basic
Books, 2015.
3. Electronic Privacy Information Center,
“Algorithmic Transparency: End Secret
Profiling,” Epic.org, 2015; https://epic.org/
algorithmic-transparency.
4. T. Gillespie, “The Relevance of Algorithms,”
Media Technologies: Essays on Communication, Materiality, and Society, T. Gillespie,
P. Boczkowski, and K. Foot, eds., MIT Press,
2014, pp. 167–194.
5. L. Introna, “Algorithms, Governance, and Governmentality: On Governing Academic Writing,” Science, Technology, & Human Values, 3
June 2015; doi:10.1177/0162243915587360.
6. H. Fang and A. Moro, “Theories of Statistical Discrimination and Affirmative Action:
A Survey,” NBER working paper no. 15860,
Nat’l Bureau of Economic Research, 2010;
www.nber.org/papers/w15860.
7. F. Saurwein, N. Just, and M. Latzer, “Governance of Algorithms: Options and Limitations,” Social Science Research Network,
vol. 17, no. 6, 2015, pp. 35–49.
8. European Data Protection Supervisor,
Towards a New Digital Ethics: Data, Dignity, and Technology, opinion 4/2015, EDPS,
11 Sept. 2015; https://secure.edps.europa.
eu/EDPSWEB/webdav/site/mySite/
shared/Documents/Consultation/Opinions/2015/15-09-11_Data_Ethics_EN.pdf.
Danilo Doneda is a professor of civil law at
the Law School of the Rio de Janeiro State
University (UERJ). His research interests
include private law and regulation, privacy, and data protection. Doneda has a
IEEE INTERNET COMPUTING

What Is Algorithm Governance?

PhD in civil law from UERJ. Contact him at
danilo@doneda.net.
Virgilio A.F. Almeida is a professor in the Computer Science Department at the Federal
University of Minas Gerais (UFMG), Brazil, and currently a visiting professor at

Harvard University. His research interests
include large-scale distributed systems, the
Internet, social computing, and cyber policies. Almeida has a PhD in computer science from Vanderbilt University. He’s the
chairman of the Brazilian Internet Steering Committee (CGI.br). Contact him at

virgilio@dcc.ufmg.br or valmeida@cyber.
law.harvard.edu.

Selected CS articles and columns
are also available for free at http://
ComputingNow.computer.org.

NOVEMBER/DECEMBER 2015
IEEE SOFTWARE
November/December 2015

IEEE Software offers

JANUARY/FEBRUARY 2016
IEEE SOFTWARE

REFACTORING

WWW.COMPUTER.ORG/SOFTWARE

pioneering ideas,

expert analyses, and

January/February 2016

thoughtful insights for
software professionals

Volume 32 Number 6

who need to keep up
WWW.COMPUTER.ORG/SOFTWARE
THE FUTURE OF SOFTWARE ENGINEERING

MARCH/APRIL 2016

with rapid technology
TINY PROGRAMMING TOOLS // 24
REQUIREMENTS
& SOCIAL RESPONSIBILITY // 109

IEEE SOFTWARE

WWW.COMPUTER.ORG/SOFTWARE

Volume 33 Number 1

March/April 2016

CODE CLARITY // 22
SOFTWARE
ON A COMET // 81

change. It’s the authority
on translating software
theory into practice.

www.computer.org/
software/subscribe

BIG DATA
Volume 33 Number 2

July/August 2016

63

