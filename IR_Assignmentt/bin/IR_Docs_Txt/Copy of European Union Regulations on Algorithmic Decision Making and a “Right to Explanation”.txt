Articles

European Union Regulations on
Algorithmic Decision Making and
a “Right to Explanation”
Bryce Goodman, Seth Flaxman

I We summarize the potential impact
that the European Union’s new General
Data Protection Regulation will have on
the routine use of machine-learning
algorithms. Slated to take effect as law
across the European Union in 2018, it
will place restrictions on automated
individual decision making (that is,
algorithms that make decisions based
on user-level predictors) that “significantly affect” users. When put into
practice, the law may also effectively create a right to explanation, whereby a
user can ask for an explanation of an
algorithmic decision that significantly
affects them. We argue that while this
law may pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in
designing algorithms and evaluation
frameworks that avoid discrimination
and enable explanation.

50

AI MAGAZINE

I

n April 2016, for the first time in more than two
decades, the European Parliament adopted a set of
comprehensive regulations for the collection, storage,
and use of personal information, the General Data Protection Regulation (GDPR)1 (European Union, Parliament and Council 2016). The new regulation has been
described as a “Copernican Revolution” in data-protection law, “seeking to shift its focus away from paperbased, bureaucratic requirements and towards compliance in practice, harmonization of the law, and
individual empowerment” (Kuner 2012). Much in the
regulations is clearly aimed at perceived gaps and inconsistencies in the European Union’s (EU) current approach
to data protection. This includes, for example, the codification of the “right to be forgotten” (Article 17), and

Copyright © 2017, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602

Articles

… for the first time in more than two decades, the
European Parliament adopted a set of comprehensive
regulations for the collection, storage, and use of
personal information

© pepifoto, IStock

regulations for foreign companies collecting data
from European citizens (Article 44).
However, while the bulk of language deals with
how data is collected and stored, the regulation contains Article 22: Automated individual decision making, including profiling (see figure 1) potentially prohibiting a wide swath of algorithms currently in use
in recommendation systems, credit and insurance
risk assessments, computational advertising, and
social networks, for example. This prohibition raises
important issues that are of particular concern to the
machine-learning community. In its current form,
the GDPR’s requirements could require a complete
overhaul of standard and widely used algorithmic
techniques. The GDPR’s policy on the right of citizens
to receive an explanation for algorithmic decisions
highlights the pressing importance of human interpretability in algorithm design. If, as expected, the
GDPR takes effect in its current form in mid-2018,
there will be a pressing need for effective algorithms
that can operate within this new legal framework.

Background
The General Data Protection Regulation is slated to
go into effect in April 2018 and will replace the EU’s
1995 Data Protection Directive (DPD). On the surface,
the GDPR merely reaffirms the DPD’s right to explanation and restrictions on automated decision making. However, this reading ignores a number of critical differences between the two pieces of legislation
(Goodman 2016a, 2016b).
First, it is important to note the difference between
a directive and a regulation. While a directive “set[s] out
general rules to be transferred into national law by
each country as they deem appropriate,” a regulation
is “similar to a national law with the difference that
it is applicable in all EU countries” (European Documentation Centre 2016). In other words, the 1995
directive was subject to national interpretation and
was only ever indirectly implemented through subsequent laws passed within individual member states
(Fromholz 2000). The GDPR, however, requires no
enabling legislation to take effect. It does not direct

FALL 2017 51

Articles

Article 22. Automated individual
decision making, including profiling
1. The data subject shall have the right not to be subject
to a decision based solely on automated processing,
including profiling, which produces legal effects
concerning him or her or similarly significantly
affects him or her.
2. Paragraph 1 shall not apply if the decision:
(a) is necessary for entering into, or performance
of, a contract between the data subject and a
data controller;
(b) is authorised by Union or Member State law to
which the controller is subject and which also
lays down suitable measures to safeguard the
data subject’s rights and freedoms and legitimate interests; or
(c) is based on the data subject’s explicit consent.
3. In the cases referred to in points (a) and (c) of
paragraph 2, the data controller shall implement
suitable measures to safeguard the data subject’s
rights and freedoms and legitimate interests, at
least the right to obtain human intervention on
the part of the controller, to express his or her
point of view and to contest the decision.
4. Decisions referred to in paragraph 2 shall not be
based on special categories of personal data
referred to in Article 9(1), unless point (a) or (g)
of Article 9(2) apply and suitable measures to safeguard the data subject’s rights and freedoms and
legitimate interests are in place.

Figure 1. Excerpt from the General
Data Protection Regulation.
(European Union, Parliament and Council 2016)

the law of EU member states, it simply is the law for
member states (or will be, when it takes effect).2
Second, the DPD and GDPR are worlds apart in
terms of the fines that can be imposed on violators.
Under the DPD, there are no explicit maximum fines.
Instead, fines are determined on a country by country basis. By contrast, the GDPR introduces EU-wide
maximum fines of 20 million euros or 4 percent of
global revenue, whichever is greater (Article 83, Paragraph 5). For companies like Google and Facebook,
this could mean fines in the billions.
Third, the scope of the GDPR is explicitly global
(Article 3, Paragraph 1). Its requirements do not just
apply to companies that are headquartered in the EU
but, rather, to any companies processing EU residents’ personal data. For the purposes of determining
jurisdiction, it is irrelevant whether that data is
processed within the EU territory or abroad.
In addition, the GDPR introduces a number of
explicit rights that increase the ability of individuals
to lodge complaints and receive compensation for
violations. These rights include the following:
The “right to lodge a complaint with a supervisory

52

AI MAGAZINE

authority” (Article 77), which individuals can exercise
in his or her place of residence, place of work, or place
of the alleged infringement.
The “right to an effective judicial remedy against a
supervisory authority” (Article 78), which may be
enforced against any supervisory authority that “does
not handle a complaint or does not inform the data
subject within three months on the progress or outcome of the complaint lodged.”
The “right to an effective judicial remedy against a
controller or processor” (Article 79) which, in the case
of more than one processors and/or controllers, specifies that each violating party has liability (see Paragraph 4).
The “right to compensation and liability” (Article 82),
which creates an obligation for both data controllers
and processors to compensate “any person who has
suffered material or nonmaterial damages as a result of
[their] infringement of this Regulation.”
The “right of representation by a body, organization or
association” (Article 80), which allows an individual to
designate a qualified not-for-profit body (such as privacy advocacy groups) to exercise data protection
rights on his or her behalf, including lodging complaints and pursuing compensation.

Taken together, these rights greatly strengthen individuals’ actual (as opposed to nominal) ability to pursue action against companies that fail to comply with
the GDPR (Pastor and Lawrence 2016).
Before proceeding with analysis, we summarize
some of the key terms employed in the GDPR as
defined in Article 4: Definitions:
Personal data is “any information relating to an identified or identifiable natural person.”
Data subject is the natural person to whom data
relates.
Processing is “any operation or set of operations which
is performed on personal data or on sets of personal
data, whether or not by automated means.”
Profiling is “any form of automated processing of personal data consisting of the use of personal data to
evaluate certain personal aspects relating to a natural
person.”

Thus profiling should be construed as a subset of
processing, under two conditions: the processing is
automated, and the processing is for the purposes of
evaluation.
The GDPR calls particular attention to profiling
aimed at “analys[ing] or predict[ing] aspects concerning that natural person’s performance at work, economic situation, health, personal preferences, interests, reliability, behavior, location or movements”
(Article 4, Paragraph 4). Given the breadth of categories, it stands to reason that the GDPR’s desideratum for profiling errs on the side of inclusion, to say
the least.
Article 22: Automated individual decision making,
including profiling, Paragraph 1 (see figure 1) pro-

Articles

hibits any “decision based solely on automated processing, including profiling” which “produces legal
effects...or similarly significantly affects” a data subject. Paragraph 2 specifies that exceptions can be
made “if necessary for entering into, or performance
of, a contract,” authorized by “Union or Member
State law” or “based on the data subject’s explicit consent.” However, Paragraph 3 states that, even in the
case of exceptions, data controllers must “provide
appropriate safeguards” including “the right to
obtain human intervention...to express his or her
point of view and to contest the decision.” Paragraph
4 specifically prohibits automated processing “based
on special categories of personal data” unless “suitable measures to safeguard the data subject’s rights
and freedoms and legitimate interests are in place.”
Note that this section does not address the conditions under which it is ethically permissible to access
sensitive data — this is dealt with elsewhere (see Article 9). Rather, it is implicitly assumed in this section
that the data is legitimately obtained. Thus the provisions for algorithmic profiling are an additional
constraint that apply even if the data processor has
informed consent from data subjects.3
These provisions present a number of practical
challenges for the design and deployment of
machine-learning algorithms. This article focuses on
two: issues raised by the GDPR’s stance on discrimination and the GDPR’s “right to explanation.”
Throughout, we highlight opportunities for
researchers.

Nondiscrimination
In general, discrimination might be defined as the
unfair treatment of an individual because of his or
her membership in a particular group, race, or gender
(Altman 2015). The right to nondiscrimination is
deeply embedded in the normative framework that
underlies the EU, and can be found in Article 21 of
the Charter of Fundamental Rights of the European
Union, Article 14 of the European Convention on
Human Rights, and in Articles 18–25 of the Treaty on
the Functioning of the European Union.
The use of algorithmic profiling for the allocation
of resources is, in a certain sense, inherently discriminatory: profiling takes place when data subjects are
grouped in categories according to various variables,
and decisions are made on the basis of subjects falling
within so-defined groups. It is thus not surprising
that concerns over discrimination have begun to take
root in discussions over the ethics of big data. Barocas and Selbst (2016) sum the problem up succinctly:
“Big data claims to be neutral. It isn’t.” As the authors
point out, machine learning depends upon data that
has been collected from society, and to the extent
that society contains inequality, exclusion, or other
traces of discrimination, so too will the data.4 Consequently, “unthinking reliance on data mining can

deny members of vulnerable groups full participation
in society” (Barocas and Selbst 2016). Indeed,
machine learning can reify existing patterns of discrimination — if they are found in the training data
set, then by design an accurate classifier will reproduce them. In this way, biased decisions are presented as the outcome of an objective algorithm.
Paragraph 71 of the recitals (the preamble to the
GDPR, which explains the rationale behind it but is
not itself law) explicitly requires data controllers to
“implement appropriate technical and organizational measures” that “prevents, inter alia, discriminatory effects” on the basis of processing sensitive data.
According to Article 9: Processing of special categories
of personal data, sensitive data includes:
personal data revealing racial or ethnic origin, political
opinions, religious or philosophical beliefs, or tradeunion membership, and the processing of genetic data,
biometric data for the purpose of uniquely identifying
a natural person, data concerning health or data concerning a natural person’s sex life or sexual orientation...

It is important to note that Paragraph 71 and Article 22 Paragraph 4 specifically address discrimination
from profiling that makes use of sensitive data. In
unpacking this mandate, we must distinguish
between two potential interpretations. The first minimal interpretation is that this requirement only pertains to cases where an algorithm is making direct use
of data that is explicitly sensitive. This would include,
for example, variables that code for race, finances, or
any of the other categories of sensitive information
referred to in Article 9. However, it is widely acknowledged that simply removing certain variables from a
model does not ensure predictions that are, in effect,
uncorrelated to those variables (Leese 2014, Hardt
2014)). For example, if a certain geographic region
has a high number of low income or minority residents, an algorithm that employs geographic data to
determine loan eligibility is likely to produce results
that are, in effect, informed by race and income.
Thus a second maximal interpretation takes a
broader view of sensitive data to include not only
those variables that are explicitly named, but also any
variables with which they are correlated. This would
put the onus on a data processor to ensure that algorithms are not provided with data sets containing
variables that are correlated with the “special categories of personal data” in Article 9.
This interpretation also suffers from a number of
complications in practice. With relatively small data
sets it may be possible to both identify and account
for correlations between sensitive and “nonsensitive”
variables. However, removing all data correlated with
sensitive variables may make the resulting predictor
virtually useless. As Calders and Verwer (2010) note,
“postal code can reveal racial information and yet at
the same time, still give useful, nondiscriminatory
information on loan defaulting.”
Furthermore, as data sets become increasingly

FALL 2017 53

Predicted probability of repayment

Articles

100

white
approval threshold

80

60

non−white

40

20

0
0

10

20

30

40

50

Non−white % of population

Figure 2. An Illustration of Uncertainty Bias.
A hypothetical algorithm is used to predict the probability of loan repayment in a setting in which the ground truth is that nonwhites and
whites are equally likely to repay. The algorithm is risk averse, so it makes an offer when the lower end of the 95 percent confidence interval for its predictions lie above a fixed approval threshold of 90 percent (dashed line). When nonwhites are less than 30 percent of the population, and assuming a simple random sample, the algorithm exhibits what we term “uncertainty bias” — the underrepresentation of
nonwhites means that predictions for nonwhites have less certainty, so they are not offered loans. As the nonwhite percentage approaches 50 percent the uncertainty approaches that of whites and everyone is offered loans.

large, correlations can become increasingly complex
and difficult to detect. The link between geography
and income may be obvious, but less obvious correlations — say between IP address and race — are likely to exist within large enough data sets and could
lead to discriminatory effects. For example, at an
annual conference of actuaries, consultants from
Deloitte explained that they can now “use thousands
of nontraditional third party data sources, such as
consumer buying history, to predict a life insurance
applicant’s health status with an accuracy comparable to a medical exam” (Robinson, Yu, and Rieke
2014). With sufficiently large data sets, the task of
exhaustively identifying and excluding data features
correlated with “sensitive categories” a priori may be
impossible. Companies may also be reluctant to
exclude certain covariates — web-browsing patterns
are a very good predictor for various recommendation systems, but they are also correlated with sensitive categories.
A final challenge, which purging variables from the
data set does not address, is posed by what we term
uncertainty bias (Goodman 2016b). This bias arises

54

AI MAGAZINE

when two conditions are met: (1) one group is underrepresented in the sample,5 so there is more uncertainty associated with predictions about that group;
and (2) the algorithm is risk averse, so it will, all
things being equal, prefer to make decisions based on
predictions about which it is more confident (that is,
those with smaller confidence intervals [Aigner and
Cain 1977]).
In practice, uncertainty bias could mean that predictive algorithms (such as for a loan approval) favor
groups that are better represented in the training
data, since there will be less uncertainty associated
with those predictions. We illustrate uncertainty bias
in figure 2. The population consists of two groups,
whites and nonwhites. An algorithm is used to decide
whether to extend a loan, based on the predicted
probability that the individual will repay the loan.
We repeatedly generated synthetic data sets of size
500, varying the true proportion of nonwhites in the
population. In every case, we set the true probability
of repayment to be independent of group membership: all individuals have a 95 percent probability of
repayment regardless of race. Using a logistic regres-

Articles

sion classifier, we consider a case in which loan decisions are made in a risk averse manner, by using the
following decision rule: check whether the lower end
of the 95 percent confidence interval for an individual is above a fixed approval threshold of 90 percent.
In all cases, all white individuals will be offered credit since the true probability is 95 percent and the sample size is large enough for the confidence interval to
be small. However, when the nonwhite population is
any fraction less than 30 percent of the total population, they will not be extended credit due to the
uncertainty inherent in the small sample.
Note that in practice, more complicated combinations of categories (occupation, location, consumption patterns, and others) would be considered by a
classifier and rare combinations will have very few
observations. This issue is compounded in an active
learning setting: consider the same setting, where
nonwhites and whites are equally likely to default. A
small initial bias toward the better represented groups
will be compounded over time as the active learning
acquires more examples of the better represented
group and their overrepresentation grows.
The GDPR thus presents us with a dilemma with
two horns: under the minimal interpretation the
nondiscrimination requirement is ineffective, under
the maximal interpretation it is infeasible. However it
would be premature to conclude that nondiscrimination measures are without merit. Rather, the complexity and multifaceted nature of algorithmic discrimination suggests that appropriate solutions will
require an understanding of how it arises in practice.
This highlights the need for human-intelligible explanations of algorithmic decision making.

Right to Explanation
The provisions outlined in Articles 13–15 specify that
data subjects have the right to access information collected about them, and also requires data processors to
ensure data subjects are notified about the data collected. However, it is important to distinguish between
these rights, which may be termed the right to access
and notification, and additional “safeguards for the
rights and freedoms of the data subject” required
under Article 22 when profiling takes place. Although
the article does not elaborate what these safeguards are
beyond “the right to obtain human intervention,”6
Articles 13 and 14 state that, when profiling takes
place, a data subject has the right to “meaningful
information about the logic involved.”7 This requirement prompts the question: what does it mean, and
what is required, to explain an algorithm’s decision?
Standard supervised machine-learning algorithms
for regression or classification are inherently based on
discovering reliable associations and correlations to
aid in accurate out-of-sample prediction, with no
concern for causal reasoning or explanation beyond
the statistical sense in which it is possible to measure

the amount of variance explained by a predictor. As
Hildebrandt (2008) writes, “correlations stand for a
probability that things will turn out the same in the
future. What they do not reveal is why this should be
the case.” The use of algorithmic decisions in an
increasingly wide range of applications has led some
(see, for example, Pasquale [2015]) to caution against
the rise of a “black box” society and demand
increased transparency in algorithmic decision making. The nature of this requirement, however, is not
always clear.
Burrell (2016) distinguishes between three barriers
to transparency: (1) intentional concealment on the
part of corporations or other institutions, where decision-making procedures are kept from public scrutiny; (2) gaps in technical literacy, which mean that,
for most people, simply having access to underlying
code is insufficient; and (3) a “mismatch between the
mathematical optimization in high-dimensionality
characteristic of machine learning and the demands
of human-scale reasoning and styles of interpretation.”
Within the GDPR, Article 13: Information to be
made available or given to the data subject goes some
way8 toward the first barrier, stipulating that data
processors inform data subjects when and why data is
collected, processed, and so forth. Article 12: Communication and modalities for exercising the rights
of the data subject attempts to solve the second by
requiring that communication with data subjects is
in “concise, intelligible and easily accessible form.”
The third barrier, however, poses additional challenges that are particularly relevant to algorithmic
selection and design. As Lisboa notes, “machine
learning approaches are alone in the spectrum in
their lack of interpretability” (Lisboa 2013).
Putting aside any barriers arising from technical
fluency, and also ignoring the importance of training
the model, it stands to reason that an algorithm can
only be explained if the trained model can be articulated and understood by a human. It is reasonable to
suppose that any adequate explanation would, at a
minimum, provide an account of how input features
relate to predictions, allowing one to answer questions such as: Is the model more or less likely to recommend a loan if the applicant is a minority? Which
features play the largest role in prediction?
There is of course a trade-off between the representational capacity of a model and its interpretability,
ranging from linear models (which can only represent
simple relationships but are easy to interpret) to nonparametric methods like support vector machines and
Gaussian processes (which can represent a rich class
of functions but are hard to interpret). Ensemble
methods like random forests pose a particular challenge, as predictions result from an aggregation or
averaging procedure. Neural networks, especially with
the rise of deep learning, pose perhaps the biggest
challenge — what hope is there of explaining the

FALL 2017 55

Articles

weights learned in a multilayer neural net with a complex architecture? These issues have recently gained
attention within the machine-learning community
and are becoming an active area of research (Kim,
Malioutov, and Varshney 2016). One promising
avenue of research concerns developing algorithms to
quantify the degree of influence of input variables on
outputs, given black-box access to a trained prediction
algorithm (Datta, Sen, and Zick 2016).

Conclusion
This article has focused on two sets of issues raised by
the forthcoming GDPR that are directly relevant to
machine learning: the right to nondiscrimination
and the right to explanation. This is by no means a
comprehensive overview of the legal landscape or the
potential challenges that engineers will face as they
adapt to the new framework. The ability of humans
to intervene in algorithmic decision making, or for
data subjects to provide input to the decision-making
process, will also likely impose requirements on algorithmic design and require further investigation.
While the GDPR presents a number of problems for
current applications in machine learning, we believe
that these are good problems to have. The challenges
described in this article emphasize the importance of
work that ensures that algorithms are not merely efficient, but transparent and fair. Research is underway
in pursuit of rendering algorithms more amenable to
ex post and ex ante inspection (Datta, Sen, and Zick
2016; Vellido, Martín-Guerrero, and Lisboa 2012; Jia
and Liang 2016). Furthermore, a number of recent
studies have attempted to tackle the issue of discrimination within algorithms by introducing tools to
both identify (Berendt and Preibusch 2012; Sandvig
et al. 2014) and rectify (Calders and Verwer 2010;
Hajian, Domingo-Ferrer, and Martinez-Balleste 2011;
Zliobaite, Kamiran, and Calders 2011; Berendt and
Preibusch 2014; Dive and Khedkar 2014; Feldman et
al. 2015) cases of unwanted bias. It remains to be seen
whether these techniques are adopted in practice.
One silver lining of this research is to show that, for
certain types of algorithmic profiling, it is possible to
both identify and implement interventions to correct
for discrimination. This is in contrast to cases where
discrimination arises from human judgment. The role
of extraneous and ethically inappropriate factors in
human decision making is well documented (see, for
example, Tversky and Kahneman [1974]; Danziger,
Levav, and Avnaim-Pesso [2011]; Abrams, Bertrand,
and Mullainathan [2012]), and discriminatory decision making is pervasive in many of the sectors where
algorithmic profiling might be introduced (see, for
example, Holmes and Horvitz [1994]; Bowen and Bok
[1998]). We believe that, properly applied, algorithms
can not only make more accurate predictions, but
offer increased transparency and fairness over their
human counterparts (Laqueur and Copus 2015).9

56

AI MAGAZINE

Above all else, the GDPR is a vital acknowledgement that, when algorithms are deployed in society,
few if any decisions are purely “technical.” Rather,
the ethical design of algorithms requires coordination
between technical and philosophical resources of the
highest caliber. A start has been made, but there is far
to go. And, with less than one year until the GDPR
takes effect, the clock is ticking.

Acknowledgements
Seth Flaxman was supported by the ERC (FP7/
617071) and EPSRC (EP/K009362/1).

Notes
1. Regulation (EU) 2016/679 on the protection of natural
persons with regard to the processing of personal data and
on the free movement of such data, and repealing Directive
95/46/EC (General Data Protection Regulation) [2016] OJ
L119/1.
2. It is crucial to note, however, that the GDPR leaves a considerable amount of latitude to member states. As a recent
article notes, while the GDPR “Europeanizes data protection
and enforcement” it also “[delegates] back to Member States
a significant power to shape the regulatory landscape for the
processing of personal data within their jurisdiction” (Mayer-Schönberger and Padova 2016, p. 325). While this tension
will no doubt be relevant to the GDPR’s implementation, a
more detailed consideration is outside our present scope.
3. Compare with “consent of the data subject should not
provide in itself a legal ground for processing such sensitive
data” (European Union, Parliament and Council 2016).
4. For an extended analysis of algorithmic discrimination,
see the paper by Goodman (2016b).
5. Note that the underrepresentation of a minority in a
sample can arise through historical discrimination or less
access to technology, but it is also a feature of a random
sample in which groups are by construction represented at
their population rates. In public health and public policy
research, minorities are sometimes oversampled to address
this problem.
6. The exact meaning and nature of the intended intervention is unspecified, and the requirement raises a number of
important questions that are beyond our current scope.
7. More precisely, the profiling must also result in a decision
that significantly affects an individual.
8. It is not clear whether companies will be required to disclose their learning algorithms or training data sets and, if
so, whether that information will be made public.
9. For an extended discussion of how the GDPR paves the
way for algorithm audits, see the papers by Goodman
(2016a, 2016b).

References
Abrams, D.; Bertrand, M.; and Mullainathan, S. 2012. Do
Judges Vary in Their Treatment of Race? Journal of Legal Studies 41(2): 347–383. doi.org/10.1086/666006
Aigner, D. J., and Cain, G. G. 1977. Statistical Theories of
Discrimination in Labor Markets. Industrial and Labor Relations Review 30(2): 175. doi.org/10.1177/00197939770
3000204
Altman, A. 2015. Discrimination. In The Stanford Encyclope-

Articles
dia of Philosophy, Fall edition, ed. E. N. Zalta. Stanford, CA: Stanford University.

Data Privacy Directive. Berkeley Technology
Law Journal 15(1): 461–484.

Barocas, S., and Selbst, A. D. 2016. Big
Data’s Disparate Impact. California Law
Review 104: 671.

Goodman, B. 2016a. Big Data’s Crooked
Timbers: Algorithmic Discrimination and
the European Union’s General Data Protection regulation. Master’s thesis, (Computational Social Science) University of Oxford,
Oxford, UK.

Berendt, B., and Preibusch, S. 2012. Exploring Discrimination: A User-Centric Evaluation of Discrimination-Aware Data Mining.
344–351. In 2012 IEEE 12th International
Conference on Data Mining Workshops. Piscataway, NJ: Institute for Electrical and Electronics Engineers.
Berendt, B., and Preibusch, S. 2014. Better
Decision Support Through Exploratory Discrimination-Aware Data Mining: Foundations and Empirical Evidence. Artificial Intelligence and Law 22(2): 175–209. doi.org/
10.1007/s10506-013-9152-0
Bowen, W. G.; and Bok, D. 1998. The Shape
of the River. Long-Term Consequences of Considering Race in College and University Admissions. Princeton, NJ: Princeton Univ. Press.
Burrell, J. 2016. How the Machine “Thinks“:
Understanding Opacity in Machine Learning Algorithms. Big Data and Society 3(1).
Calders, T., and Verwer, S. 2010. Three
Naive Bayes Approaches for DiscriminationFree Classification. Data Mining and Knowledge Discovery 21(2): 277–292. doi.org/
10.1007/s10618-010-0190-x
Danziger, S.; Levav, J.; and Avnaim-Pesso, L.
2011. Extraneous Factors in Judicial Decisions. Proceedings of the National Academy of
Sciences 108(17): 6889–6892. doi.org/10.
1073/pnas.1018033108
Datta, A.; Sen, S.; and Zick, Y. 2016. Algorithmic Transparency via Quantitative
Input Influence. Paper presented at the 37th
IEEE Symposium on Security and Privacy,
23–25 May, San Jose, CA.
Dive, R., and Khedkar, A. 2014. An
Approach for Discrimination Prevention in
Data Mining. International Journal of Application or Innovation in Engineering and Management 3(6).
European Documentation Centre. 2016.
European Commission — Legislation.
Nicosia, Cyprus: University of Nicosia.
(www.library.unic.ac.cy/EDC/EU_Resources
/EU_ Legislation.html.)
European Union, Parliament and Council.
2016. General Data Protection Regulation.
Official Journal of the European Union L 119/1
April 5.

Goodman, B. 2016b. Computer Says No:
Economic Models of (Algorithmic) Discrimination. Unpublished paper, Oxford, UK.
Hajian, S.; Domingo-Ferrer, J.; and Martinez-Balleste, A. 2011. Discrimination Prevention in Data Mining for Intrusion and
Crime Detection. In 2011 IEEE Symposium
on Computational Intelligence in Cyber Security (CICS), 4754. Piscataway, NJ: Institute for
Electrical and Electronics Engineers.
doi.org/10.1109/CICYBS.2011.5949405
Hardt, M. 2014. How Big Data Is Unfair:
Understanding Sources of Unfairness in
Data Driven Decision Making. Unpublished
paper. (medium.com/@mrtz/how-big-datais-unfair-9aa544d739de)
Hildebrandt, M. 2008. Defining Profiling: A
New Type of Knowledge? In Profiling the
European Citizen. Cross-Disciplinary Perspectives, ed. M. Hildebrandt and S. Guthwirth,
17–30. Berlin: Springer. doi.org/10.1007/
978-1-4020-6914-7_2
Holmes, A., and Horvitz, P. 1994. Mortgage
Redlining: Race, Risk, and Demand. Journal
of Finance 49(1):81–100. doi.org/ 10.1111/
j.1540-6261.1994.tb04421.x
Jia, R., and Liang, P. 2016. Data Recombination for Neural Semantic Parsing. In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics
(ACL 2016), 12–22. Stroudsberg, PA: Association for Computational Linguistics.
doi.org/10.18653/v1/p16-1002
Kim, B.; Malioutov, D. M.; and Varshney, K.
R. 2016. Unpublished Proceedings of the
2016 ICML Workshop on Human Interpretability in Machine Learning (WHI
2016). arXiv:1607.02531. Ithaca, NY: Cornell University Library.
Kuner, C. 2012. The European Commission’s Proposed Data Protection Regulation:
A Copernican Revolution in European Data
Protection Law. Bloomberg BNA. Privacy and
Security 6(2012): 115.
Laqueur, H., and Copus, R. 2015. Machines
Learning Justice: The Case for Judgmental
Bootstrapping of Legal Decisions. SSRN.
Amsterdam, Netherlands: Elsevier. doi.org/
10.2139/ ssrn.2694326

Feldman, M.; Friedler, S. A.; Moeller, J.; Scheidegger, C.; and Venkatasubramanian, S.
2015. Certifying and Removing Disparate
Impact. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, 259–268. New
York: Association for Computing Machinery.
doi.org/10.1145/2783258.2783311

Leese, M. 2014. The New Profiling: Algorithms, Black Boxes, and the Failure of AntiDiscriminatory Safeguards in the European
Union. Security Dialogue 45(5): 494–511.
doi.org/ 10.1177/0967010614544204

Fromholz, J. M. 2000. The European Union

Lisboa, P. J. 2013. Interpretability in

Machine Learning — Principles and Practice. In Fuzzy Logic and Applications. (WILF
2013), ed. F. Musulli, G. Pasi, and R. Yager.
Lecture Notes in Computer Science, vol
8256. Berlin: Springer. doi.org/10.1007/9783-319-03200-9_2
Mayer-Schönberger, V., and Padova, Y. 2016.
Regime Change? Enabling Big Data
Through Europe’s New Data Protection Regulation. The Columbia Science & Technology
Law Review 17(1)(Spring): 315–335
Pasquale, F. 2015. The Black Box Society: The
Secret Algorithms That Control Money and
Information. Cambridge, MA: Harvard University Press. doi.org/10.4159/harvard.
9780674736061
Pastor, N., and Lawrence, G. 2016. Getting to
Know the GDPR, Part 10: Enforcement under the
GDPR — What Happens If You Get It Wrong?
Amsterdam, Netherlands: Fieldfisher.
Robinson, D.; Yu, H.; and Rieke, A. 2014.
Civil Rights, Big Data, and Our Algorithmic
Future: A September 2014 Report on Social
Justice and Technology. Unpublished
Report of the Center for Media Justice, Oakland, CA.
Sandvig, C.; Hamilton, K.; Karahalios, K.;
and Langbort, C. 2014. Auditing Algorithms: Research Methods for Detecting Discrimination on Internet Platforms. Paper
presented at Data and Discrimination: Converting Critical Concerns into Productive
Inquiry, a preconference of the 64th Annual Meeting of the International Communication Association. Seattle, WA, May 22.
Tversky, A., and Kahneman, D. 1974. Judgment Under Uncertainty: Heuristics and
Biases. Science 185(4157): 11241131. doi.
org/10.1126/science.185.4157.112
Vellido, A.; Martín-Guerrero, J. D.; and Lisboa, P. J. 2012. Making Machine Learning
Models Interpretable. Paper presented at the
European Symposium on Artificial Neural
Networks, Computational Intelligence, and
Machine Learning, Bruges, Belgium, 25–27
April.
Zliobaite, I.; Kamiran, F.; and Calders, T.
2011. Handling Conditional Discrimination. In Proceedings, 2011 IEEE 11th International Conference on Data Mining, 992–1001.
Piscataway, NJ: Institute for Electrical and
Electronics Engineers. doi.org/10.1109/
ICDM.2011.72

Bryce Goodman is a research associate at
the Oxford Internet Institute. He received
his MSc from the University of Oxford.
Seth Flaxman is a researcher in the Department of Statistics at the University of
Oxford. He received his PhD from Carnegie
Mellon University.

FALL 2017 57

