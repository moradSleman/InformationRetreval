Fairness in Reinforcement Learning

arXiv:1611.03071v4 [cs.LG] 6 Aug 2017

Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Aaron Roth
Department of Computer and Information Science, University of Pennsylvania
{jabbari, majos, mkearns, jamiemor, aaroth}@cis.upenn.edu

Abstract
We initiate the study of fairness in reinforcement learning, where the actions of a learning
algorithm may affect its environment and future rewards. Our fairness constraint requires that
an algorithm never prefers one action over another if the long-term (discounted) reward of
choosing the latter action is higher. Our first result is negative: despite the fact that fairness
is consistent with the optimal policy, any learning algorithm satisfying fairness must take time
exponential in the number of states to achieve non-trivial approximation to the optimal policy.
We then provide a provably fair polynomial time algorithm under an approximate notion of
fairness, thus establishing an exponential gap between exact and approximate fairness.

1

Introduction

The growing use of machine learning for automated decision-making has raised concerns about the
potential for unfairness in learning algorithms and models. In settings as diverse as policing [22],
hiring [19], lending [4], and criminal sentencing [2], mounting empirical evidence suggests these
concerns are not merely hypothetical [1, 25].
We initiate the study of fairness in reinforcement learning, where an algorithmâ€™s choices may
influence the state of the world and future rewards. In contrast, previous work on fair machine
learning has focused on myopic settings where such influence is absent, e.g. in i.i.d. or no-regret
models [5, 6, 9, 10]. The resulting fairness definitions therefore do not generalize well to a reinforcement learning setting, as they do not reason about the effects of short-term actions on long-term
rewards. This is relevant for the settings where historical context can have a distinct influence on
the future. For concreteness, we consider the specific example of hiring (though other settings such
as college admission or lending decisions can be embedded into this framework). Consider a firm
aiming to hire employees for a number of positions. The firm might consider a variety of hiring
practices, ranging from targeting and hiring applicants from well-understood parts of the applicant
pool (which might be a reasonable policy for short-term productivity of its workforce), to exploring
a broader class of applicants whose backgrounds might differ from the current set of employees at
the company (which might incur short-term productivity and learning costs but eventually lead to
a richer and stronger overall applicant pool).
We focus on the standard model of reinforcement learning, in which an algorithm seeks to
maximize its discounted sum of rewards in a Markovian decision process (MDP). Throughout, the
reader should interpret the actions available to a learning algorithm as corresponding to choices
or policies affecting individuals (e.g. which applicants to target and hire). The reward for each
action should be viewed as the short-term payoff of making the corresponding decision (e.g. the
short-term influence on the firmâ€™s productivity after hiring any particular candidate). The actions
taken by the algorithm affect the underlying state of the system (e.g. the companyâ€™s demographics

1

as well as the available applicant pool) and therefore in turn will affect the actions and rewards
available to the algorithm in the future.
Informally, our definition of fairness requires that (with high probability) in state s, an algorithm
never chooses an available action a with probability higher than another action a0 unless Qâˆ— (s, a) >
Qâˆ— (s, a0 ), i.e. the long-term reward of a is greater than that of a0 . This definition, adapted
from Joseph et al. [10], is weakly meritocratic: facing some set of actions, an algorithm must pick
a distribution over actions with (weakly) heavier weight on the better actions (in terms of their
discounted long-term reward). Correspondingly, a hiring process satisfying our fairness definition
cannot probabilistically target one population over another if hiring from either population will
have similar long-term benefit to the firmâ€™s productivity.
Unfortunately, our first result shows an exponential separation in expected performance between the best unfair algorithm and any algorithm satisfying fairness. This motivates our study
of a natural relaxation of (exact) fairness, for which we provide a polynomial time learning algorithm, thus establishing an exponential separation between exact and approximately fair learning
in MDPs.
Our Results Throughout, we use (exact) fairness to refer to the adaptation of Joseph et al.
[10]â€™s definition defining an actionâ€™s quality as its potential long-term discounted reward. We also
consider two natural relaxations. The first, approximate-choice fairness, requires that an algorithm
never chooses a worse action with probability substantially higher than better actions. The second,
approximate-action fairness, requires that an algorithm never favors an action of substantially lower
quality than that of a better action.
The contributions of this paper can be divided into two parts. First, in Section 3, we give a
lower bound on the time required for a learning algorithm to achieve near-optimality subject to
(exact) fairness or approximate-choice fairness.
Theorem (Informal statement of Theorems 3, 4, and 5). For constant , to achieve -optimality, (i)
any fair or approximate-choice fair algorithm takes a number of rounds exponential in the number
of MDP states and (ii) any approximate-action fair algorithm takes a number of rounds exponential
in 1/(1 âˆ’ Î³), for discount factor Î³.
Second, we present an approximate-action fair algorithm (Fair-E3 ) in Section 4 and prove a
polynomial upper bound on the time it requires to achieve near-optimality.
Theorem (Informal statement of Theorem 6). For constant  and any MDP satisfying standard
assumptions, Fair-E3 is an approximate-action fair algorithm achieving -optimality in a number
of rounds that is (necessarily) exponential in 1/(1 âˆ’ Î³) and polynomial in other parameters.
The exponential dependence of Fair-E3 on 1/(1 âˆ’ Î³) is tight: it matches our lower bound on
the time complexity of any approximate-action fair algorithm. Furthermore, our results establish
rigorous trade-offs between fairness and performance facing reinforcement learning algorithms.

1.1

Related Work

The most relevant parts of the large body of literature on reinforcement learning focus on constructing learning algorithms with provable performance guarantees. E3 [13] was the first learning
algorithm with a polynomial learning rate, and subsequent work improved this rate (see Szita and
SzepesvaÌri [26] and references within). The study of robust MDPs [16, 18, 20] examines MDPs
with high parameter uncertainty but generally uses â€œoptimisticâ€ learning strategies that ignore
(and often conflict with) fairness and so do not directly apply to this work.

2

Our work also belongs to a growing literature studying the problem of fairness in machine
learning. Early work in data mining [8, 11, 12, 17, 21, 29] considered the question from a primarily
empirical standpoint, often using statistical parity as a fairness goal. Dwork et al. [5] explicated
several drawbacks of statistical parity and instead proposed one of the first broad definitions of
algorithmic fairness, formalizing the idea that â€œsimilar individuals should be treated similarlyâ€.
Recent papers have proven several impossibility results for satisfying different fairness requirements
simultaneously [7, 15]. More recently, Hardt et al. [9] proposed new notions of fairness and
showed how to achieve these notions via post-processing of a black-box classifier. Woodworth et al.
[27] and Zafar et al. [28] further studied these notion theoretically and empirically.

1.2

Strengths and Limitations of Our Models

In recognition of the duration and consequence of choices made by a learning algorithm during its
learning process â€“ e.g. job applicants not hired â€“ our work departs from previous work and aims
to guarantee the fairness of the learning process itself. To this end, we adapt the fairness definition
of Joseph et al. [10], who studied fairness in the bandit framework and defined fairness with respect
to one-step rewards. To capture the desired interaction and evolution of the reinforcement learning
setting, we modify this myopic definition and define fairness with respect to long-term rewards: a
fair learning algorithm may only choose action a over action a0 if a has true long-term reward at
least as high as a0 . Our contributions thus depart from previous work in reinforcement learning by
incorporating a fairness requirement (ruling out existing algorithms which commonly make heavy
use of â€œoptimisticâ€ strategies that violates fairness) and depart from previous work in fair learning
by requiring â€œonlineâ€ fairness in a previously unconsidered reinforcement learning context.
First note that our definition is weakly meritocratic: an algorithm satisfying our fairness definition can never probabilistically favor a worse option but is not required to favor a better option.
This confers both strengths and limitations. Our fairness notion still permits a type of â€œconditional
discriminationâ€ in which a fair algorithm favors group A over group B by selecting choices from A
when they are superior and randomizing between A and B when choices from B are superior. In
this sense, our fairness requirement is relatively minimal, encoding a necessary variant of fairness
rather than a sufficient one. This makes our lower bounds and impossibility results (Section 3)
relatively stronger and upper bounds (Section 4) relatively weaker.
Next, our fairness requirement holds (with high probability) across all decisions that a fair algorithm makes. We view this strong constraint as worthy of serious consideration, since â€œforgivingâ€
unfairness during the learning may badly mistreat the training population, especially if the learning process is lengthy or even continual. Additionally, it is unclear how to relax this requirement,
even for a small fraction of the algorithmâ€™s decisions, without enabling discrimination against a
correspondingly small population.
Instead, aiming to preserve the â€œminimalâ€ spirit of our definition, we consider a relaxation
that only prevents an algorithm from favoring a significantly worse option over a better option
(Section 2.1). Hence, approximate-action fairness should be viewed as a weaker constraint: rather
than safeguarding against every violation of â€œfairnessâ€, it instead restricts how egregious these
violations can be. We discuss further relaxations of our definition in Section 5.

2

Preliminaries

In this paper we study reinforcement learning in Markov Decision Processes (MDPs). An MDP is
a tuple M = (SM , AM , PM , RM , T, Î³) where SM is a set of n states, AM is a set of k actions, T
is a horizon of a (possibly infinite) number of rounds of activity in M , and Î³ is a discount factor.
3

PM : SM Ã— AM â†’ SM and RM : SM â†’ [0, 1] denote the transition probability distribution and
reward distribution, respectively. We use RÌ„M to denote the mean of RM .1 A policy Ï€ is a mapping
from a history h (the sequence of triples (state, action, reward) observed so far) to a distribution
over actions. The discounted state and state-action value functions are denoted by V Ï€ and QÏ€ ,
and V Ï€ (s, T ) represents expected discounted reward of following Ï€ from s for T steps. The highest
values functions are achieved by the optimal policy Ï€ âˆ— and are denoted by V âˆ— and Qâˆ— [24]. We
use ÂµÏ€ to denote the stationary distribution of Ï€. Throughout we make the following assumption.
Assumption 1 (Unichain Assumption). The stationary distribution of any policy in M is independent of its start state.
We denote the -mixing time of Ï€ by TÏ€ . Lemma 1 relates the -mixing time of any policy Ï€ to
Ï€ values of the visited states by Ï€ are close to the expected V Ï€
the number of rounds until the VM
M
values (under the stationary distribution ÂµÏ€ ). We defer all the omitted proofs to the Appendix.
Lemma 1. Fix  > 0. For any state s, following Ï€ for T â‰¥ TÏ€ steps from s satisfies
#
" T
X
Ï€
Ï€

,
VM
(st ) â‰¤ 1âˆ’Î³
Esâˆ¼ÂµÏ€ [VM
(s)] âˆ’ E T1
t=1

where st is the state visited at time t when following Ï€ from s and the expectation in the second
term is over the transition function and the randomization of Ï€.2
The horizon time HÎ³ := log ((1 âˆ’ Î³)) / log(Î³) of an MDP captures the number of steps an
approximately optimal policy must optimize over. The expected discounted reward of any policy
after HÎ³ steps approaches the expected asymptotic discounted reward (Kearns and Singh [13],
Lemma 2). A learning algorithm L is a non-stationary policy that at each round takes the entire
history and outputs a distribution over actions. We now define a performance measure for learning
algorithms.
Definition 1 (-optimality). Let  > 0 and Î´ âˆˆ (0, 1/2). L achieves -optimality in T steps if for
any T â‰¥ T
"
#
T
1X âˆ—
2
âˆ—
Esâˆ¼Âµâˆ— [VM (s)] âˆ’ E
VM (st ) â‰¤
,
(1)
T
1âˆ’Î³
t=1

with probability at least 1 âˆ’ Î´, for st the state L reaches at time t, where the expectation is taken
over the transitions and the randomization of L, for any MDP M .
We thus ask that a learning algorithm, after sufficiently many steps, visits states whose values are
arbitrarily close to the values of the states visited by the optimal policy. Note that this is stronger
than the â€œhand-raisingâ€ notion in Kearns and Singh [13],3 which only asked that the learning
algorithm stop in a state from which discounted return is near-optimal, permitting termination in
a state from which the optimal discounted return is poor. In Definition 1, if there are states with
poor optimal discounted reward that the optimal policy eventually leaves for better states, so must
Ï€ values of states
our algorithms. We also note the following connection between the average VM
visited under the stationary distribution of Ï€ (and in particular an optimal policy) and the average
undiscounted rewards achieved under the stationary distribution of that policy.
1

Note that RÌ„M â‰¤ 1 and Var(RM ) â‰¤ 1 for all states. The bounded reward assumption can be relaxed (see e.g. [13]).
Also assuming rewards in [0, 1] can be made w.l.o.g. up to scaling.
2
Lemma 1 can be stated for a weaker notion of mixing time called the -reward mixing time which is always
linearly bounded by the -mixing time but can be much smaller in certain cases (see Kearns and Singh [13] for a
discussion).
3
We suspect unfair E3 also satisfies this stronger notion.

4

Lemma 2 (Singh [23]). Let RÌ„M be the vector of mean rewards in states of M and VÏ€M the vector
of discounted rewards in states under Ï€. Then ÂµÏ€ Â· RÌ„M = (1 âˆ’ Î³)ÂµÏ€ Â· VÏ€M .
We design an algorithm which quickly achieves -optimality and we bound the number of steps
T before this happens by a polynomial in the parameters of M .

2.1

Notions of Fairness

We now turn to formal notions of fairness. Translated to our setting, Joseph et al. [10] define
action aâ€™s quality as the expected immediate reward for choosing a from state s and then require
that an algorithm not probabilistically favor a over a0 if a has lower expected immediate reward.
However, this naive translation does not adequately capture the structural differences between
bandit and MDP settings since present rewards may depend on past choices in MDPs. In particular,
defining fairness in terms of immediate rewards would prohibit any policy sacrificing short-term
rewards in favor of long-term rewards. This is undesirable, since it is the long-term rewards that
matter in reinforcement learning, and optimizing for long-term rewards often necessitates shortterm sacrifices. Moreover, the long-term impact of a decision should be considered when arguing
about its relative fairness. We will therefore define fairness using the state-action value function
Qâˆ—M .
Definition 2 (Fairness). L is fair if for all input Î´ > 0, all M , all rounds t, all states s and all
actions a, a0
Qâˆ—M (s, a) â‰¥ Qâˆ—M (s, a0 ) â‡’ L(s, a, htâˆ’1 ) â‰¥ L(s, a0 , htâˆ’1 )
with probability at least 1 âˆ’ Î´ over histories htâˆ’1 .

4

Fairness requires that an algorithm never probabilistically favors an action with lower long-term
reward over an action with higher long-term reward. In hiring, this means that an algorithm cannot
target one applicant population over another unless the targeted population has a higher quality.
In Section 3, we show that fairness can be extremely restrictive. Intuitively, L must play
uniformly at random until it has high confidence about the Qâˆ—M values, in some cases taking
exponential time to achieve near-optimality. This motivates relaxing Definition 2. We first relax
the probabilistic requirement and require only that an algorithm not substantially favor a worse
action over a better one.
Definition 3 (Approximate-choice Fairness). L is Î±-choice fair if for all inputs Î´ > 0 and Î± > 0:
for all M , all rounds t, all states s and actions a, a0 :
Qâˆ—M (s, a) â‰¥ Qâˆ—M (s, a0 ) â‡’ L(s, a, htâˆ’1 ) â‰¥ L(s, a0 , htâˆ’1 ) âˆ’ Î±,
with probability of at least 1 âˆ’ Î´ over histories htâˆ’1 . If L is Î±-choice fair for any input Î± > 0, we
call L approximate-choice fair.
A slight modification of the lower bound for (exact) fairness shows that algorithms satisfying approximate-choice fairness can also require exponential time to achieve near-optimality. We
therefore propose an alternative relaxation, where we relax the quality requirement. As described
in Section 1.1, the resulting notion of approximate-action fairness is in some sense the most fitting
relaxation of fairness, and is a particularly attractive one because it allows us to give algorithms
circumventing the exponential hardness proved for fairness and approximate-choice fairness.
4

L(s, a, h) denotes the probability L chooses a from s given history h.

5

Definition 4 (Approximate-action Fairness). L is Î±-action fair if for all inputs Î´ > 0 and Î± > 0,
for all M , all rounds t, all states s and actions a, a0 :
Qâˆ—M (s, a) > Qâˆ—M (s, a0 ) + Î± â‡’ L(s, a, htâˆ’1 ) â‰¥ L(s, a0 htâˆ’1 )
with probability of at least 1 âˆ’ Î´ over histories htâˆ’1 . If L is Î±-action fair for any input Î± > 0, we
call L approximate-action fair.
Approximate-choice fairness prevents equally good actions from being chosen at very different
rates, while approximate-action fairness prevents substantially worse actions from being chosen
over better ones. In hiring, an approximately-action fair firm can only (probabilistically) target one
population over another if the targeted population is not substantially worse. While this is a weaker
guarantee, it at least forces an approximately-action fair algorithm to learn different populations to
statistical confidence. This is a step forward from current practices, in which companies have much
higher degrees of uncertainty about the quality (and impact) of hiring individuals from underrepresented populations. For this reason and the computational benefits mentioned above, our
upper bounds will primarily focus on approximate-action fairness.
We now state several useful observations regarding fairness. We defer all the formal statements
and their proofs to the Appendix. We note that there always exists a (possibly randomized) optimal
policy which is fair (Observation 1); moreover, any optimal policy (deterministic or randomized)
is approximate-action fair (Observation 2), as is the uniformly random policy (Observation 3).
Finally, we consider a restriction of the actions in an MDP M to nearly-optimal actions (as
measured by Qâˆ—M values).
Definition 5 (Restricted MDP). The Î±-restricted MDP of M , denoted by M Î± , is identical to
M except that in each state s, the set of available actions are restricted to {a : Qâˆ—M (s, a) â‰¥
maxa0 âˆˆAM Qâˆ—M (s, a0 ) âˆ’ Î± | a âˆˆ AM }.
M Î± has the following two properties: (i) any policy in M Î± is Î±-action fair in M (Observation 4)
and (ii) the optimal policy in M Î± is also optimal in M (Observation 5). Observations 4 and 5 aid
our design of an approximate-action fair algorithm: we construct M Î± from estimates of the Qâˆ—M
values (see Section 4.3 for more details).

3

Lower Bounds

We now demonstrate a stark separation between the performance of learning algorithms with and
without fairness. First, we show that neither fair nor approximate-choice fair algorithms achieve
near-optimality unless the number of time steps T is at least â„¦(k n ), exponential in the size of the
state space. We then show that any approximate-action fair algorithm requires a number of time
1

steps T that is at least â„¦(k 1âˆ’Î³ ) to achieve near-optimality. We start by proving a lower bound for
fair algorithms.
Theorem 3. If Î´ < 41 , Î³ >

1
2

and  < 18 , no fair algorithm can be -optimal in T = O(k n ) steps.5

Standard reinforcement learning algorithms (absent a fairness constraint) learn an -optimal
policy in a number of steps polynomial in n and 1 ; Theorem 3 therefore shows a steep cost of
imposing fairness. We outline the idea for proof of Theorem 3. For intuition, first consider the
special case when the number of actions k = 2. We introduce the MDPs witnessing the claim in
Theorem 3 for this case.
5

We have not optimized the constants upper-bounding parameters in the statement of Theorems 3, 4 and 5. The
values presented here are only chosen for convenience.

6

Definition 6 (Lower Bound Example). For AM = {L, R}, let M (x) = (SM , AM , PM , RM , T, Î³, x)
be an MDP with
â€¢ for all i âˆˆ [n], PM (si , L, s1 ) = PM (si , R, sj ) = 1 where j = min{i + 1, n} and is 0 otherwise.
â€¢ for i âˆˆ [n âˆ’ 1], RM (si ) = 0.5, and RM (sn ) = x.

Figure 1: MDP(x): Circles represent states (labels denote the state name and deterministic reward). Arrows represent
actions.

Figure 1 illustrates the MDP from Definition 6. All the transitions and rewards in M are
deterministic, but the reward at state sn can be either 1 or 12 , and so no algorithm (fair or otherwise)
can determine whether the Qâˆ—M values of all the states are the same or not until it reaches sn and
observes its reward. Until then, fairness requires that the algorithm play all the actions uniformly
at random (if the reward at sn is 21 , any fair algorithm must play uniformly at random forever).
Thus, any fair algorithm will take exponential time in the number of states to reach sn . This can
be easily modified for k > 2: from each state si , k âˆ’1 of the actions from state si (deterministically)
return to state s1 and only one action (deterministically) reaches any other state smin{i+1,n} . It
will take k n steps before any fair algorithm reaches sn and can stop playing uniformly at random
(which is necessary for near-optimality). The same example, with a slightly modified analysis, also
provides a lower bound of â„¦((k/(1 + kÎ±))n ) time steps for approximate-choice fair algorithms as
stated in Theorem 4.
Theorem 4. If Î´ < 14 , Î± <
k
T = O(( 1+kÎ±
)n ) steps.

1
4, Î³

>

1
2

and  <

1
8,

no Î±-choice fair algorithm is -optimal for

Fairness and approximate-choice fairness are both extremely costly, ruling out polynomial time
learning rates. Hence, we focus on approximate-action fairness. Before moving to positive results,
we mention that the time complexity of approximate-action fair algorithms will still suffer from an
1
.
exponential dependence on 1âˆ’Î³
Theorem 5. For Î´ <

1
4,

Î± <

algorithm is -optimal for T =

1
8,

Î³ > max(0.9, c), c âˆˆ ( 21 , 1) and  <

1
O((k 1âˆ’Î³ )c )

1âˆ’ecâˆ’1
16 ,

no Î±-action fair

steps.

The MDP in Figure 1 also witnesses the claim of Theorem 5 when n = d log(1/(2Î±))
e. The discount
1âˆ’Î³
1
factor Î³ is generally taken as a constant, so in most interesting cases 1âˆ’Î³  n: this lower bound
is substantially less stringent than the lower bounds proven for fairness and approximate-choice
fairness. Hence, from now on, we focus on designing algorithms satisfying approximate-action
1
, and with tight dependence on
fairness with learning rates polynomial in every parameter but 1âˆ’Î³
1
1âˆ’Î³ .
7

4

A Fair and Efficient Learning Algorithm

We now present an approximate-action fair algorithm, Fair-E3 with the performance guarantees
stated below.

Theorem 6. Given  > 0, Î± > 0, Î´ âˆˆ 0, 21 and Î³ âˆˆ [0, 1) as inputs, Fair-E3 is an Î±-action fair
algorithm which achieves -optimality after
!
1
+5
n5 Tâˆ— k 1âˆ’Î³
(2)
T = OÌƒ
min{Î±4 , 4 }2 (1 âˆ’ Î³)12
steps where OÌƒ hides poly-logarithmic terms.
The running time of Fair-E3 (which we have not attempted to optimize) is polynomial in all
1
the parameters of the MDP except 1âˆ’Î³
; Theorem 5 implies that this exponential dependence on
1
1âˆ’Î³ is necessary.
Several more recent algorithms (e.g. R-MAX [3]) have improved upon the performance of
3
E . We adapted E3 primarily for its simplicity. While the machinery required to properly balance
fairness and performance is somewhat involved, the basic ideas of our adaptation are intuitive. We
further note that subsequent algorithms improving on E3 tend to heavily leverage the principle of
â€œoptimism in face of uncertaintyâ€: such behavior often violates fairness, which generally requires
uniformity in the face of uncertainty. Thus, adapting these algorithms to satisfy fairness is more
difficult. This in particular suggests E3 as an apt starting point for designing a fair planning
algorithm.
The remainder of this section will explain Fair-E3 , beginning with a high-level description
in Section 4.1. We then define the â€œknownâ€ states Fair-E3 uses to plan in Section 4.2, explain
this planning process in Section 4.3, and bring this all together to prove Fair-E3 â€™s fairness and
performance guarantees in Section 4.4.

4.1

Informal Description of Fair-E3

Fair-E3 relies on the notion of â€œknownâ€ states. A state s is defined to be known after all actions have
been chosen from s enough times to confidently estimate relevant reward distributions, transition
probabilities, and QÏ€M values for each action. At each time t, Fair-E3 then uses known states to
reason about the MDP as follows:
â€¢ If in an unknown state, take a uniformly random trajectory of length HÎ³ .
â€¢ If in a known state, compute (i) an exploration policy which escapes to an unknown state
quickly and p, the probability that this policy reaches an unknown state within 2Tâˆ— steps,
and (ii) an exploitation policy which is near-optimal in the known states of M .
â€“ If p is large enough, follow the exploration policy; otherwise, follow the exploitation
policy.
Fair-E3 thus relies on known states to balance exploration and exploitation in a reliable way.
While Fair-E3 and E3 share this general idea, fairness forces Fair-E3 to more delicately balance
exploration and exploitation. For example, while both algorithms explore until states become
â€œknownâ€, the definition of a known state must be much stronger in Fair-E3 than in E3 because
Fair-E3 additionally requires accurate estimates of actionsâ€™ QÏ€M values in order to make decisions
8

without violating fairness. For this reason, Fair-E3 replaces the deterministic exploratory actions
of E3 with random trajectories of actions from unknown states. These random trajectories are
then used to estimate the necessary QÏ€M values.
In a similar vein, Fair-E3 requires particular care in computing exploration and exploitation
policies, and must restrict the set of such policies to fair exploration and fair exploitation policies.
Correctly formulating this restriction process to balance fairness and performance relies heavily on
the observations about the relationship between fairness and performance provided in Section 2.1.

4.2

Known States in Fair-E3

We now formally define the notion of known states for Fair-E3 . We say a state s becomes known
when one can compute good estimates of (i) RM (s) and PM (s, a) for all a, and (ii) Qâˆ—M (s, a) for
all a.
Definition 7 (Known State). Let
m1 = O k

HÎ³ +3


n

1
(1 âˆ’ Î³) Î±

2

 !

4
 !
k
n
1
and m2 = O
.
log
HÎ³ 8 log
Î´
min{, Î±}
Î´

A state s becomes known after taking
mQ := k Â· max{m1 , m2 }

(3)

length-HÎ³ random trajectories from s.
It remains to show that motivating conditions (i) and (ii) indeed hold for our formal definition
of a known state. Informally, m1 random trajectories suffice to ensure that we have accurate
estimates of all Qâˆ—M (s, a) values, and m2 random trajectories suffice to ensure accurate estimates
of the transition probabilities and rewards.
To formalize condition (i), we rely on Theorem 7, connecting the number of random trajectories
Ï€ estimates.
taken from s to the accuracy of the empirical VM
Theorem 7 (Theorem 5.5, Kearns et al. [14]). For any state s and Î± > 0, after

2

!
Î³
1
|Î |
m = O k H +3
log
(1 âˆ’ Î³) Î±
Î´
random trajectories of length HÎ³ from s, with probability of at least 1 âˆ’ Î´, we can compute estimates
Ï€ such that |V Ï€ (s) âˆ’ VÌ‚ Ï€ (s) | â‰¤ Î±, simultaneously for all Ï€ âˆˆ Î .
VÌ‚M
M
M
Theorem 7 enables us to translate between the number of trajectories taken from a state and
Ï€ values for all policies (including Ï€ âˆ— and hence V âˆ— ). Since |Î | = k n ,
the uncertainty about its VM
M
âˆ— (s) values we increase
we substitute log (|Î |) = n log (k). To estimate Qâˆ—M (s, a) values using the VM
the number of necessary length-HÎ³ random trajectories by a factor of k.
For condition (ii), we adapt the analysis of E3 [13], which states that if each action in a state s is
taken m2 times, then the transition probabilities and reward in state s can be estimated accurately
(see Section 4.4).

9

Figure 2: Left: An MDP M with two actions (L and R) and deterministic transition functions and rewards. Green
denotes the set of known states Î“. Middle: MÎ“ . Right: M[n]\Î“ .

4.3

Planning in Fair-E3

We now formalize the planning steps in Fair-E3 from known states. For the remainder of our
exposition, we make Assumption 2 for convenience (and show how to remove this assumption in
the Appendix).
Assumption 2. Tâˆ— is known.
Fair-E3 constructs two ancillary MDPs for planning: MÎ“ is the exploitation MDP, in which
the unknown states of M are condensed into a single absorbing state s0 with no reward. In the
known states Î“, transitions are kept intact and the rewards are deterministically set to their mean
value. MÎ“ thus incentivizes exploitation by giving reward only for staying within known states. In
contrast, M[n]\Î“ is the exploration MDP, identical to MÎ“ except for the rewards. The rewards in
the known states Î“ are set to 0 and the reward in s0 is set to 1. M[n]\Î“ then incentivizes exploration
by giving reward only for escaping to unknown states. See the middle (right) panel of Figure 2 for
an illustration of MÎ“ (M[n]\Î“ ), and Appendix for formal definitions.
Fair-E3 uses these constructed MDPs to plan according to the following natural idea: when in
a known state, Fair-E3 constructs MÌ‚Î“ and MÌ‚[n]\Î“ based on the estimated transition and rewards
observed so far (see the Appendix for formal definitions), and then uses these to compute additional
Î±
for approximate-action fairness. Fair-E3 then uses these restricted
restricted MDPs MÌ‚Î“Î± and MÌ‚[n]\Î“
MDPs to choose between exploration and exploitation.
Î±
More formally, if the optimal policy in MÌ‚[n]\Î“
escapes to the absorbing state of MÎ“ with high
âˆ—
enough probability within 2T steps, then Fair-E3 explores by following that policy. Otherwise,
Fair-E3 exploits by following the optimal policy in MÌ‚Î“Î± for Tâˆ— steps. While following either of
these policies, whenever Fair-E3 encounters an unknown state, it stops following the policy and
proceeds by taking a length-HÎ³ random trajectory.

4.4

Analysis of Fair-E3

In this section we formally analyze Fair-E3 and prove Theorem 6. We begin by proving that MÎ“Î±
is useful in the following sense: MÎ“Î± has at least one of an exploitation policy achieving high reward
or an exploration policy that quickly reaches an unknown state in M .
Lemma 8 (Exploit or Explore Lemma). For any state s âˆˆ Î“, Î² âˆˆ (0, 1) and any T > 0 at least
one of the statements below holds:
â€¢ there exists an exploitation policy Ï€ in MÎ“Î± such that
max E
Ï€Ì„âˆˆÎ 

T
X

T
X


Ï€Ì„
Ï€
VM
Ï€Ì„ t (s), T âˆ’ E
VM
Ï€ t (s), T â‰¤ Î²T
Î“

t=1

t=1

10

where the random variables Ï€ t (s) and Ï€Ì„ t (s) denote the states reached from s after following
Ï€ and Ï€Ì„ for t steps, respectively.
â€¢ there exists an exploration policy Ï€ in MÎ“Î± such that the probability that a walk of 2T steps
from s following Ï€ will terminate in s0 exceeds Î²/T .
We can use this fact to reason about exploration as follows. First, since Observation 2 tells us
that the optimal policy in M is approximate-action fair, if the optimal policy stays in the set of M â€™s
known states MÎ“ , then following the optimal policy in MÎ“Î± is both optimal and approximate-action
fair.
However, if instead the optimal policy in M quickly escapes to an unknown state in M , the
optimal policy in MÎ“Î± may not be able to compete with the optimal policy in M . Ignoring fairness, one natural way of computing an escape policy to â€œkeep upâ€ with the optimal policy is
to compute the optimal policy in M[n]\Î“ . Unfortunately, following this escape policy might violate approximate-action fairness â€“ high-quality actions might be ignored in lieu of low-quality
exploratory actions that quickly reach the unknown states of M . Instead, we compute an escape
Î±
policy in M[n]\Î“
and show that if no near-optimal exploitation policy exists in MÎ“ , then the optimal
Î±
policy in M[n]\Î“ (which is fair by construction) quickly escapes to the unknown states of M .
Î±
Next, in order for Fair-E3 to check whether the optimal policy in M[n]\Î“
quickly reaches the
3
absorbing state of MÎ“ with significant probability, Fair-E simulates the execution of the optimal
Î±
policy of M[n]\Î“
for 2Tâˆ— steps from the known state s in MÎ“Î± several times, counting the ratio of
the runs ending in s0 , and applying a Chernoff bound; this is where Assumption 2 is used.
Having discussed exploration, it remains to show that the exploitation policy described in
Lemma 8 satisfies -optimality as defined in Definition 1. By setting T â‰¥ Tâˆ— in Lemma 8 and
applying Lemmas 1 and 10, we can prove Corollary 9 regarding this exploitation policy.
Corollary 9. For any state s âˆˆ Î“ and T â‰¥ Tâˆ— if there exists an exploitation policy Ï€ in MÎ“Î± then
T


1 X Ï€ t

âˆ—
E
VM Ï€ (s), T âˆ’ Esâˆ¼Âµâˆ— VM
(s) â‰¤
.
T
1âˆ’Î³
t=1

Finally, we have so far elided the fact that Fair-E3 only has access to the empirically estimated
Î±
(see the Appendix for formal definitions). We remedy this issue by showing
MDPs MÌ‚Î“Î± and MÌ‚[n]\Î“
Î±
that the behavior of any policy Ï€ in MÌ‚Î“Î± (and MÌ‚[n]\Î“
) is similar to the behavior of Ï€ in MÎ“Î± (and
Î±
M[n]\Î“
). To do so, we prove a stronger claim: the behavior of any Ï€ in MÌ‚Î“ (and MÌ‚[n]\Î“ ) is similar
to the behavior of Ï€ in MÎ“ (and M[n]\Î“ ).

Lemma 10. Let Î“ be the set of known states and MÌ‚Î“ the approximation to MÎ“ . Then for any
state s âˆˆ Î“, any action a and any policy Ï€, with probability at least 1 âˆ’ Î´:
Ï€ (s) âˆ’ min{Î±/2, } â‰¤ V Ï€ (s) â‰¤ V Ï€ (s) + min{Î±/2, },
1. VM
MÎ“
Î“
MË†
Î“

2. QÏ€MÎ“ (s, a) âˆ’ min{Î±/2, } â‰¤ QÏ€MË† (s, a) â‰¤ QÏ€MÎ“ (s, a) + min{Î±/2, }.
Î“

We now have the necessary results to prove Theorem 6.
Proof of Theorem 6. We divide the analysis into separate parts: the performance guarantee of
Fair-E3 and its approximate-action fairness. We defer the analysis of the probability of failure of
Fair-E3 to the Appendix.
11

We start with the performance guarantee and show that when Fair-E3 follows the exploitation
âˆ— values of the visited states is close to E
âˆ—
policy the average VM
sâˆ¼Âµâˆ— VM (s). However, when following
âˆ— values can be small. To
an exploration policy or taking random trajectories, visited statesâ€™ VM
3
bound the performance of Fair-E , we bound the number of these exploratory steps by the MDP
parameters so they only have a small effect on overall performance.
âˆ—
Note that in each Tâˆ— -step exploitation phase of Fair-E3 , the expectation of the average VM
âˆ— (s)âˆ’/(1âˆ’Î³)âˆ’/2 by Lemmas 1, 8 and Observation 5.
values of the visited states is at least Esâˆ¼Âµâˆ— VM
âˆ— values of the visited states is less
By a Chernoff bound, the probability that the actual average VM
âˆ— (s) âˆ’ /(1 âˆ’ Î³) âˆ’ 3/4 is less than Î´/4 if there are at least
than Esâˆ¼Âµâˆ— VM
We now bound the total number of exploratory steps of Fair-E3 by

 n 
Tâˆ—
Î³
T1 = O nmQ H + nmQ
log
,

Î´

1
log( Î´ )
2

exploitation phases.

where mQ is defined in Equation 3 of Definition 7. The two components of this term bound the
number of rounds in which Fair-E3 plays non-exploitatively: the first bounds the number of steps
taken when Fair-E3 follows random trajectories, and the second bounds how many steps are taken
following explicit exploration policies. The former bound follows from the facts that each random
trajectory has length HÎ³ ; that in each state, mQ trajectories are sufficient for the state to become
known; and that random trajectories are taken only before all n states are known. The latter bound
follows from the fact that Fair-E3 follows an exploration policy for 2Tâˆ— steps; and an exploration
âˆ—
policy needs to be followed only O( T log( nÎ´ )) times before reaching an unknown state (since any
exploration policy will end up in an unknown state with probability of at least Tâˆ— according to

Lemma 8, and applying a Chernoff bound); that an unknown state becomes known after it is visited
mQ times; and that exploration policies are only followed before all states are known.
Finally, to make up for the potentially poor performance in exploration, the number of 2Tâˆ—
steps exploitation phases needed is at least


T1 (1 âˆ’ Î³)
T2 = O
.

Therefore, after T = T1 + T2 steps we have
T

âˆ—
Esâˆ¼Âµâˆ— VM
(s) âˆ’

1 X âˆ—
2
E
VM (st ) â‰¤
,
T
1âˆ’Î³
t=1

3

2

as claimed in Equation 2. The running time of Fair-E3 is O( nT ): the additional nT factor comes
Î±
from offline computation of the optimal policies in MÌ‚Î“Î± and MÌ‚[n]\Î“
.
3
We wrap up by proving Fair-E satisfies approximate-action fairness in every round. The
actions taken during random trajectories are fair (and hence approximate-action fair) by ObservaÎ±
tion 3. Moreover, Fair-E3 computes policies in MÌ‚Î“Î± and MÌ‚[n]\Î“
. By Lemma 10 with probability at
Î±
least 1 âˆ’ Î´ any Qâˆ— or V âˆ— value estimated in MÌ‚Î“Î± or MÌ‚[n]\Î“
is within Î±/2 of its corresponding true
Î±
Î±
value in MÎ“Î± or M[n]\Î“
. As a result, MÌ‚Î“Î± and MÌ‚[n]\Î“
(i) contain all the optimal policies and (ii) only
âˆ—
contain actions with Q values within Î± of the optimal actions. It follows that any policy followed
Î±
in MÌ‚Î“Î± and MÌ‚[n]\Î“
is Î±-action fair, so both the exploration and exploitation policies followed by
3
Fair-E satisfy Î±-action fairness, and Fair-E3 is therefore Î±-action fair.

12

5

Discussion and Future Work

Our work leaves open several interesting questions. For example, we give an algorithm that has an
undesirable exponential dependence on 1/(1 âˆ’ Î³), but we show that this dependence is unavoidable
for any approximate-action fair algorithm. Without fairness, near-optimality in learning can be
achieved in time that is polynomial in all of the parameters of the underlying MDP. So, we can
ask: does there exist a meaningful fairness notion that enables reinforcement learning in time
polynomial in all parameters?
Moreover, our fairness definitions remain open to further modulation. It remains unclear
whether one can strengthen our fairness guarantee to bind across time rather than simply across
actions available at the moment without large performance tradeoffs. Similarly, it is not obvious
whether one can gain performance by relaxing the every-step nature of our fairness guarantee in
a way that still forbids discrimination. These and other considerations suggest many questions
for further study; we therefore position our work as a first cut for incorporating fairness into a
reinforcement learning setting.

References
[1] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. Propublica,
2016.
[2] Anna Barry-Jester, Ben Casselman, and Dana Goldstein. The new science of sentencing. The
Marshall Project, August 8 2015. URL https://www.themarshallproject.org/2015/08/04/
the-new-science-of-sentencing/. Retrieved 4/28/2016.
[3] Ronen Brafman and Moshe Tennenholtz. R-MAX - A general polynomial time algorithm for
near-optimal reinforcement learning. Journal of Machine Learning Research, 3:213â€“231, 2002.
[4] Nanette Byrnes. Artificial intolerance. MIT Technology Review, March 28 2016. URL https:
//www.technologyreview.com/s/600996/artificial-intolerance/. Retrieved 4/28/2016.
[5] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science,
pages 214â€“226, 2012.
[6] Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259â€“268,
2015.
[7] Sorelle Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im)possibility
of fairness. CoRR, abs/1609.07236, 2016.
[8] Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination
prevention in data mining. IEEE Transactions on Knowledge and Data Engineering, 25(7):
1445â€“1459, 2013.
[9] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning.
In Proceedings of the 30th Annual Conference on Neural Information Processing Systems, pages
3315â€“3323, 2016.

13

[10] Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in learning:
Classic and contextual bandits. In Proceedings of the 30th Annual Conference on Neural
Information Processing Systems, pages 325â€“333, 2016.
[11] Faisal Kamiran, Asim Karim, and Xiangliang Zhang. Decision theory for discrimination-aware
classification. In Proceedings of the 12th IEEE International Conference on Data Mining, pages
924â€“929, 2012.
[12] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier
with prejudice remover regularizer. In Proceedings of the European Conference on Machine
Learning and Knowledge Discovery in Databases, pages 35â€“50, 2012.
[13] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.
Machine Learning, 49(2-3):209â€“232, 2002.
[14] Michael Kearns, Yishay Mansour, and Andrew Ng. Approximate planning in large POMDPs
via reusable trajectories. In Proceedings of the 13th Annual Conference on Neural Information
Processing Systems, pages 1001â€“1007, 2000.
[15] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair
determination of risk scores. In Proceedings of the 7th Conference on Innovations in Theoretical
Computer Science, 2017.
[16] Shiau Hong Lim, Huan Xu, and Shie Mannor. Reinforcement learning in robust markov decision processes. In Proceedings of the 27th Annual Conference on Neural Information Processing
Systems, pages 701â€“709, 2013.
[17] Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. k-NN as an implementation of
situation testing for discrimination discovery and prevention. In Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 502â€“510,
2011.
[18] Shie Mannor, Ofir Mebel, and Huan Xu. Lightning does not strike twice: Robust MDPs with
coupled uncertainty. In Proceedings of the 29th International Conference on Machine Learning,
2012.
[19] Clair Miller.
Can an algorithm hire better than a human?
The New York
Times, June 25 2015.
URL http : / / www . nytimes . com / 2015 / 06 / 26 / upshot /
can-an-algorithm-hire-better-than-a-human.html/. Retrieved 4/28/2016.
[20] Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation, 17(2):
335â€“359, 2005.
[21] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In
Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 560â€“568. ACM, 2008.
[22] Cynthia Rudin.
Predictive policing using machine learning to detect patterns of
crime.
Wired Magazine, August 2013.
URL http : / / www . wired . com / insights /
2013/08/predictive-policing-using-machine-learning-to-detect-patterns-of-crime/. Retrieved
4/28/2016.
[23] Satinder Singh. Personal Communication, June 2016.
14

[24] Richard Sutton and Andrew Barto. Introduction to Reinforcement Learning. MIT Press,
Cambridge, MA, USA, 1st edition, 1998.
[25] Latanya Sweeney. Discrimination in online ad delivery. Communications of the ACM, 56(5):
44â€“54, 2013.
[26] Istvâ€™an Szita and Csaba SzepesvaÌri. Model-based reinforcement learning with nearly tight exploration complexity bounds. In Proceedings of the 27th International Conference on Machine
Learning, pages 1031â€“1038, 2010.
[27] Blake Woodworth, Suriya Gunasekar, Mesrob Ohannessian, and Nathan Srebro. Learning
non-discriminatory predictors. In Proceedings of the 30th Conference on Learning Theory,
2017.
[28] Muhammad Bilal Zafar, Isabel Valera, Gomez-Rodriguez Manuel, and Krishna P. Gummadi.
Fairness beyond disparate treatment and disparate impact: Learning classification without
disparate mistreatment. In Proceedings of the 26th International World Wide Web Conference,
2017.
[29] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In Proceedings of the 30th International Conference on Machine Learning, pages
325â€“333, 2013.

A

Omitted Proofs

A.1

Omitted Proofs for Section 2

Proof of Lemma 1. Let ÂµÌ‚Ï€T denote the distribution of Ï€ on states of M after following Ï€ for T steps
starting from s. Then we know
Ï€
Esâˆ¼ÂµÏ€ VM
(s) âˆ’

â‰¤

T

n

t=1

i=1

X
1 X Ï€
Ï€
E
VM (st ) =
(ÂµÏ€ (si ) âˆ’ ÂµÌ‚Ï€T (si )) VM
(si )
T

n
X

Ï€
|ÂµÏ€ (si ) âˆ’ ÂµÌ‚Ï€T (si )| VM
(si ) â‰¤

i=1


.
1âˆ’Î³

Ï€ (s ) â‰¤ 1 as rewards are in [0, 1]
The last inequality is due to the following observations: (i) VM
i
1âˆ’Î³
n
Ï€
Ï€
and (ii) Î£i=1 |Âµ (si ) âˆ’ ÂµÌ‚T (si )| â‰¤  since T is at least the -mixing time of Ï€.

A.2

Omitted Proofs for Section 3

We first state the following useful Lemma about M .
âˆ— (s ) <
Lemma 11. Let M be the MDP in Definition 6. Then for any i âˆˆ {1, . . . , n}, VM
i

1+2Î³ nâˆ’i+1
2(1âˆ’Î³) .

Proof.
âˆ—
VM
(si ) = discounted reward before reaching state n + discounted reward from staying at state n
"nâˆ’iâˆ’1 #
 

X Î³t
Î³ nâˆ’i+1
1
1
Î³ nâˆ’i
Î³ nâˆ’i+1
1 âˆ’ Î³ nâˆ’i Î³ nâˆ’i+1
+
=
âˆ’
=
+
<
+
2
1âˆ’Î³
2 1âˆ’Î³ 1âˆ’Î³
1âˆ’Î³
2(1 âˆ’ Î³)
1âˆ’Î³
t=1

1 + Î³ nâˆ’i (2Î³ âˆ’ 1)
1 + 2Î³ nâˆ’i+1
=
<
,
2(1 âˆ’ Î³)
2(1 âˆ’ Î³)
15

via two applications of the summation formula for geometric series.
Proof of Theorem 3. We prove Theorem 3 for the special case of k = 2 first. Consider coupling the
run of a fair algorithm L on both M (0.5) and M (1). To achieve this, we can fix the randomness of
L up front, and use the same randomness on both MDPs. The set of observations and hence the
actions taken on both MDPs are identical until L reaches state sn . Until then, with probability
at least 1 âˆ’ Î´, L must play L and R with equal probability in order to satisfy fairness (since, for
M (0.5), the only fair policy is to play both actions with equal probability at each time step). We
will upper-bound the optimality of uniform play and lower-bound the number of rounds before
which sn is visited by uniformly random play.
nâˆ’2fÎ³ for n â‰¥ 100(f )2 . First observe that the probability of
Let fÎ³ = d 1âˆ’1âˆš
Î³
3 Î³ e and T = 2
reaching a fixed state si for any i â‰¥ n âˆ’ fÎ³ from a random walk of length T is upper bounded by
the probability that the random walk takes i â‰¥ n âˆ’ fÎ³ consecutive steps to the right in the first T
steps. This probability is at most p = 2nâˆ’2fÎ³ ( 12 )nâˆ’fÎ³ = 2âˆ’fÎ³ for any fixed i. Since reaching any
state i > i0 requires reaching state i0 , the probability that the T step random walk arrives in any
state si for i â‰¥ n âˆ’ fÎ³ is also upper bounded by p.
âˆ— (s ) is a nondecreasing function of i for both MDPs. Then the
Next, we observe that VM
i
âˆ—
average VM values of the visited states of any fair policy can be broken into two pieces: the average
conditioned on (the probability at least 1 âˆ’ Î´ event) that the algorithm plays uniformly at random
before reaching state sn and never reaching a state beyond snâˆ’fÎ³ , and the average conditioned on
(the probability at most Î´ event) that the algorithm does not make uniformly random choices or
the uniform random walk of length T reaches a state beyond snâˆ’fÎ³ . So, we have that
T

1 X âˆ—
1
âˆ—
E
VM (st ) â‰¤ (1 âˆ’ p âˆ’ Î´) VM
(snâˆ’fÎ³ ) + (p + Î´)
T
1âˆ’Î³
t=1

â‰¤ (1 âˆ’ p âˆ’ Î´)

1 + 2Î³ fÎ³ +1
1
+ (p + Î´)
.
2(1 âˆ’ Î³)
1âˆ’Î³

âˆ— (s ) â‰¤ 1 for all i, and the second from
The first inequality follows from the fact that VM
i
1âˆ’Î³
âˆ— values being nondecreasing in i. Putting it all together,
Lemma 11 along with VM
âˆ—
Esâˆ¼Âµâˆ— VM
(s)



T
1 X âˆ—
1
1 + 2Î³ fÎ³ +1
1
âˆ’ E
VM (st ) â‰¥
âˆ’ (1 âˆ’ p âˆ’ Î´)
+ (p + Î´)
T
1âˆ’Î³
2(1 âˆ’ Î³)
1âˆ’Î³
t=1


1âˆ’pâˆ’Î´
1 + 2Î³ fÎ³ +1
=
1âˆ’
.
1âˆ’Î³
2

So -optimality requires


2
1âˆ’pâˆ’Î´
1 + 2Î³ fÎ³ +1
â‰¥
1âˆ’
.
1âˆ’Î³
1âˆ’Î³
2
However, if  <

1
8

(4)

we get





2
1 âˆ’ 0.04 âˆ’ 1/4
1 + 2 Ã— eâˆ’3
1 âˆ’ 2âˆ’fÎ³ âˆ’ Î´
1 + 2Î³ fÎ³ +1
<
1âˆ’
<
1âˆ’
,
1âˆ’Î³
1âˆ’Î³
2
1âˆ’Î³
2
where the third inequality follows when Î´ < 14 and Î³ > 21 . This means  < 18 makes -optimality
impossible, as desired.
Throughout we considered the special case of k = 2 and proved a lower bound of â„¦(2n ) time
steps for any fair algorithm satisfying the -optimality condition. However, it is easy to see that
16

MDP M in Definition 6 can be easily modified in a way that k âˆ’ 1 of the actions from state si
reach state s1 and only one action in each state si reaches states smin{i+1,n} . Hence, a lower bound
of â„¦(k n ) time steps can be similarly proved.
Proof of Theorem 4. We mimic the argument used to prove Theorem 3 with the difference that,
until visiting sn , L may not play R with probability more than 12 +Î± (as opposed to 12 in Theorem 3).
2
nâˆ’2fÎ³ for n â‰¥ 100(f )2 . By a similar process as in Theorem 3,
Let fÎ³ = d 1âˆ’1âˆš
Î³
3 Î³ e and T = ( 1+2Î± )
the probability of reaching state si for any i â‰¥ n âˆ’ fÎ³ from a random walk of length T is bounded
2
)âˆ’fÎ³ , and so the probability that the T steps random walk arrives in any state si for
by p = ( 1+2Î±
i â‰¥ n âˆ’ fÎ³ is bounded by p. Carrying out the same process used to prove Theorem 3 then once
more implies that -optimality requires Equation 4 to hold when Î´ < 41 , Î± < 14 and Î³ > 12 . Hence,
 < 81 violates this condition as desired.
Finally, throughout we considered the special case of k = 2. The same trick as in the proof of
k
Theorem 3 can be used to prove the lower bound of â„¦(( 1+kÎ±
)n ) time steps for any fair algorithm
satisfying the -optimality condition.
Proof of Theorem 5. We also prove Theorem 5 for the special case of k = 2 first, again considering
log(

1

)

2Î± e. Then given the
the MDP in Definition 6. We set the size of the state space in M to be n = d 1âˆ’Î³
âˆ—
âˆ—
parameter ranges, for any i, QM (si , R)âˆ’QM (si , L) > Î± in M(1). Therefore, any approximate-action
fair algorithm should play actions R and L with equal probability.
Let T = 2cn = â„¦((21/(1âˆ’Î³) )c ). First observe that the probability of reaching a fixed state si
for any i â‰¥ (c + 1)n/2 from a random walk of length T is upper bounded by the probability that
the random walk takes i â‰¥ (c + 1)n/2 consecutive steps to the right in the first T steps. This
probability is at most p = 2cn 2âˆ’(c+1)n/2 = 2(câˆ’1)n/2 for any fixed i. Then the probability that the
T steps random walk arrives in any state si for i â‰¥ (c + 1)n/2 is also upper bounded by p.
âˆ— (s ) is a nondecreasing function of i, for both MDPs. Then the
Next, we observe that VM
i
âˆ—
average VM values of the visited states of any fair policy can be broken into two pieces: the average
conditioned on the 1 âˆ’ Î´ fairness and never reaching a state beyond s(c+1)n/2 , and the average
when fairness might be violated or the uniform random walk of length T reaches a state beyond
s(c+1)n/2 . So, we have that
T

1 X âˆ—
1
âˆ—
E
VM (st ) â‰¤ (1 âˆ’ p âˆ’ Î´) VM
(s(c+1)n/2 ) + (p + Î´)
T
1âˆ’Î³
t=1

1 + (2Î³ âˆ’ 1)Î³
â‰¤ (1 âˆ’ p âˆ’ Î´)
2(1 âˆ’ Î³)

(1âˆ’c)n
2

= (p + Î´)

1
.
1âˆ’Î³

âˆ— (s ) â‰¤ 1 for all i, and the second from (the
The first inequality follows from the fact that VM
i
1âˆ’Î³
âˆ—
line before the last in) Lemma 11 along with VM values being nondecreasing in i. Putting it all
together,
(1âˆ’c)n

T

1 X âˆ—
1
1 + (2Î³ âˆ’ 1)Î³ 2
1
âˆ’ E
VM (st ) â‰¥
âˆ’ (1 âˆ’ p âˆ’ Î´)
âˆ’ (p + Î´)
T
1âˆ’Î³
2(1 âˆ’ Î³)
1âˆ’Î³
t=1
"
#
"
#
(1âˆ’c)n
(1âˆ’c)n
1 + (2Î³ âˆ’ 1)Î³ 2
1 âˆ’ p âˆ’ Î´ 1 (2Î³ âˆ’ 1)Î³ 2
1âˆ’pâˆ’Î´
=
1âˆ’
=
âˆ’
.
1âˆ’Î³
2
1âˆ’Î³
2
2

âˆ—
Esâˆ¼Âµâˆ— VM
(s)

17

So -optimality requires
2
1âˆ’pâˆ’Î´
â‰¥
1âˆ’Î³
1âˆ’Î³

"

1 (2Î³ âˆ’ 1)Î³
âˆ’
2
2

(1âˆ’c)n
2

#
.

Rearranging and using Î´ < 41 , we get that -optimality requires
ih
i
h
(1âˆ’c)n
(câˆ’1)n
1 âˆ’ (2Î³ âˆ’ 1)Î³ 2
4 â‰¥ 0.75 âˆ’ 2 2
and expand n to get
"
# "
#
1 )
1 )
(câˆ’1) log( 2Î±
(1âˆ’c) log( 2Î±
1
xy
â‰¥
0.75 âˆ’ 2 2(1âˆ’Î³)
Ã— 1 âˆ’ (2Î³ âˆ’ 1)Î³ 2(1âˆ’Î³)
â‰¡
.
4
4
Noting that x is minimized when
when

1
log( 2Î±
)
2(1âˆ’Î³)

1
(câˆ’1) log( 2Î±
)
2(1âˆ’Î³)
2

is maximized, and that this quantity is maximized

is minimized (as c âˆ’ 1 is negative), we get that -optimality requires
i
h
câˆ’1
0.75 âˆ’ 2 1âˆ’Î³ y
â‰¥
4

from Î± < 81 . Similarly, Î± <

1
8

implies that -optimality requires
ih
i
h
1âˆ’c
câˆ’1
0.75 âˆ’ 2 1âˆ’Î³ 1 âˆ’ (2Î³ âˆ’ 1)Î³ 1âˆ’Î³
.
â‰¥
4

câˆ’1

Note that 0.75 âˆ’ 2 1âˆ’Î³ is minimized when Î³ is small, so Î³ > c implies that -optimality requires
i

h
1âˆ’c
0.75 âˆ’ 2âˆ’1 1 âˆ’ (2Î³ âˆ’ 1)Î³ 1âˆ’Î³
1âˆ’c i
1 h
â‰¥
â‰¥
1 âˆ’ (2Î³ âˆ’ 1)Î³ 2(1âˆ’Î³) .
4
16
1âˆ’c

Conversely, 1 âˆ’ (2Î³ âˆ’ 1)Î³ 1âˆ’Î³ is minimized when Î³ is large, so as
1âˆ’c

lim (2Î³ âˆ’ 1) Î³ 1âˆ’Î³ = ecâˆ’1

Î³â†’1

we get that -optimality requires

1
1 âˆ’ ecâˆ’1 .
16
Finally, the same trick as in the proof of Theorem 3 can be used to prove the â„¦((k 1/(1âˆ’Î³) )c )
lower bound for k > 2 actions.
â‰¥

A.3

Omitted Proofs for Section 4

Proof of Lemma 8. We first show that either
â€¢ there exists an exploitation policy Ï€ in MÎ“ such that
T

T

X
 1 X Ï€

1
Ï€Ì„
Ï€Ì„ t (s), T âˆ’ E
VMÎ“ Ï€ t (s), T â‰¤ Î²
max E
VM
T Ï€Ì„âˆˆÎ 
T
t=1

Ï€ t (s)

where the random variables
and
Ï€ and Ï€Ì„ for t steps, respectively, or

t=1

Ï€Ì„ t (s)

18

denote the states reached from s after following

â€¢ there exists an exploration policy Ï€ in MÎ“ such that the probability that a walk of 2T steps
from s following Ï€ will terminate in s0 exceeds TÎ² .
Let Ï€ be a policy in M satisfying
T

T

t=1

t=1

X 0
1 X Ï€ t
1
Ï€
E
VM (Ï€ (s), T ) = max E
VM
(Ï€Ì„ t (s), T ) := VÌƒ .
T
T Ï€Ì„âˆˆÎ 
For any state s0 , let p(s0 ) denote all the paths of length T in M that start in s0 , q(s0 ) denote all the
paths of length T in M that start in s0 such that all the states in every path of length T in q(s0 )
are in Î“ and r(s0 ) all the paths of length T in M that start in s0 such that at least one state in
every path of length T in r(s0 ) is not in Î“. Suppose
T

1 X Ï€
E
VMÎ“ (Ï€ t (s)) < VÌƒ âˆ’ Î².
T
t=1

Otherwise, Ï€ already witnesses the claim. We show that a walk of 2T steps from s following Ï€ will
terminate in s0 with probability of at least TÎ² . First,
E

T
X

Ï€
VM
(Ï€ t (s), T )

=E

T
X
X

P[p(Ï€ t (s))]VM (p(Ï€ t (s)))

t=1 p(Ï€ t (s))

t=1

=E

T
X
X

t

t

P[q(Ï€ (s))]VM (q(Ï€ (s))) + E

t=1 q(Ï€ t (s))

T
X
X

P[r(Ï€ t (s))]VM (r(Ï€ t (s)))

t=1 r(Ï€ t (s))

since p(Ï€ t (s)) = q(Ï€ t (s)) âˆª r(Ï€ t (s)), which is a disjoint union. Next,

E

T
X
X

t

t

P[q(Ï€ (s))]VM (q(Ï€ (s))) = E

t=1 q(Ï€ t (s))

T
X
X

PÏ€MÎ“ [q(Ï€ t (s))]VMÎ“ (q(Ï€ t (s)))

t=1 q(Ï€ t (s))

â‰¤E

T
X

Ï€
VM
(Ï€ t (s), T ),
Î“

t=1

where the equality is due to Definition 9 and the definition of q, and the inequality follows
Ï€ (Ï€ t (s), T ) is the sum over all the T -paths in M , not just those that avoid the absorbing
because VM
Î“
Î“
state s0 . Therefore by our original assumption on Ï€,
E

T
X
X

t

t

P[q(Ï€ (s))]VM (q(Ï€ (s))) â‰¤ E

t=1 q(Ï€ t (s))

T
X

Ï€
VM
(Ï€ t (s), T ) < T VÌƒ âˆ’ T Î².
Î“

t=1

This implies
E

T
X
X
t=1 r(Ï€ t (s))

P[r(Ï€ t (s))]VM (r(Ï€ t (s))) = E

T
X

Ï€
VM
(Ï€ t (s), T ) âˆ’ E

P[q(Ï€ t (s))]VM (q(Ï€ t (s)))

t=1 q(Ï€ t (s))

t=1

= T VÌƒ âˆ’ E

T
X
X

T
X
X
t=1 q(Ï€ t (s))

19

P[q(Ï€ t (s))]VM (q(Ï€ t (s))) â‰¥ T Î²,

where the last step is the result of applying the previous inequality. However,
E

T
X
X

P[r(Ï€ t (s))]VM (r(Ï€ t (s))) â‰¤ T E

t=1 r(Ï€ t (s))

T
X
X

P[r(Ï€ t (s))],

t=1 r(Ï€ t (s))

P P
because it is immediate that VM (r(Ï€ t (s))) â‰¤ T for all Ï€ t (s). So T Î² â‰¤ T E Tt=1 r(Ï€t (s)) P[r(Ï€ t (s))].
Finally, if we let PÏ€2T denote the probability that a walk of 2T steps following Ï€ terminates in s0 ,
i.e.P the probability that Ï€ escapes to an unknown state within 2T steps, then for each t âˆˆ [T ],
E r(Ï€t (s)) â‰¤ T PÏ€2T . It follows that
T Î² â‰¤ T 2 PÏ€2T
and rearranging yields PÏ€2T â‰¥ TÎ² as desired.
Next, note that the exploitation policy (if it exists) can be derived by computing the optimal
policy in MÎ“ . Moreover, the exploration policy (if it exists) in the exploitation MDP MÎ“ can
indeed be derived by computing the optimal policy in the exploration MDP M[n]\Î“ as observed
Î±
by [13]. Finally, by Observation 5, any optimal policy in MÌ‚Î“Î± (MÌ‚[n]\Î“
) is an optimal policy in MÌ‚Î“
(MÌ‚[n]\Î“ )
To prove Lemma 10, we need some useful background adapted from Kearns and Singh [13].
Definition 8 (Definition 7, Kearns and Singh [13]). Let M and MÌ‚ be two MDPs with the same
set of states and actions. We say MÌ‚ is a Î²-approximation of M if
â€¢ For any state s,
RÌ„M (s) âˆ’ Î² â‰¤ RÌ„MÌ‚ (s) â‰¤ RÌ„M (s) + Î².
â€¢ For any states s and s0 and action a,
PM (s, a, s0 ) âˆ’ Î² â‰¤ PMÌ‚ (s, a, s0 ) â‰¤ PM (s, a, s0 ) + Î².
Lemma 12 (Lemma 5, Kearns and Singh [13]). Let M be an MDP and Î“ the set of known states of
M . For any s, s0 âˆˆ Î“ and action a âˆˆ A, let PÌ‚M (s, a, s0 ) denote the empirical probability transition
Â¯
estimates obtained from the visits to s. Moreover, for any state s âˆˆ Î“ let RÌ‚(s) denote the empirical
estimates of the average reward obtained from visits to s. Then with probability at least 1 âˆ’ Î´,


min{, Î±}2
|PÌ‚M (s, a, s0 ) âˆ’ PM (s, a, s0 )| = O
,
4
n2 HÎ³
and
Â¯
|RÌ‚M (s) âˆ’ RÌ„M (s)| = O



min{, Î±}2
n2 HÎ³

4


.

2

Lemma 12 shows that MÌ‚Î“ and MÌ‚[n]\Î“ are O( min{,Î±}
4 )-approximation MDPs for MÎ“ and M[n]\Î“ ,
n2 HÎ³
respectively.
2

Lemma 13 (Lemma 4, Kearns and Singh [13]). Let M be an MDP and MÌ‚ its O( min{,Î±}
4 )n2 HÎ³
approximation. Then for any policy Ï€ âˆˆ Î  and any state s and action a
Î±
Ï€
Ï€
Ï€
VM
(s) âˆ’ min{, Î±} â‰¤ VMÌ‚
(s) â‰¤ VM
(s) + min{, },
4
and
Î±
Î±
QÏ€M (s, a) âˆ’ min{ , } â‰¤ QÏ€MÌ‚ (s, a) â‰¤ QÏ€M (s, a) + min{ , }.
4
4
20

2

Proof of Lemma 10. By Definition 7 and Lemma 12, MÌ‚Î“ is a O( min{,Î±}
4 )-approximation of MÎ“ .
n2 HÎ³
Then the statement directly follows by applying Lemma 13.
Rest of the Proof of Theorem 6. The only remaining part of the proof of Theorem 6 is the analysis
of the probability of failure of Fair-E3 . To do so, we break down the probability of failure of
Fair-E3 by considering the following (exhaustive) list of possible failures:
1. At some known state the algorithm has a poor approximation of the next step, causing MÌ‚Î“
2
to not be a O( min{,Î±}
)-approximation of MÎ“ .
2 Î³4
n H

2. At some known state the algorithm has a poor approximation of the Qâˆ—M values for one of
the actions.
3. Following the exploration policy for 2Tâˆ— steps fails to yield enough visits to unknown states.
4. At some known state, the approximation value of that state in MÌ‚Î“ is not an accurate estimate
for the value of the state in MÎ“ .
We allocate

Î´
4

1. Set Î´ 0 =

Î´
4n

2. Set Î´ 0 =

Î´
4nk

of our total probability of failure to each of these sources:
in Lemma 10.
in Theorem 7.

3. By Lemma 8, each attempted exploration is a Bernoulli trial with probability of success of
at least 4T âˆ— . In the worst case we might need to make every state known before exploiting,

leading to the nmQ trajectories (mQ as Equation 3 in Definition 7) of length HÎ³ . Therefore,
the probability of taking fewer than nmQ trajectories of length HÎ³ would be bounded by 4Î´
if the number of 2Tâˆ— steps explorations is at least
 âˆ—
 n 
T nmQ
mexp = O
log
.
(5)

Î´
4. Set Î´ 0 = 4mÎ´exp (mexp as defined in Equation 5) in Lemma 10, as Fair-E3 might make 2Tâˆ—
steps explorations up to mexp times.

A.4

Relaxing Assumption 2

Throughout Sections 4.3 and 4.4 we assumed that Tâˆ— , the -mixing time of the optimal policy Ï€ âˆ— ,
was known (see Assumption 2). Although Fair-E3 uses the knowledge of Tâˆ— to decide whether
to follow the exploration or exploitation policy, Lemma 8 continues to hold even without this
assumption. Note that Fair-E3 is parameterized by Tâˆ— and for any input Tâˆ— runs in time poly(Tâˆ— ).
Thus if Tâˆ— is unknown, we can simply run Fair-E3 for Tâˆ— = 1, 2, . . . sequentially and the running
time and sample complexity will still be poly(Tâˆ— ). Similar to the analysis of Fair-E3 when Tâˆ— is
âˆ—
known we have to run the new algorithm for sufficiently many steps so that the possibly low VM
âˆ—
values of the visited states in the early stages are dominated by the near-optimal VM values of the
visited states for large enough guessed values of Tâˆ— .

21

B

Observations on Optimality and Fairness

Observation 1. For any MDP M , there exists an optimal policy Ï€ âˆ— such that Ï€ âˆ— is fair.
Proof. In time t, let state st denote the state from which Ï€ chooses an action. Let aâˆ— = argmaxa Qâˆ—M (st , a)
and Aâˆ— (st ) = {a âˆˆ A | Qâˆ—M (st , a) = Qâˆ—M (st , aâˆ— )}. The policy of playing an action uniformly at random from Aâˆ— (st ) in state st for all t, is fair and optimal.
Approximate-action fairness, conversely, can be satisfied by any optimal policy, even a deterministic one.
Observation 2. Let Ï€ âˆ— be an optimal policy in MDP M . Then Ï€ âˆ— is approximate-action fair.
Proof. Assume that Ï€ âˆ— is not approximate-action fair. Given state s, the action that Ï€ âˆ— takes from
s is uniquely determined since Ï€ âˆ— is deterministic we may denote it by aâˆ— . Then there exists a time
step in which Ï€ âˆ— is in state s and chooses action aâˆ— (s) such that there exists another action a with
Qâˆ—M (s, a) > Qâˆ—M (s, aâˆ— (s)) + Î±,
a contradiction of the optimality of Ï€ âˆ— .
Observations 1 and 2 state that policies with optimal performance are fair; we now state that
playing an action uniformly at random is also fair.
Observation 3. An algorithm that, in every state, plays each action uniformly at random (regardless of the history) is fair.
Proof. Let L denote an algorithm that in every state plays uniformly at random between all available actions. Then L(s, htâˆ’1 )a = L(s, htâˆ’1 )a0 regardless of state s, (available) action a, or history
htâˆ’1 . Qâˆ—M (s, a) > Qâˆ—M (s, a0 ) + Î± â‡’ L(s, htâˆ’1 )a â‰¥ L(s, htâˆ’1 )a0 then follows immediately, which
guarantees both fairness and approximate-action fairness.
Observation 4. Let M be an MDP and M Î± the Î±-restricted MDP of M . Let Ï€ be a policy in M Î± .
Then Ï€ is Î±-action fair.
Proof. Assume Ï€ is not Î±-action fair. Then there must exist round t, state s, and action a such
that Qâˆ—M (s, a) > Qâˆ—M (s, a0 ) + Î± and L(s, htâˆ’1 )a < L(s, htâˆ’1 )a0 . Therefore L(s, htâˆ’1 )a0 > 0, so M Î±
must include action a0 from state s. But this is a contradiction, as in state s M Î± only includes
actions a0 such that Qâˆ—M (s, a0 ) + Î± â‰¥ Qâˆ—M (s, a). Ï€ is therefore Î±-action fair.
Observation 5. Let M be an MDP and M Î± the Î±-restricted MDP of M . Let Ï€ âˆ— be an optimal
policy in M Î± . Then Ï€ âˆ— is also optimal in M .
Proof. If Ï€ âˆ— is not optimal in M , then there exists a state s and action a such that Qâˆ—M (s, a) >
Eaâˆ— (s)âˆ¼Ï€âˆ— (s) Qâˆ—M (s, aâˆ— (s)) where aâˆ— (s) is drawn from Ï€ âˆ— (s) and the expectation is taken over choices
of aâˆ— (s). This is a contradiction because action a is available from state s in M Î± by Definition 5.

22

C

Omitted Details of Fair-E3

We first formally define the exploitation MDP MÎ“ and the exploration MDP M[n]\Î“ :
Definition 9 (Definition 9, Kearns and Singh [13]). Let M = (SM , AM , PM , RM , T, Î³) be an MDP
with state space SM and let Î“ âŠ‚ SM . We define the exploration MDP MÎ“ = (SMÎ“ , AM , PMÎ“ , RMÎ“ , T, Î³)
on Î“ where
â€¢ SMÎ“ = Î“ âˆª {s0 }.
â€¢ For any state s âˆˆ Î“, RÌ„MÎ“ (s) = RÌ„M (s), rewards in MÎ“ are deterministic, and RÌ„MÎ“ (s0 ) = 0.
â€¢ For any action a, PMÎ“ (s0 , a, s0 ) = 1. Hence, s0 is an absorbing state.
â€¢ For any states s1 , s2 âˆˆ Î“ and any action a, PMÎ“ (s1 , a, s2 ) = PM (s1 , a, s2 ), i.e. transitions
between states in Î“ are preserved in MÎ“ .
â€¢ For any state s1 âˆˆ Î“ and any action a, PMÎ“ (s1 , a, s0 ) = Î£s2 âˆˆÎ“
/ PM (s1 , a, s2 ). Therefore, all
the transitions between a state in Î“ and states not in Î“ are directed to s0 in MÎ“ .
Definition 10 (Implicit, Kearns and Singh [13]). Given MDP M and set of known states Î“,
the exploration MDP M[n]\Î“ on Î“ is identical to the exploitation MDP MÎ“ except for its reward
function. Specifically, rewards in M[n]\Î“ are deterministic as in MÎ“ , but for any state s âˆˆ Î“,
RÌ„M[n]\Î“ (s) = 0, and RÌ„M[n]\Î“ (s0 ) = 1.
We next define the approximation MDPs MÌ‚Î“ and MÌ‚[n]\Î“ which are defined over the same set
of states and actions as in MÎ“ and M[n]\Î“ , respectively.
Let M be an MDP and Î“ the set of known states of M . For any s, s0 âˆˆ Î“ and action a âˆˆ A,
let PÌ‚MÎ“ (s, a, s0 ) denote the empirical probability transition estimates obtained from the visits to
Â¯
s. Moreover, for any state s âˆˆ Î“ let RÌ‚MÎ“ (s) denote the empirical estimates of the average reward
obtained from visits to s. Then MÌ‚Î“ is identical to MÎ“ except that:
Â¯
â€¢ in any known state s âˆˆ Î“, RÌ‚MÌ‚Î“ (s) = RÌ‚MÎ“ (s).
â€¢ for any s, s0 âˆˆ Î“ and action a âˆˆ A, PMÌ‚Î“ (s, a, s0 ) = PÌ‚MÎ“ (s, a, s0 ).
Also MÌ‚[n]\Î“ is identical to M[n]\Î“ except that:
â€¢ for any s, s0 âˆˆ Î“ and action a âˆˆ A, PMÌ‚

[n]\Î“

(s, a, s0 ) = PÌ‚M[n]\Î“ (s, a, s0 ).

23

