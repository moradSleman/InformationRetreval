O M E G A Int J of M g m t Scl. Vol 17, N o 4. pp 335-344. 1989

0305-0483/89 53 00 + 0 00

Printed m Great Britain All nights reserved

Copynght ~ 1989 Maxwell Pergamon Macmdlan pie

Cognitive Biases in the Use of
Computer-Based Decision Support Systems
CT KYDD
Umverslty o f Delaware, Newark, U S A

(Recewed December 1988)
When cognitive biases interfere with the decision-making process, they can seriously influence the
quality of the final decision. Such biases may also, therefore, be limiting the effective use of
eompmer-baed decision support systems. Thus, it is important to determine whether o r n o t biases can
he JuuelJoruted during decision-making when the process is supported by a computerized system and,
if so, how this deblasing con he achieved. This paper suggests several ways of deblu/ng Individuals
in a compater-supported decis/on proeess and presents the results of a study which tests o n e dchjasing
method. Results indicate that hiases do hamper computer-supported decision-making and that
debisoing may he very diflicolt to achieve.

Key words---cognmve bias, decision support systems, decision-making, reformation processmg.
biases

INTRODUCTION
DurttNG THE PAST 20 years, a large body of
literature has developed which reports the use of
simplifying heuristics in the decision-making
process. Such heuristics are helpful to decisionmakers in problem settings that are highly
complex, riddled with uncertainty, or stifled by
limited relevant information. Although heuristics can facilitate the decision-making process,
systematic cognitive biases are often introduced
into the process through the use of such heuristics, resulting in serious and persistent errors in
judgment and final decision outcome (see
[16, 17, 26] for summaries of such biases). As
these biases pervade decision-making behaviour
and can seriously influence decisions, they may
also be limiting the effective use of various
computer-based decision aids and support
systems.
To date, few studies have examined the
role that cognitive biases play in the decisionmaking process when that process is dependent upon use of a computer-based decision
support system. Those that have examined
cognitive bias in the context of a DSS have
done little to suggest methods for correcting

errors due to bias [21, 27], or have focused
exclusively on bias related to statistical concept
errors [26]. It Is time to take a broader look
at how general cognitive biases can interfere with effective decision-making when the
decision-maker zs supported by a computer
system, and how subsequent errors can perhaps
be eliminated
In order to explore the posslbihty of debiasing, first, it is important to idenufy several
specific ways in which cognitive biases may
interfere with the effective use of a decision
support system, or inhibit full use of such a
system. Such conclusions are based on what is
known about the occurrence of biases in general
decision settings. Second, it is important to
consider whether it ts possible to debias, and if
so, how cognitive bias can be reduced when an
individual is utilizing a computer-based decision
aiding system. Two strategies have been suggested to date for augmenting a DSS, including
(1) to build mechanisms into the internal structure of the decision-aiding system [1, 26], or (2)
implement mechanisms external to the support
system but within the broader decision setting
[1, 3]. However, there have been no empirical
335

336

Kydd--Cogmttve Bmses m Computer-Based Dectsmn Support Systems

perception bias occurs when individuals selectively acquire information which is consistent
with preconceived notions, and fall to collect
and attend to other, perhaps conflicting, data
(1) to gain a better understanding of how [2, 8, 34]. For example, a manager may pay
cognitive biases interfere with the effec- attention only to positive performance evaluative use of computer-based decision tions regarding a candidate for promotion
about whom the manager has a preconceived
support systems;
favorable impression. An individual using a
(2) to examine potential ways of ameliorat- decision-aiding system may exhtbit this bias by
ing the detrimental effects of such biases; using the system to access only a portion of the
available mformation that is relevant to the
(3) to study one particular debiasing decision at hand, and which supports some
prior viewpoint Paradoxically, a decision supmethod.
port system may actually facilitate an individual
To accomplish this, specific cognitive biases that falling prey to this bias, as most systems provide
have been found to interfere with effective deci- powerful query languages specifically designed
sion-making are reviewed. We suggest how deci- to support selective retrieval of information
sion-makers may be particularly susceptible to from large databases. In addition, the fact that
several of these biases in the course of using a the reformation "came from the computer" may
computer-based support system. In Section 3, tend to legitimize decisions based on the inforwe recommended potential strategies for debias- mation irrespective of the completeness of the
ing in a DSS setting for particular biases. The information.
results of an empirical study concerning the
A second example of a bias that may occur
existence of bias and corrective procedures are during information acquisition is the availreported in Section 4, followed by discussion ability bias. In th~s case, an indiv~duars judgand conclusion.
ment concerning the frequency of an event is
affected by the ease with which the individual
can
recall from memory specific instances of
SUMMARY OF SELECTED BIASES
that event [19,,20, 31]. Events that are more
A comprehensive summary of well- sahent for a variety of reasons (e.g. recency
documented judgmental biases can be found in effect, primacy effect) are more easily remem[16,26,27]. Hogarth and Makridakis [16] bered and therefore influence the perceived
provide a useful grouping of these biases ac- probability of that event occurring again in
cording to the stages of information processing the future. It also occurs when the chance
m which they are most likely to occur. The availability of a particular 'cue' in the
stages are information acquisition, information immediate environment affects an individual's
processing, output and feedback. Examples are judgment. For example, the frequency of a
provided below for each stage of decision- particular type of event where one instance is
making where cognitive biases may have well publicized, is often over-estimated, such as
a detrimental impact on decision behaviour death due to homicide, because ~t ~s easily
supported by a computerized system. Specific 'available' in memory. In the context of a
biases are mentioned and described briefly for computer-based decision support system there
each stage.
may be factors inherent in the system itself, such
as information presentation mode [6], which
Information acquisition
makes certain pieces of information more easily
During the first stage in the overall informa- available to the user than others. This may lead
tion processing task, information which is rele- to a poor decision due to the interference of the
vant to the decision at hand must be gathered. availability bias. Further, the use of the system
Since the typical decision-maker cannot poss- could be influenced by the ease with which
ibly collect all information that has some certain information in the user's memory can be
relevance for his or her particular problem, it is recalled. This could also lead to a less than
often acquired in a selective way. The selective objectively determined decision.
tests of either of these strategies reported in the
literature.
The purpose of this research is:

Omega, Vol 17, No 4

A third judgmental bias that may interfere
with the information acquisition stage of decision-making occurs when concrete information
is allowed to dominate abstract data [4, 18]. This
bias is generally referred to as base-rate error.
It occurs when an individual places greater
weight on a single first-hand experience than on
the usually more valid statistical base-rate information. In the context of a DSS, a human
resource manager, for example, may ignore
base-rate salary increase numbers provided by a
computer system and instead focus on his or her
own salary increase in a particularly good or
poor year. The perception of salary increases IS
biased in this case toward the individual's own
single increase experience.
As the user of a DSS controls what information is accessed, acquisition biases such as
these can easily lead the user to request only
a subset of the information potentially available. To correct this, users need to be forced
to realize that these biases may be predisposing them to a limited use of the decision
support system. A specific example of how this
might be accomplished is given in the next
section.

Information processing
Once information has been acquired, it must
then be sorted and processed before any conclusions can be reached. During this stage of
decision-making, individuals are faced with the
assimilation and processing of large amounts
of information. To overcome the difficulty associated with this task, they may resort to the use
of simplifying heuristics which can make them
susceptible to cognitive bias.
One such bias is the conservatism bias, which
occurs when prior probabilities concerning a
specific event are not correctly revised upon
receipt of new information about that event
[7,21]. Specifically, after receiving new and
more recent data, individuals may not update
prior estimates to the extent called for by Bayes'
theorem. This can result in a far more conservative final estimate than should be the case.
Related to the conservatism bias is the anchoring and adjustment bias [29, 31]. In this case,
individuals predict future values simply by
making what is thought to be an appropriate
adjustment to some anchor or base value.
However, it is often the case that insufficient
adjustment is made to the base value to allow

337

for the circumstances of the current case. Therefore, a bmsed estimate results.
Another related bias occurs when individuals assume that a small sample has the
same statistical properties as a large sample
[31, 33]. Based on this belief, inferences are
made which may not be correct. This is generally referred to as the 'law of small numbers'.
In a computerized decision support setting, it
~s easy to imagine how this bias could occur,
particularly in light of the numerous, highly
user-friendly statistical packages that are
currently available. Even when a small sample
of data points is analyzed, most systems provide
the same extensive statisUcs that would be provlded for a large sample, thereby contributing
to the decision-maker's faulty conclusion that
the results are valid. Few packages make a
dlstincUon between those results based on large
samples compared to small samples. Those that
do (e.g. SAS [28] provides warning messages)
seem to be meffectwe at alleviating these types
of biases.
The major reason why decision-makers fall
prey to these biases during information processing is that individuals are often poor intuitive
statisticians and so do not make the most
effectwe use of the information available
through the DSS. Although users may intend to
correctly utilize the available data, they may not
be able to do so if they do not have a good
understanding of the appropriate underlying
statistical concepts. To correct for biases that
may undermine effective information processing, we suggest that mechanisms be budt into
the DSS to help users overcome their hmitations. Such a mechanism would be an internal
corrective procedure (a specific example is given
m the next section)

Output and feedback
In this last stage of decision-making, the
individual makes a choice among alternatwes
and receives feedback about the quality of the
decision that was made. Several biases can
interfere with the use of a DSS during this stage,
based primarily on the need of individuals to
receive certain types of feedback about their
decision. The problem at this stage is a critical
one because individuals learn about their mistakes as well as about their successes through
feedback. If they mistakenly attribute failures to
poor luck in chance events rather than to their

338

Kydd-Cogntttve

Biases tn Computer-Based

own shortcomings (success/failure attribution
error [24,25]), or if they conveniently ‘forget’
why they made the choice they did, or even what
that choice was, then it is almost impossible
for them to learn from their mistakes. This
inhibition of learning during the feedback stage
can be caused by a number of biases, including
hindsight (mistakenly thinking that foresight
was as accurate as hindsight now is [1 1, 13, 14]),
success/failure
attribution
bias (described
above) and learning based on outcome irrelevant structures (observed outcomes lead to
belief in incorrect predictive relationships based
on incomplete information [9, lo]).
The major way in which biases interfere
with effective use of decision support systems
during feedback is that they inhibit learning
and, therefore, growth and development for
future decision requirements. In the case of the
success/failure
attribution
bias, individuals
cannot learn from their mistakes, when they
automatically attribute good decisions to their
own abilities and blame poor ones on uncontrollable factors. Similarly, in the case of outcome irrelevant learning structures, complete
information is usually not available, so it is
almost impossible for individuals to correct
their perceptions of predictive relationships.
There are several procedures that could be
instituted to correct for cognitive biases during
output and feedback. First, systematic procedures could be developed for keeping track of
reasons for a resulting ‘good’ or ‘poor’ decision.
That is, the individual decision-maker’s process
of choosing one alternative over another could
be recorded so that the true reasons for either
success or failure would be known. Individuals’
attributions to other causes could then be identified as biased justifications for one or the other
type of outcome.
Further, this decision record, including the
decision-maker’s assumptions, expectations and
processes, could help the individual to realize
why the decision was determined in the way
that it was rather than in the way in which
the individual remembers making it. This
record should provide insight for the decisionmaker in terms of being made to realize that
hindsight is far more accurate than is foresight.
This is another case where the most effective
debiasing methods may be external to the DSS
itself, but very important to the overall use of
the system.

Dectston Support Systems

PROPOSED DEBIASINC TECHNIQUES
Although it is now widely recognized that
cognitive biases often influence the results of
complex information processing and decisionmaking, little research has been completed to
date which attempts to reduce or eliminate the
problems that they present. Those techniques
which have been proposed for debiasing can
generally be characterized according to the
framework set out by Fischhoff [12]. In his
attempt to identify ways in which biases might
be ameliorated, he has categorized debiasing
methods into three broad classes including:

(1) correcting

flaws in the experimental
design surrounding the decision task;

(2) demonstrating

to decision-makers that
the biases exist and teaching them ways
to avoid them;

for
possible
mismatches
(3) correcting
between decision-maker and task.
More specifically, the techniques include such
methods as warning individuals of the potential bias problem, providing feedback and/or
extensive training in avoidance of biases, forcing
individuals to search for discrepant information, decomposing the problem, offering alternative formulations of the problem and relying
on expert opinion which is relatively unbiased.
Recently, Remus and Kotteman [26] proposed
the inclusion of an expert system component
within DSS to help decision-makers overcome
their statistical shortcommgs.
The implicit hypothesis is that the implementation of several of these techniques can help
reduce the effects of cognitive biases during use
of decision support systems. The techniques can
be built in as internal aspects of the system or
instituted as external procedures which accompany use of the DSS. This dichotomy depends
in part on the nature of the bias itself and in part
on what each debiasing technique involves. Two
examples are provided below regarding ways in
which debiasing may be achieved within the
context of decision support system use. One
focuses on an internal implementation of combatting the ‘law of small numbers’, while the
other discusses an external method for avoiding
the selective perception bias.

Omega, Vol 17, No 4

Debiasing internal to the D S S
Decision support systems provide powerful
retrieval capabilities for extracting subsets or
samples from the database. Statistical capabilities are also often incorporated within the DSS.
Interpretation of such information acquired
from the DSS may be biased by the recipient's
reliance on the 'law of small numbers', i.e.
granting too much influence to results based on
a small sample. One approach which might help
prevent this bias is to notify the individual of the
sample size from which a statistic has been
calculated and explicitly warn the individual if
the sample size is small. Further, the DSS could
calculate and display information such as the
confidence interval which would give the user a
better sense of the implications of the sample
size. In addRion, computer graphics could be
utilized to present the idea of 'power' (B error)
m an intuitive way.
Some systems do provide such information
but it is easily overlooked by the user examining
a large amount of data. Thus, it is important to
highlight warnings so that the user is certain to
attend to them. Of course, such information on
sample size and confidence intervals would be
more meaningful to users who had previous
exposure to statistics. By building such automatic responses and warning messages into the
DSS itself, the user would be forced to pay
attention to important details such as sample
size, and possibly gain insight into critical statistical concepts. The expert DSS described by
Remus and Kotteman [26] is an example of an
internal debmsing method.
Debiasing external to the D S S
Selective perception occurs when individuals
selectively seek and acquire information that
confirms their views and values, and at the same
time, disregard or ignore disconfirming evidence. An individual using a DSS may exhibit
this bias by utilizing the DSS to access only a
portion of the available information which is
relevant to the decision at hand. With the
advent of friendly and powerful database systems which facilitate selective retrieval, individuals may be even more prone to such biased
information acquisition now than in the past.
An approach which might mitigate the
effects of selective perception is the use of a
'devil's advocate' [5]. Here, another individual

339

(the devil's advocate) makes use of the same
DSS in a specific attempt to discover other
relevant and possibly disconfirming information. The results may prove surprising to the
victim of selective perception by clearly demonstrating the extent of his or her myopia and its
effect on decision-making. In addition, if properly institutionalized, this strategy might serve
as a deterrent to those who would make opportumsUc use of a DSS.
The remainder of the paper reports the results
of a study which examined cognitive bias in a
decision task supported by a computer system,
and a debmsing strategy. The study provides
some evidence concerning whether or not cogniUve bias interferes with the effective use of a
DSS, as well as debiasing possibilmes. The
major hypotheses of the study included:
(1) biases do hamper effective use of a DSS
m decision-making;

(2) those individuals who are exposed to the
deblasmg strategy will make less biased
decisions than will those who are not
exposed to debiasing.
METHODOLOGY
An experiment was developed specifically to
test for (I) the influence of cognmve blas on
a decision made with the help of a declsion
support system; and (2) the effectiveness of
an external debiasing technique. A forecasting
decision was chosen to be the setting for the
experiment because this type of declsion provldes sufficient uncertainty and complexity. It
is also an application area in whlch numerous
biases could conceivably exist and influence
decisions [16]. Further, complicated forecasting
decisions in the field are generally made with the
help of some type of computer system and
database.
The specific decision support system selected
for the experiment was MICROSTAT, a statistical package with options for data mampulation and selection, as well as forecasting using a
variety of methods. This particular system was
chosen because it is easy for subjects to learn to
use and it is readily available. It also provides a
moderate amount of support, while not completely structuring the forecasting problem for
the individual.

340

Kydd--Cognmve Bzase$ m Computer-Based Decmon Support Systems

The experiment consisted of the presentation
to subjects of two different versions of a forecasting scenario. In the first case, the subject was
asked to develop a forecast of sales for a
hypothetical company for the next three periods. Baseline information was provided about
the firm in a small database. The specific
method to be used to determine the forecast was
unspecified. However, the subject was furnished
with pencil and paper, calculator and the use of
MICROSTAT on an IBM PC. This provided
the subject with a complete range in terms of the
complexity and level of sophistication of the
forecasting method chosen.
In the second case, the subject was provided
with the same baseline conditions and information as in the first case. However, this scenario
included additional information concerning
what the subject's hypothetical manager felt
would be an appropriate forecast for the next
three periods. The manager's forecast was
significantly higher than what the data supported. The hypothesis was that those subjects
given the second scenario would forecast different (i.e. higher) sales figures than would those
subjects who were given the first scenario. This
should occur if a bias such as availability or
selective perception, for example, was operating
during the decision.
One manifestation of the availability bias
would be final forecast numbers that closely
followed the manager's preferences, based on
the salience of the manager's recommended
forecast. In the case of selective perception,
the subject may selectively (and inadvertently) attend to only those data which supported a forecast similar to the manager's
recommendation.
The third condition in the experiment focused
on debiasing. In this condition, subjects were
provided with the second scenario (including
information about the boss's opinion), but were
also warned about the existence of cognitive
biases. The instruction and warning about
biases were given verbally by the experimentor.
In this way, the participants received some training concerning cognitive biases and associated pitfalls.The hypothesis in this case was that those
individuals who received the debiasing treatment would forecast sales figures which were
closer to those forecasted by the firstexperimental group than to those forecasted by the second
group, if the debiasing training was successful.

The instrument itself presented one of the
scenarios described above, along with a listing
of the data available in the database. This data
included:
(1) the company's past sales figures;
(2) the firm's selling and administrative
expenses;
(3) net income per share;
(4) the sales of the major competitor;
(5) the competitor's selling and administrative expenses;
(6) GNP;

(7) personal consumption figures for the
relevant product group;
(8) unemployment rate;
(9) industrial index for the relevant product
group;
(10) disposable personal income;
(11) stock prices for the firm, all for the last
ten periods.
Each subject was asked to develop three forecasts (one for each of the next three periods) for
the firm. The subjects were also asked to report
what method had been used in arriving at the
forecasts, and what other factors (if any) had
been taken into consideration in making their
forecasts.
The sample of participants in this experiment
was made up of upper level undergraduate
business administration students. All subjects
had previous instruction in the basics of forecasting methods and philosophy, and all had
experience with MICROSTAT. There were 19,
45, and 22 subjects in Condition l, Condition 2
and Condition 3, respectively. Although there
has been concern in recent literature regarding
the use of student subjects [15], in this case we
feel that student subjects are acceptable. There
is no evidence to date that suggests that cognitive biases discriminate in terms of who falls
prey to their pitfalls. Further, the great majority

Omega, Vol 17, No 4

of the subjects were either cooperative education students or had had business experience
through summer or part time jobs. Therefore,
they were roughly comparable to company analysts with minimal firm experience but a good
understanding of forecasting.

Table 2 Mean sales forecasts for next three periods
Coulltiw I vs ComUflm 3
Condlton I
(control)

Condmon 3
(detnasmg)

Period I

5,382,532

7,120,201

Period 2

5,986,041

7,829,925

Penod 3

6,732,090

8,461,048

RESULTS

Existence of biasmCondition 1 vs Condition 2

When the forecasts resulting from those subjects in Condition 1 (control group: straight
forecasting task) were compared to those based
on subjects' responses in Condition 2 (given
reformation about the manager's preferences),
there was a significant difference (p <0.01)
between the mean forecasted values for the two
groups. The average forecasted value for Condition 1 was significantly lower than the average
for Condition 2. This held true for all three
periods for which the subjects were asked to
develop a forecast. The data are summarized in
Table 1. Th~s result suggests that a bias may
have been operating within Condition 2.
Debiasing warningmCondition 3 vs Conditions
I and 2
The mean forecast values resulting from those
subjects in Condition 3 (given information
about the manager's preferences but warned
about biases) were compared to the mean forecast values from Condition 1 and Condition 2.
There was a significant difference between the
Table 1 Mean sales for the next three periods
Cendlflm I vs Cosdttim 2

Period I

CondtUon 2
(manager mfo)

5,382,532

6,898,547

t-value
- 2 339
t6e o02s - - 2 . 0

Period 2

5,986,041

7,608,597

Penod 3

6,732,090

8,220,265

- 2 708
teo ooe~ ffi --2 66

OME L7~4----C

t -value
- 2 789
t,o 0oo~= - 2 . 7 0 4
- 3 058
t4e 0ee~.s= - 2 9 7 1
- 2 569
he o0t = - 2 423

Condlflna 2 vs Ceadifloa 3

Several interesting findings emerged from the
analysis of the data collected in this experiment.
First the general hypothesis concerning the existence of cognitive bins in using computer-based
decismn support systems is strongly supported.
Second, the attempt at debiasing through instrucuon and warning of the existence of cogniUve bias did not appear to be successful. Both
of these results are discussed further below, with
supporting data.

Condition I
(control)

341

- 2 396
t~ ool = - 2 39

Condmon 2
(manager mfo)

Condmon 3
(debtaung)

Penod I

6,898,547

7,120,201

Penod 2

7,608,597

7,829.925

Penod 3

8,220,265

8,461,048

t-value
- 0 8074
NS
- 0 7272
NS
- 0 558
NS

mean forecasts for Condition 3 versus Condition 1 (p < 0.01), and no significant difference
between the mean values for Condmuon 3
vs Condition 2. The data are summanzed in
Table 2. This result runs counter to the hypothesis that participants who received a warning
about biases would forecast values that were less
biased (i.e. lower in this case) than those participants who received no warning.
DISCUSSION
The first result supports the initial hypothesis
regarding Condition 1 vs Condition 2. Compared to the control group's mean forecasted
values, the average forecasts were significantly
higher in Condition 2. This was perhaps an
attempt by the subjects to create forecasts m line
with the manager's suggested 13-15% increase,
but which was not supported by the data provlded to the subjects.
This result suggests that the subjects may
have been biased in their development of forecast numbers. A 'biased' forecast does not mean
that subjects intentionally developed forecasts
to be exactly 13% above the previous value. It
does mean, however, that the manager's preferences may have subconsciously 'directed' the
subjects toward that value, and thereby predisposed them toward the desired 13 % increase.
This predisposition toward certain values could
be explained by the availability bias. Since the
manager's preferences were easily available in
each subject's memory, they may have unduly
influenced judgment.
A second possible explanation for this
result is provided by selective perception, where

342

Kydd--Cognmve B~ases m Computer-Based Dectslon Support Systems

individuals seek information that is consistent
with their own views or with the desired outcome. In this case, if the manager's preferences
mattered to the subject, then the data could
have been chosen selectively for inclusion in the
computation of forecast values based on the
manager's preference for a 13-15% increase.
The second major result runs counter to the
second hypothesis of this study, which states
that the subjects in Condition 3 would forecast
values that were not significantly different from
those values forecasted in Condition 1. There
are two possible explanations for this result.
The first is that the debiasing treatment (i.e.
the warning about the existence of biases in
decision-making) was not at all effective. This
could be the case, particularly because the warning consisted of a brief description of what
cognitive bias is and how it can influence decision-making behavior. Subjects were not explicitly trained in how to recognize biased behavior
or how to avoid the pitfalls concerning biases.
Although warning individuals about the possibility of cognitive bias has been suggested as one
possible remedy [12], based on the limited
results reported here, a simple warning may not
be enough.
An alternative explanation for the results
concerning debiasing is that the warning not
only did not ameliorate the effects of the bias,
but contributed to its existence. This is, perhaps
by suggesting to the participants that individuals are sometimes overinfluenced by certain
items of information, it inadvertently brought
the manager's preferences to their attention.
They therefore reacted to these preferences
more than they would have, had the warning
not been given at all. The data support this
alternative explanation, in that the mean forecasted values for Condition 3 were greater than
those for Condition 2, although not significantly
greater.
Further support for this alternative explanation is found in the analysis of the subject's
descriptions of how they developed their
forecasted values, and what other factors, if
any, played a role in this development. In
Condition 2, almost half of the subjects (22
out of 45) reported that they had considered
other factors besides the data presented in
the scenario in determining their forecasted
numbers. A few of these subjects explicitly
stated that the manager's suggested increase of

13-15% influenced their decision. However, in
Condition 3, well over half of the subjects (15
out of 22) stated that they had determined their
forecasts primarily on the manager's suggested
percentage increase rather than basing their
numbers on the objective data provided.
Apparently, the warning concerning cognitive
bias did not prevent bias affecting the decision
process, and it may have contributed to the bias
in the forecast decisions. This suggests that
merely telling individuals about the pitfalls of
cognitive biases is not a strong enough treatment to help them overcome the effects of bias.
In fact, it may cause individuals to become more
susceptible to potential detrimental effects of the
biases.
CONCLUSION

This study has attempted to determine
whether or not cognitive bias interferes with the
decision process when supported by a computer-based decision support system, and how
such bias could be reduced or eliminated from
this process. Our results indicate that bias does
appear to hamper the decision-making process
for the forecasting decision studied here when
supported by MICROSTAT. Whether or not
this effect is pervasive throughout use of other
computer-based support systems and for other
decisions, remains as an empirical question.
This research, however, provides a first step
toward a better understanding of whether and
how such biases as availability and selective
perception may effect DSS-supported decisionmaking behavior, and how this effect may be
able to be alleviated in the future.
As the approach to debiasing taken here did
not appear to be effective, the results suggest
that a simple warning is not a strong enough
treatment to mitigate the effects of cognitive
bias. Perhaps more extensive training complete
with examples and suggestions about how to
recognize bias and correct it would be more
effective. On the other hand, such external
methods for treating the problem may not
be useful at all, as several effects of cognitive
bias are very subtle and difficult to recognize.
Further, Remus and Kotteman [26] contend
that biases cannot be eliminated through training because they are based on the neurophysiological limitations of the human brain. This
study supports that contenuon.

Omega, Vol 17, No 4

An alternative method for correcting the
errors that are made due to cognitive bias
during use of computer-based decision support
systems is to build something into the system
itself, such as interactive checks with the user
concerning biased information processing or
graphical confidence interval information [1].
An artificially intelligent DSS for statistical
analysis has been recently proposed along these
hnes. It helps the user choose appropriate tools
for analysts as well as providing support that
aids the user m avoiding common data acquisitton and processing errors [26].
Cognitive biases have long hampered effecttve
decision-making in certain areas, particularly in
cases where estimates of probabihties are cructal
to the accuracy of the decision. This study
suggests that in the use of computerized dectston
support systems, other types of btases also,
such as availability and selective perception,
prevent effective dectsion-makmg. Since the
debiasmg treatment studied here did not ehminate the effect of the bias, perhaps either a
stronger external debiasing method, or an internal method is required. In any case, cogmtive
biases appear to mfluence the quality of the
declston made when that process is supported
by a DSS. Managers should recognize that these
effects may be detracting from full effective use
of computer-based deciston support systems.
ACKNOWLEDGEMENTS
This research was supported by a Umverslty of Delaware,
General Umverstty Research Grant and a summer grant
from the Center for Informauon Systems Management,
Education and Research, Umverstty of Delaware.

REFERENCES
1 Aucom-Drew L and Kydd CT (1983) Strategies for
reducing cogmtlve bins m the design and implementatmn of decision support systems. Northeast Dectswn
Sciences Institute Proceedings, Philadelphm, USA.
2 Batson CD (1975) Rattonal precessing or ratzonahzatlon?. The effect of dnconfirmlng informaUon on stated
religious behef J. Person. soc. Psychol. 32(1), 176-184.
3 Benbasat I and Taylor R (1982) Behavioral aspects of
mformauon processing for the dcsign of management
mformatmn systems. IEEE Trans Systems Man Cyber.
netzcs. SMC-12(4), 439-450
4 Borglda E and Ntsbett RE (1977) The differential
smpact of abstract vs concrete mformat~on on decisions. J appl soc. Phychol 7(3), 258-271.
5. Coster RA (1978) The effects of three potentaal atds for
making strategqc decasions on predicuon accuracy
Organzzatwnal Behav. Human Performance 22(2)
6 DeSanctus G (1984) Computer graphics as decision
aids Directions for research. Deczs. Sc:. 15(4), 463-487.

343

7 DuCharme WM (1970) A response btas explanauon of
conservaUve human reference. J exp. Phychol, ~(1),
66--74
8 Egeth H (1967) Selective attentxon Phychol. Bull 67(1),
41-57
9. Emhorn H (1980) Learning from experience and subopUmal rules m dectslon making In CognitweProcesses
m Chmce andDecmon Behavtor (EdRed by Wallsten T),
Erlbaum, NJ
10. Emhorn HJ (1980) Overconfidence m judgment
In Fallible Judgment m Behavioral Research, (EdRed by
Shweder RA), pp !-16 Jossey-Bass, San Francisco,
CA
11 Flschhoff B (1975) Hindsight # foresight The effect of
outcome knowledge on Judgment under uncertainty
J exp Psychol Human Percep Perform 1(2), 288-299
12 Ftschhoff B (1982) Debmsmg In Judgment Under
Uncertainty Heurlst:cs and Bzases (Edited by Kahneman D, Slovlc P, Tversky A) pp 422 AAA. Cambridge
Umv Press
13 Ftschhoff B and Blyth R (1975) I knew tt would happen
Remembered probabdlUes of once future tlungs Orgamzattonal Behav Human Performance 13(1), 1-16.
14. FIschhoff B, Slowc P and Llchtenstein S (1977) Knowmg wRh certainty The approprtateness of extreme
confidence, J exp Psychol Human Percep Perform
3(4), 552-564
15 Gordon ME, Slade L A and SchmRt N (1986) The
'scienceof the sophomore' rewslted From conject~veto
emplnclsm Acad Mgmt Rev II(I), 191-207
16 Hogarth R and Makndakls S (1981) Forecasting and
planning An evaluaUon Mgmt Scz 27(2), 115-138
17 Kahneman D and Tversky A (1982) Intumve predtction Bmscs and corrective procedures In Judgment
Under Uncertamty Heunsttcs and Bzases (EdRed by
Kahneman D, Slovlc P, Tversky A) Chap 30, pp
414--421 Cambridge Umv Press
18 Kahneman D and Tversky A (1972) Subjective probabdRy, a judgment of representativeness Cogn Psychol
3(3), 430--454
19 Kunreuther H (1976) LtmRed knowledge and msurance
protectton Pubhc Pohcy 24(2), 227-261
20 Lichtenstem SC, Slovlc P, Fischhoff B, Layman M and
Combs B (1976) Judged frequency of lethal events
J exp Psychol. Human Learning Memory, 4(6),
551-578
21 Mason RO and Moskowltz H (1972) ConservaUsm m
mformauon processing Imphcatlons for management
mformauon systems Decls Scl 3(4), 35-55
22 Mowan M and Mowen JC (1986) An empmcal examination of the biasing effects of framing on business
declstons. Dec~ Sct. 17(4), 596--602.
23 Ntsbett RE, Krantz D, Jepson C and Fong G (1982)
Improving mducuve reference In Judgment Under
Uncertainty Heur~ncs and Bmses (Edited by Kahneman D, Slovtc P and Tversky A) Chap 32 pp 445-462.
Cambridge UmversRy Press
24 Ntsbett RE and Ross L (1980) Human Inference" Strate.
g~esand Shortcomings, Prentice-Hall, Englewood Chffs,
NJ
25 Ntsbett RE and Wtlson TD (1977) Telhng more than we
can know" Verbal reports on mental processes Psychol.
Rev. 84(3), 231-259.
26 Remus E and Kotteman JE (1986) Toward mtelhgent
declston support. An arUficially intelligent staustictan
MIS Q 10(4), 403--418.
27. Sage A (1981) Behaworal and organizaUonal consider.
atlons m the design of reformation systems and processes for planning and decision support, IEEE Trans.
Systems Man Cybernencs SMC-11(9), pp 640-678

344

Kydd--Cogmtwe Btases m Computer-Bused Deczston Support Systems

28. SAS InstRute, Inc. (1985) SAS User's Grade:Stattsttcs,
Verswn 5 Edmon, SAS Insutute, Cary, NC.
29. Slovlc P and Lichtenstem S (1971) Companson of
bayesian and regression approaches to the study of
reformation processing m judgment Orgamzanonal
Behav Human Performance 6(6), 649-744
30. Stabeli CB (1983) A decision-oriented approach to
building DSS. In Budding Dectswn Support Systems
(Edited by Bennett JL), Chap I0 Addison-Wesley,
Reading, MA
31. Tversky A (1974) Judgment under uncertainty. Heunstac$ and biases. Sctence 185(4157), 1124-1131

32. Tversky A (1972) Ehnunauon by aspects: A theory of
choice. Psychol Rev 79(4), 281-299.
33 Tversky A and Kahneman D (1972) The belief m the
'Law of small numbers'. Phychol, Bull 76(2), 105-110.
34 Wason PC and Johnson-Laird PN (1972) Psychology
of Reasoning" Structure and Content Harvard Univ
Press, Boston, MA
ADDaV..SSFORCORRESPONOV.~CI~Professor CT Kydd, College

of Business and Economtcs, Department of Business
Admmlstratwn, Umverstty of Delaware, Newark,
Delaware, DE 19716, USA

